% !TeX encoding = UTF-8
\documentclass[a4paper,11pt]{article}

\newcommand{\project}[1]{\textbf{#1}\xspace}
\newcommand{\BLESS}{\project{BLeSS}}
\newcommand{\TheProject}{\BLESS}

\ifdefined\final
\else
\newcommand{\final}{}
\fi

\input{preamble}
\input{participants}

\begin{document}
\pagenumbering{arabic} % for pageslts

\begin{titlepage}

\begin{center}
{\Huge \textsc{\TheProject}}
\end{center}

\begin{tabular}{lp{5in}r}
\textbf{Title of Proposal:} & \textbf{Building BLocks for Scalable Systems} & \\[4ex] 
%\textbf{Title of Proposal:} & \textbf{\TheProject{}: Refactoring Tools for Highly Parallel High-Performance Multicore/Manycore Systems} & \\[4ex] 
%\textbf{Title of Proposal:} & \textbf{\TheProject{}: Refactoring Tools for Parallel Heterogeneous Manycore Systems} & \\[4ex] % \includeimage[scale=0.5]{logo} \\
\textbf{Date of preparation:} & \textbf{\today} & \comment{}{$
$Revision: 0.0$ $}\\[4ex]
\textbf{List of participants} && \\[1ex]


\end{tabular}

%% Participants Table
\newcounter{p}
\begin{center}
\begin{tabular}{|l|p{5in}|l|l|}\hline
\textbf{Participant no} & \textbf{Participant organisation name} & \textbf{Country}\\ \hline 
1 (Coordinator) & {\sc \longparticipant{1}} \hfill (\shortparticipant{1}) & \country{1}  \\ \hline
\forloop{p}{2}{\value{p} < \theparticipant}{%
\thep & {\sc \longparticipant{\thep}} \hfill  (\shortparticipant{\thep}) & \country{\thep}  \\ \hline}%
\theparticipant & {\sc \longparticipant{\theparticipant}} \hfill  (\shortparticipant{\theparticipant})& \country{\theparticipant}  \\ \hline
\end{tabular}\end{center}

\tableofcontents

\end{titlepage}

% \input{snags}
\newpage


\pagenumbering{roman}

% ---------------------------------------------------------------------------
%  Section 1: Excellence
% ---------------------------------------------------------------------------

\pagebreak

\pagenumbering{arabic}
\setcounter{page}{2}

\khcomment{Fix US spelling of modelling.}

\begin{figure}[tp]
\begin{center}
%\framebox{\parbox{\textwidth}{
%\vspace{1.5in}
\begin{center}
\includeimage[width=0.9\textwidth]{BlessVision1.png}
\end{center}
%\vspace{1.5in}
%}}
% \vspace*{-20mm}
% \hspace*{5mm}
% \includeimage[width=0.8\textwidth]{RePhormVision}
%\vspace*{-10mm}
\end{center}
\caption{Scaling Complex Software using Building Blocks and Patterns: the \TheProject{} Vision} 
\label{fig:aims}
\end{figure}


\section{Excellence}

\khcomment{New motivation added.  Nice diagram needed.}

Software-defined infrastructures offer the opportunity to abstract across formerly disparate kinds
of computing technology: fusing cloud, HPC and IoT into a single coherent framework that supports
the flexible and scalable use of varying kinds of computing resource.  By exploiting emerging concepts of block-diagram modelling,
pattern-based software development~\cite{RPL}, and first-class resources, \TheProject{} addresses the key abstraction
issues that must be considered in constructing complex, multi-level software.
\TheProject{} builds on successful previous and ongoing work in the ICT-LEIT \paraphrase, \rephrase and \teamplay projects
on component-based abstraction, software engineering, high-performance computing, data-intensive computation,
and computational clouds.  % It aims to construct a 
\khcomment{Moved this forward.  Still a bit plodding. Rewrote it.}
%% Virtualisation of computing resources in the form of computational clouds has brought an appealing prospect of reducing
%% the cost of development and deployment of applications while still offering potentially large computing power
%% to them. Coupled with the emergence of Internet-of-Things (IoT) systems, this has resulted in increased interest
%% in distributed, and especially data-intensive computing. Efficient data analytics on large quantities of data
%% has become one of the crucial problems for modern computer science. This, however, often requires taking
%% advantage of parallel computing to exploit the potential power offered by parallel, distributed systems. Developing
%% application for these systems is, however, still very complicated and requires knowledge of low-level 
%% distributed programming models such as MPI combined with some shared-memory programming model such as 
%% pthreads or OpenMP. Additional problems come from the heterogeneity at a node-level, where central processors 
%% are combined with one or more different accelerators. For example, Amazon EC2 offers instances that combine 
%% multi-core CPUs with multiple GPU accelerators. This offers even more computing potential, but at the expense
%% of additional difficulty in programming such systems, as all the before mentioned programming models need to
%% be combined with additional models/libraries for programming accelerators, such as CUDA or OpenCL.
%% \emph{Block-diagram} modelling offers an appealing, high-level way of designing applications using visual
%% methods of composing basic \emph{building blocks} into \emph{application models}, from which then low-level
%% (usually C or C++) code is generated, compiled and executed. This way of programming is gaining large 
%% popularity in many industries, including automotive and XX domains. However, state-of-the-art block-diagram
%% modeling solutions, such as \textbf{Simulink/Matlab}, \textbf{Heptagon} and XX currently do not support 
%% parallel/distributed data-intensive applications. 
% 
% The \TheProject{} project
It will combine block-diagram modelling and pattern-based programming into a uniform,
tool-supported, software development methodology that will target both new and existing data-intensive applications,
support alternative hardware platforms and meet all necessary extra-functional properties and business requirements.
% while  abstracting over the target hardware platforms to allow portability and easier maintenance of data-sensitive applications.
In this way, we will provide a new scalable, but secure, reliable, and above all human-comprehensible means to construct
complex, flexible software systems.

\subsection{Aims and Objectives}
\label{sect:objectives}

\eucommentary{\emph{Describe the specific objectives for the project1, which should be clear, measurable, realistic and achievable within the duration of the project. Objectives should be consistent with the expected exploitation and impact of the project (see section 2).}}

\TOWRITE{KH, then others to refine}{This needs some thought and perhaps discussion to get right.}


%\TheProject{} aims to \emph{democratise} the production of highly parallel software, promoting new and revolutionary high-level pattern-based
%software development processes and methodologies that cover the entire software stack from the application to the runtime system.
%The project will produce an integrated, tool-supported software development methodology that will enable the effective production
%of highly optimised parallel software that takes into account all necessary performance, energy, security \emph{etc.} concerns.
%The project will be evaluated using real-world use cases taken from the automotive, telecommunications, 3d graphics and internet-of-things application domains. It will form the basis for
%a large and sustainable community of commercial parallel software developers.
%
The specific \emph{aims} and \emph{objectives} of the \TheProject{} project are:

\begin{description}
\item[Aim 1:]

\khcomment{data-intensive needs motivation.}
\noindent
   To develop a new composable and scalable
   % pattern-based?
   approach to developing the next generation of complex data-intensive software, using techniques that abstract uniformly over computation, communication and data;
   % pattern-based?

% \khcomment{Breathy and not exciting.  Too much detail.  Doesn't cover composition.  Doesn't really address the main abstraction issues.  Really about the solution rather than the problem.}
   %% \noindent To exploit emerging model-based software development techniques to decompose data-intensive applications into building-block components, abstracting over computations, communications and data and raising abstraction levels well above existing widely-used approaches and technologies, such as Matlab/Simulink and Heptagon.
%   such as XX, XX and XX.%pThreads, OpenMP and Intel TBB.%OpenCL, SYCL \emph{etc.}.
% and that will cover

\item[Aim 2:]

\noindent
  To develop tools and techniques to ensure that key system properties of correctness, performance, security, reliability, robustness and conformance to coding standards
  are exposed as first-class citizens through composable system components;

\item[Aim 3:]
   \noindent
   To integrate the \TheProject{} tools and technologies into a coherent software development methodology that
   supports the full lifecycle of an application (requirements, implementation, deployment, maintenance etc.);

%% \khcomment{Does this meet the call text?  Very muddled.  Two conflicting things going on here.}
%%   \noindent To integrate the \TheProject{} tools and technologies into a coherent software development methodology that
%%   supports full lifecycle of an application and ensures that all the required operational constraints and business requirements
%%   (including performance, security, reliability, robustness and conformance to coding standards) are satisfied.

\item[Aim 4:]
   \noindent
   To provide mechanisms that abstract over alternative heterogeneous deployment platforms to support highly scalable and adaptive data-intensive applications;

\item[Aim 5:]
 \noindent
 To demonstrate the applicability of the \TheProject{} tools and methodology on real-world use cases from the AI, Internet-of-Things (IoT), and
 automotive domains, and to promote long-term uptake;

\item[Aim 6:]

\noindent
 To improve programmer productivity and allow development of complex scalable data-intensive applications at different
 levels of abstraction (from high-level block-diagram models to executable code).

%\khcomment{OK, but only hits one of the specified application domains, and not sure C/C++ matters as an objective.  Also mixes up applicability and productivity.}
  %% \noindent To demonstrate the applicability of the \TheProject{} tools and methodology on real-world use cases from the automotive, machine learning
  %% and Internet-of-Things (IoT) application domains, increasing programmer productivity and allowing application development at different
  %% levels of abstraction (from high-level block-diagram models to executable code).
\end{description}

%\pagebreak
%% The issues we want to address include:
%% \begin{itemize}
%% \item
%% Massive parallelism in an energy constrained environment
%% \item
%% Multi-objective optimisation using deep learning techniques for energy, real-time etc
%% \item
%% Security-by-design
%% \item
%% Runtime systems integration, including feedback, monitoring and dynamic analysis
%% \end{itemize}

% The corresponding concrete \emph{objectives} are: % of the \textbf{\TheProject{}} project are: % (Figure~\ref{fig:objectives}):
%% \TODO{These need to match the Work Programme description per Objective.}

%% \begin{description}

%% \item[Objective 1:]
%%   \noindent To build on the existing techniques for block-diagram, model-based design, extending them with new
%%   building blocks to encapsulate computations, communications and data distribution, adapting these techniques to
%%   development of data-intensive applications and significantly raising the level of abstraction over the existing approaches.
%%   %and provide building blocks that capture data, computations and communication while properly taking into
%% %account relevant business requirements such as security, performance, reliability and robustness.
%% %
%% \emph{This fulfils part of \textbf{Aim 1} and part of \textbf{Aim 3}.}

%% \item[Objective 2:]

%%   \noindent To develop a new pattern-based programming methodology based on \emph{composable} C/C++ components, generated from higher-level building blocks,
%%   that capture the important extra-functional properties of applications and allow developing application code at different levels of abstraction.
  
%% \emph{This fulfils part of \textbf{Aim 1}, part of \textbf{Aim 2} and part of \textbf{Aim 3}.}

%% \item[Objective 3:]

%%   \noindent To develop novel techniques for generating software-defined infrastructures to carry information about execution environment and network topologies and for matching these with the patterns of computation and communication in the applications to ensure automatic adaptation of software to different deployment platforms.
%% %\noindent To develop complementary techniques to enable automated discovery of advanced parallel patterns in existing software applications, and to provide
%% %programmer-directed refactorings to aid the introduction of parallelism, also while properly taking into account energy, safety, security and performance \emph{etc.} concerns.
%% \emph{This further fulfils part of \textbf{Aim 1} and part of \textbf{Aim 3}.}

%% \item[Objective 4:]

%%   \noindent To produce new software development and optimisation techniques to ensure that all relevelant business requirements, such as security, performance, reliability and robustness, are satisfied.
%%   %
%% \emph{This fulfils part of \textbf{Aim 3}.}

%% \item[Objective 5:]
%% \noindent
%% %\noindent To properly support the parallel software development methodology by providing new refactoring and other tools to support parallel software development, both using model-based design and using C++ and
%% %fully integrating with widely-used Integrated Development Environments, including both Microsoft Visual Studio and Eclipse, to
%% %support standard and widely-used collaboration, testing, debugging, compilation \emph{etc.} frameworks.
%%   %
%% To develop automated mechanisms for ensuring the compliance of the code generated from the high-level models, as well as of the software-defined infrastructure, to the relevant coding standards in automotive and other industries.  
%% \emph{This fulfils part of \textbf{Aim 2} and part of \textbf{Aim 3}.}


%% \item[Objective 6:]

%% \noindent To demonstrate that the \TheProject{} tools and techniques can support the full lifecycle of applications, from requirements capturing and design, to deployment and maintenance, using a number of real-world use cases, taken from the automotive, machine learning and internet-of-things (IoT) domains.
%% %
%% \emph{This fulfils part of \textbf{Aim 4}.}

%% \item[Objective 7:]

%% \noindent To demonstrate that the \TheProject{} tools and techniques increase performance, security, reliability and robustness of the data-intentsive applications, while at the same time also increasing productivity by decreasing the development and deployment time of the applications. 
%% %
%% \emph{This fulfils part of \textbf{Aim 1} and part of \textbf{Aim 4}.}

%% \item[Objective 8:]

%% \noindent To build a sustainable user community that will ensure the long-term uptake, development and ultimate commercial success of the \TheProject{} tools and methodology.
%% %
%% \emph{This fulfils part of \textbf{Aim 4}.}


%% % \item[Objective 8:]

%% % \noindent to deploy an effective dissemination and exploitation model where
%% %     public \TheProject{} research outputs (software and
%% %     scientific publications) are released in a timely fashion to a
%% %     wider audience in order to boost European
%% %     competitiveness.
%% % This fulfils the remainder of \textbf{Aim X}.

%% \end{description}

%% Collectively, the achievement of these objectives will ensure that all four aims of \TheProject{} are successfully achieved.
%% %In order to do this, we will exploit new techniques based on \emph{deep learning} and \emph{multi-objective optimisation}
%% %from \SCCH{} and \INRIA{} to understand and reason about performance and security \emph{etc.} properties, advanced refactorings and software transformations from \USTAN{} to ensure that models and patterns are properly reflected
%% %in the parallel software,
%% %advanced provably secure techniques from \IBM{} to ensure that parallel software maintains a high degree of integrity,
%% %new static and dynamic analysis techniques from \PRL{}, \USTAN{} and \SCCH{} to determine parallel software quality and performance,
%% %and advanced compilation technology from \UM{} both to ensure that code is energy efficient and performant
%% %and to ensure that full feedback on performance, energy usage \emph{etc.} is provided to the software developer %through the
%% %design model, parallel patterns and associated \TheProject{} tools.

%% %\pagebreak

%% \subsubsection{Detailed Description of the Objectives}
\label{sect:objs-detailed}

\TOWRITE{KH, then others to refine}{This needs to be done after
the objectives are defined above   We need about 1-2 paragraphs
per objective, just to flesh it out.}


\subsubsection*{Objective 1: Significantly Raise Abstraction Levels for Developing Complex Data-Intensive Applications.}
\vspace{-6pt}

%%   \noindent To build on the existing techniques for block-diagram, model-based design, extending them with new
%%   building blocks to encapsulate computations, communications and data distribution, adapting these techniques to
%%   development of data-intensive applications and significantly raising the level of abstraction over the existing approaches.
%%   %and provide building blocks that capture data, computations and communication while properly taking into

Driven by:
\begin{inparaenum}[i)]
\item
increasing levels of application sophistication and systems intelligence, including the incorporation of advanced AI
techniques;
\item
a corresponding growth in the volumes of data that need to be processed rapidly, correctly, and efficiently;
and
\item
rapidly evolving and scaling hardware;
\end{inparaenum}
it is increasingly necessary to provide abstractions that can encapsulate computations, communications and data distribution.
%
At present, complex data-intensive applications are mostly developed using relatively low-level programming 
techniques such as MPI. % Despite the increase in the level of abstraction that is brought by some state-of-the
% programming models such as Google's Map-Reduce, 
In typical approaches, many low-level aspects still need to be handled explicitly and laboriously by the programmer. 
These include e.g. ensuring an appropriate decomposition of the problem,
distributing the work and data appropriately to a  possibly  heterogeneous hardware platform, ensuring appropriate
task and data synchronisation, dynamically handling high workload situations etc.
\khcomment{More examples, please.  I added some possibilities.  This doesn't really apply to map-reduce, does it?}
%
In contrast, high-level, block-diagram and pattern-based modelling and implementation
techniques offer an intuitive and, above all, human-understandable way of designing
applications, supporting automatic generation both of the application model and of
portable, efficient and retargetable implementations, and allowing automated and flexible
runtime control.
% targeting e.g. C/C++ as standard, portable and performant
% back-ends. 
However, these techniques are currently unsuitable for constructing complex, data-intensive parallel
applications on the large-scale distributed, and highly heterogeneous, cloud-like systems of the future. 
They must be significantly enhanced and developed if we are to deal with the demands of (near and longer-term) future
data-intensive applications.  In \TheProject{}, we will build on existing block-diagram based modelling technologies, such
as \textbf{Simulink/Matlab}, \textbf{Scade} and \textbf{Heptagon}, and extend them with
building blocks for constructing large-scale composable  applications. We will also develop new building block libraries and code generators
to generate efficient parallel and composable C/C++ code from the applications models, 
so allowing development of data-intensive applications using block-diagram modelling techniques
and significantly increasing the level of abstraction.
\newline
\emph{This fulfils part of \textbf{Aim 1}, part of \textbf{Aim 4} and part of \textbf{Aim 6}.}


\subsubsection*{Objective 2: Develop a Composable Pattern-Based Programming Methodology.}
\vspace{-6pt}

%%   \noindent To develop a new pattern-based programming methodology based on \emph{composable} C/C++ components, generated from higher-level building blocks,
%%   that capture the important extra-functional properties of applications and allow developing application code at different levels of abstraction.
While pattern-based parallel programming is a well-established technique for developing compute-intensive
parallel applications in various industrial and scientific domains, with parallel patterns being an
integral part of common parallel programming frameworks and libraries such as Map-Reduce, OpenMP and 
Intel TBB, the concept is still mostly restricted to a few patterns of computations, such as parallel application
of the same operation over different data items (``maps''). In the \TheProject{} project, we will generalise the concept 
of patterns to also provide abstractions over data, data distribution and data communication.  These new and advanced
patterns will incorporate different business requirements of the applications, including performance, security, reliability and robustness. 
Dealing with business requirements as an intrinsic part of the development process
will make pattern-based programming methodologies applicable to a much wider class of applications
than is currently the case, enabling us to deal with emerging classes of large-scale, data-intensive
applications that may be deployed on highly heterogeneous computing systems.
\newline
\emph{This fulfils part of \textbf{Aim 1}, part of \textbf{Aim 2}, part of \textbf{Aim 4} and part of \textbf{Aim 6}.}


\subsubsection*{Objective 3: Exploit Software-Defined Infrastructures as part of a Coherent Implementation Strategy.}
\vspace{-6pt}

As high-performance technologies, including massive parallelism and specialised acceleration become increasingly
prevalent in the computing space,
% 
% With the emergence of many-core hardware, 
so the target platforms for data-intensive applications are 
becoming increasingly diverse and heterogeneous. It is not uncommon to find ``many-core'' systems that include several
accelerators of different types even in low-power computing systems. It is impossible to expect the programmer
to deal with the diversity of the systems on which their applications could be executed (including ones that have not yet been
designed), or to expect existing programming models or libraries to automatically adapt to all possible target systems. In the \TheProject{} project,
we will build on the concept of software-defined infrastructures (SDIs), developing solutions that will
enable the programmer to specify characteristics of the target platforms and applications and,
subsequently, \emph{automatically} generate code to enable the distribution and synchronisation of work and data on different systems
architectures, including incorporating the necessary fault tolerance mechanisms. We will use the \textbf{SYCL} extension to C/C++ for generating portable code that can target different accelerators within heterogeneous systems. We will also develop mechanisms to match the program characteristics,
abstracted into patterns, and the target information in SDIs to allow automatic adaptivity of the applications.
\newline
\emph{This fulfils part of \textbf{Aim 2} and part of \textbf{Aim 4}.}

\subsubsection*{Objective 4: Provide New optimisations for Key Business Requirements.} % , including security, performance, reliability and robustness.}
\vspace{-6pt}

In addition to ensuring the functional correctness of an application, in terms of the 
results that are produced, in many cases it is equally important to ensure that additional, extra-functional requirements
are met. For example, in real-time applications, there may be hard limits on the time in which the result has to be
produced.  It follows that the performance and consistency of the application is often as important as its functional correctness. 
In other  cases, it is a requirement to have a certain level of security and/or reliability, even if this comes at the 
expense of performance. In the \TheProject{} project, we will develop novel methods for dealing with the reliability and 
security of data-intensive applications in distributed settings. We will also develop novel optimisation
techniques based on machine-learning that will be able to optimise for a combination of possibly conflicting 
extra-functional properties (such as, for example, performance and reliability), thus ensuring the business 
requirements of data-intensive applications are satisfied.
\newline
\emph{This fulfils part of \textbf{Aim 2} and part of \textbf{Aim 3}.}

\subsubsection*{Objective 5: Produce Automated Mechanisms for Verifying the Compliance of Code to Standards.}
\vspace{-6pt}

In many industries, there exist coding standards to which the code is expected to conform in order to
avoid the most common sources of unreliability or insecurity. For example, in the automotive industry, the
de-facto standards are \textbf{MISRA} and \textbf{AUTOSAR}. It is, therefore, essential to devise automated
code analysis mechanisms that will check for compliance to the required standards and, possibly, semi-automatically suggest suitable fixes or even 
repair the code to ensure standards compliance. In the \TheProject{} project, we will build on existing technology
for quality assurance that has been developed by the project partners as part of their commercial operations,
and extend it to address complex distributed, data-intensive applications. We will also extend our diagnostic techniques to
cover multiple levels of abstraction, providing
compliance checking and repair both at the block-diagram model level and at the level of the C/C++ code that we
will generate.
\newline
\emph{This fulfils part of \textbf{Aim 2} and part of \textbf{Aim 3}.}



\subsubsection*{Objective 6: Demonstrate that the \TheProject{} Tools can Cover the Full Software Lifecycle.}
\vspace{-6pt}

Our goal in the \TheProject{} project is to produce tools and technologies that will support the full
lifecycle of data-intensive applications, including requirements capture, implementation, testing, debugging,
deployment and maintenance. While the tools and technologies that we will produce over the course of the project
will mostly  target the implementation, deployment and maintenance phases, we will also demonstrate how the tools
that have developed in previous and ongoing projects, such the \textbf{RePhrase} EU H2020 project (see page~\pageref{projects}), can be used to support
requirements capture, testing and debugging of complex distributed, data-intensive applications. 
\newline
\emph{This fulfils part of \textbf{Aim 3}.}


\subsubsection*{Objective 7: Demonstration of the \TheProject{} tools on realistic use cases.}
%  from automotive, machine learning and IoT domains.}
\vspace{-6pt}

We will validate the \TheProject{} approach using complex, real-world applications from a number of
different domains, 
including automotive, machine learning and smart cities using IoT. These applications have been chosen to 
demonstrate both the development of new applications from scratch, using the block-diagram modelling and 
generation of efficient C/C++ code from it, and adapting the existing applications by reusing the pieces of
their code as components in new application models. 
They are used to illustrate different scales of problem, alternative deployments, and varying classes of complexity in the
underlying algorithms and data, including exploiting tightly-coupled many-core
accelerators, adaptive and heterogeneous cloud-like systems, and fog/edge computing systems.
%
The chosen applications (described in more detail on page~\pageref{sect:applications}),
will be used to evaluate the effectiveness of the various aspects of the \TheProject{} approach against real-world requirements.
% , from the application
% modelling to compilation and dynamic adaptation. 
We will evaluate the increase in developer productivity from using \TheProject{} technologies,
the robustness and reliability of the systems that are produced,  and the ability of the resulting applications to adapt to alternative
(classes of) hardware platforms.
We will also show that all of the required business requirements are satisfied,
including correctness, security, performance and consistency requirements.
\newline
\emph{This fulfils part of \textbf{Aim 1}, part of \textbf{Aim 5}, and part of \textbf{Aim 6}.}

\subsubsection*{Objective 8: Building a sustainable user community.}
\vspace{-6pt}

 \TheProject{} aims to build a sustainable user community in a number of ways: firstly, we have built in
 specific user-community building activities,
 including workshops, tutorials, and webinars, that will serve to
 actively promote the use of the \TheProject{} tools and technologies.
 Secondly, we will, as far as possible, follow an Open Science approach to our research,
 making our results, data and tools publicly available as they are
 developed through widely-used open source and
 open data repositories such as GitHub (\url{http://www.github.org}) and
 Zenodo (\url{http://www.zenodo.org}).  Thirdly, we will actively
 engage with potential industry and academic users through exhibitions
 and demonstrations at existing trade events, through relevant user
 conferences, such as CPPCon and EclipseCon\khcomment{Are these the right ones?}, through the annual ICT
 conference, and through other relevant events.
 Finally, we will engage with existing and emerging standards.
 in the automotive, machine learning, IoT, smart cities and other domains that
 will be considered by the \TheProject{} project,
 We will ensure compliance with existing standards where
 these are relevant and applicable to the domains that we will study, will help 
 adapt existing standards where these do not
 sufficiently consider abstraction and composition, and will help develop new standards
 and guidelines where  appropriate standards do not currently exist.
\newline
 \emph{This fulfils part of \textbf{Aim 4}.}
 
\pagebreak
\subsection{Relation to the Workprogramme}

\eucommentary{
Indicate the work programme topic to which your proposal relates, and explain how your proposal addresses the specific challenge and scope of that topic, as set out in the work programme.}

\TOWRITE{Someone with an overview first}{We all need to take a
pass here.  We can be focused: we don't need to hit all targets
equally, or even at all - better to say we don't target
something than mistakenly say that we do!  KH}

The \TheProject{} project clearly addresses the challenge outlined for ICT-16-2018 on 
\textbf{software technologies}, targeting specifically \textbf{integrated programming models \& techniques 
for exploiting the potential of virtualised and software defined infrastructures}. % research and innovation action.
Our aim is to develop a novel programming methodology based on block-diagram modelling and patterns, enhancing existing
methods with elements that abstract over computation, communications and patterns of data access/distribution.
This will allow the treatment of \textbf{software, data, computing and communication resources as abstract 
elements}. We will develop static and dynamic mechanisms for mapping and synchronising
computations and data to large-scale distributed, heterogeneous systems. These mechanisms will, together with
the high-level of abstraction that block-diagram pattern-based modelling offers \textbf{enable data to flow freely 
over heterogeneous infrastructures in a scalable, distributed and human-understandable fashion}.
In addition, the clear benefit of using block-diagram modelling and pattern-based development extending these models with
blocks/patterns that abstract over computation, communication and data will be the \textbf{increased degree of
abstraction without losing controllability or correctness}. The expected \TheProject{}
outputs will deliver new tools, methodologies and techniques for development, deployment and
maintenance of data-intensive applications. 
They will be linked with existing technologies for requirements capture, debugging and testing,
% We will also exploit the existing technologies
% for requirements capturing, debugging and testing data-intensive applications. 
Collectively, this will support the
\textbf{full software lifecycle in adopting this new paradigm}. Finally, by developing methods to encapsulate
parts of existing code bases into components that can be part of either the high-level block-diagram
model or the patterned C/C++ code, we will enable the \textbf{ exploitation of reusable code and software 
components from existing code bases}.
%
The \TheProject{} project primarily targets \textbf{code and resources abstraction} as part of \textbf{integrated
programming models \& techniques for exploiting the potential of virtualised and software defined infrastructures}. 
By building on and extending the block-diagram modelling approach, and by also providing opportunities for developing
data-intensive applications at different levels of abstraction (the model level and the code level), we 
will make clear \textbf{advances in how to abstract code and data beyond simple semantic annotations that 
are expressive and machine-readable}. We will employ our expertise in optimisation methods using machine-learning
techniques, exposing extra-functional parameters from applications and developing automatic mechanisms to
enable them to be tuned for a particular combination of metrics. This will allow our annotations to \textbf{carry
out additional information about execution requirements, network topologies, data sources} and to 
\textbf{(de)compose the non-functional properties}. Finally, using the portable SYCL extension to C/C++ as 
an intermediate language and developing further compilation and adaptation mechanisms, as well as developing
mechanisms to specify and exploit software-defined infrastructures, will allow automatic \textbf{conversion 
to different target platforms}.


%\pagebreak
\khcomment{Diagram here?}
\subsection{Concept and Approach}

\eucommentary{Describe and explain the overall concept underpinning the project. Describe the main ideas, models or assumptions involved. Identify any trans-disciplinary considerations;}

%% Virtualisation of computing resources in a form of computing clouds has brought an appealing prospect of reducing
%% the cost of development and deployment of applications while still offering potentially large computing power
%% to them. Coupled with the emergence of Internet-of-Things (IoT) systems, this has resulted in increased interest
%% in distributed, and especially data-intensive computing. Efficient data analytics on large quantities of data
%% has become one of the crucial problems for modern computer science. This, however, often requires taking
%% advantage of parallel computing to exploit the potential power offered by parallel, distributed systems. Developing
%% application for these systems is, however, still very complicated and requires knowledge of low-level 
%% distributed programming models such as MPI combined with some shared-memory programming model such as 
%% pthreads or OpenMP. Additional problems come from the heterogeneity at a node-level, where central processors 
%% are combined with one or more different accelerators. For example, Amazon EC2 offers instances that combine 
%% multi-core CPUs with multiple GPU accelerators. This offers even more computing potential, but at the expense
%% of additional difficulty in programming such systems, as all the before mentioned programming models need to
%% be combined with additional models/libraries for programming accelerators, such as CUDA or OpenCL.

%% \emph{Block-diagram} modelling offers an appealing, high-level way of designing applications using visual
%% methods of composing basic \emph{building blocks} into \emph{application models}, from which then low-level
%% (usually C or C++) code is generated, compiled and executed. This way of programming is gaining large 
%% popularity in many industries, including automotive and XX domains. However, state-of-the-art block-diagram
%% modeling solutions, such as \textbf{Simulink/Matlab}, \textbf{Heptagon} and XX currently do not support 
%% parallel/distributed data-intensive applications. 

%% The \TheProject{} project aims to combine the block-diagram modelling and pattern-based programming into uniform
%% tool-supported methodology for adapting the existing and developing new data-intensive applications, addressing at the same time any extra-functional properties and business requirements of the application. At the same time, we will develop novel ways for abstracting over the target hardware platforms to allow portability and easier maintenance of data-sensitive applications.



%We will extend the existing modelling systems with the building blocks that encapsulate and abstract over computations,
%communication and data (including data distribution and synchronisation) of the data-intensive applications,
%thus allowing development of these applications at a very high-level of abstraction. We will also provide a strong
%link between the C/C++ code generated from the application model and the model itself, allowing us to take
%advantage of the extensive tooling for C/C++ parallel applications (including safety detection, security and
%optimisation tools), allowing us to reflect the results of various analyses at lower level to the model itself and
%to (automatically) tune the model to address the potential issues. We will also address the critical issues of
%security and reliability of distributed applications, also developing optimisation mechanisms to target a 
%combination of potential requirements of the applications, including security, reliability, robustness and 
%performance. Finally, we will address the problem of automatic adaptation of the applications generated from
%high-level models to various distributed, heterogeneous target hardware systems, by developing novel sophisticated
%mapping and compilation mechanisms, together with novel ways to specify and generate 
%\emph{software-defined infrastructures} to abstract over concrete hardware platforms and automatically match the
%application structure with the generated hardware abstractions.





\vspace{-6pt}
\subsubsection{Challenges for Large-Scale Data-Intensive Applications}
% massively-parallel heterogeneous systems}

Raising the level of abstraction for programming data-intensive applications, while at the same time allowing portability and optimising for given extra-functional properties presents a number of key challenges:

\begin{itemize}
\item \textbf{High-level modelling of data-sensitive applications.} In
  order to bridge the gap between the high level of abstraction that
  programmers want to use in designing, implementing and deploying their
  code, and the increasing heterogeneity and decentralisation of modern
  hardware systems, it is necessary to use programming models that are
  sufficiently high-level to abstract over low-level implementation details
  \emph{and} expressive enough to describe distributed
  data-intensive applications with all their potentially complicated
  communication and data-flow patterns. Current programming models that are used for data-intensive applications, such
  as MPI, are not suitable for this, as they are very low-level and
  require the programmer to deal with many detailed implementation
  issues. On the other hand, high-level programming methods, for
  example the block-diagram modelling systems used by
  \textbf{Simulink/Matlab} and \textbf{Heptagon} or pattern-based systems such as \textbf{Google Map-Reduce} are not tailored to
  data-intensive distributed applications, as they do not have building
  block/pattern constructs that can be used to represent possibly complicated
  interactions and flow of the data of such applications. \textbf{We
    must, therefore, adapt these high-level programming models to
    data-intensive applications, developing new building blocks and patterns to
    represent data, communication and computation.}

%\pagebreak
\item \textbf{Describing and optimising application properties and
  business requirements.} Data-intensive applications may have a range of
  different requirements that they need to satisfy, in terms of levels of
  security (especially if they are executed on computational clouds), safety
  and performance. It is crucial to be able to be able to specify these
  requirements at the model-level and to carry this information through to
  the generated code and the runtime system. %  that deploys and controls the  application execution. 
  Additionally, it may be necessary to dynamically
  optimise the application execution for a combination of these
  requirements, including possible tradeoffs between different
  requirements (e.g.~security and performance). \textbf{We must,
    therefore, develop novel methods for specification and dynamic
    runtime (multiobjective) optimisation of application properties and
    requirements.}

\item \textbf{Abstracting over distributed heterogeneous hardware.} With
  new advances in hardware design, the range of different classes of
  systems that the modern data-intensive application will be expected to
  execute is dramatically increasing. Data-intensive applications might
  be required to run on vastly different hardware, from   homogeneous shared-memory systems
  to large-scale  heterogeneous distributed systems, containing different
  types of accelerators. To be able to deal with this variety in
  hardware, it is necessary to develop new ways of abstracting over the
  actual systems on which the applications will be executed and to
  develop mechanisms to \emph{generate} from this abstraction an
  execution model that is tailored to specific hardware, and that will be able to
  accommodate different runtime optimisations. \textbf{We must,
    therefore, develop new ways of describing and generating
    software-defined infrastructures for data intensive applications on
    distributed hardware.}

\item \textbf{Developing new runtime system mechanisms for data-intensive
  applications on heterogeneous distributed systems.} Increasing the
  level of abstraction for designing and implementing applications, as
  well as for describing hardware properties, means that most of the
  mechanisms for the efficient execution of the applications are delegated
  to compilers and runtime systems. Since the programmer needs to specify
  fewer decisions about the actual execution, more burden falls to the
  runtime system to map the application to the hardware and to control its
  execution. The runtime system needs to work together with the
  software-defined infrastructure and to take into account any
  application properties and requirements. \textbf{We therefore need to
    develop new runtime system mechanisms for dynamic adaptation of the
    applications to different hardware platforms.}

\item \textbf{Compliance of the block-diagram model to existing
  standards.} Many industries have their own coding standards that the
  application code is required to comply to. There are tools,
  such as \PRLshort{}'s \textbf{QA-Verify} that ensure
  compliance of the source code to the required coding standards. However, there
  are currently no tools that can check block-diagram application
  models for compliance to these standards and can identify the parts of the
  model that make it incompatible with the standards. Since
  block-diagram modelling tools generally generate intermediate C/C++ code from
  the application model, it is possible to apply C/C++ diagnostics
  tools to this code. However, it is currently not possible to link these
  diagnostics back to the block-diagram model. \textbf{We need to,
    therefore, develop new mechanisms to link the code that is generated from the
    block-diagram model to the model, and to reflect code diagnostics
    at the C/C++ model back to the application model.}

\item \textbf{Addressing safety and security of data-intensive
  applications and software-defined infrastructures.}  Security and
  safety are two of the most important requirements for the
  data-intensive applications, since they ensure that the code is not
  vulnerable for exploitation via cyberattacks and, also, that the code
  operates correctly and produces the desired results. The distributed nature
  of new hardware systems and data-intensive applications presents
  new issues for ensuring the safety (in terms of e.g. the absence of deadlocks and
  race-conditions in the code) and security of these
  applications. Furthermore, decoupling the application from the
  middleware that is used to execute it (the software-defined infrastructure) makes it
  harder to establish security and safety properties, since we have to analyse both
  the application and the middleware, as well as the interaction between them.
  \textbf{We need to, therefore, develop new methods for
    ensuring safety and security of the distributed data-intensive
    applications when executed under software-defined infrastructures.}



\end{itemize}


The challenges identified above require a new and radical
approach that tackles these issues in a coherent and
holistic way. 
\emph{Fundamentally, the problem is that there does not exist a coherent methodology and associated toolchain for programming data-intensive applications on scalable distributed computing systems that, at the same time, ensures that  the
given extra-functional requirements are satisfied.}
The \TheProject{} project aims to produce a new
tool-supported structured design and implementation process for modern scalable systems
systems, in which performance, safety, security
and integrity concerns can be simultaneously ensured through design and through construction. % , and through tooling.

\clearpage
 \begin{figure}[tp]
 \begin{center}
% \vspace*{-5mm}
%\vspace{-5cm}
%\hspace{-1in}
 \includeimage[width=1\textwidth]{RePhorm2Vision1.pdf}
 \end{center}
\vspace{-1cm}
 \caption{The \TheProject{} Approach.}
 \label{fig:approach}
 \end{figure}

%% \begin{figure}[tp]
%% \begin{center}
%% \vspace*{-5mm}
%% \hspace{-1in}\includeimage[width=0.9\textwidth]{Flow3_updated-TN.jpg}
%% %\vspace*{30mm}
%% \end{center}
%% \caption{Code-flow diagram.}
%% \label{fig:CodeFlowDiagram}
%% \end{figure}

%% \begin{figure}[tp]
%% \centering
%% %\vspace*{-5mm}
%% %\hspace{-1in}
%% \includeimage[width=0.8\textwidth]{RuntimeMOO_2.jpg}
%% %\vspace*{30mm}
%% \caption{Information-flow (blue) and decision making/execution (red) during runtime.}
%% \label{fig:InformationFlowMOO}
%% \end{figure}

\subsubsection{Achieving The \TheProject{} Vision}


%\noindent

%\draftpage

% \subsubsection{Achieving the \TheProject{} Project Vision}

\textbf{Figure~\ref{fig:approach}} shows the overall \TheProject{} approach and
methodology. The programmer starts either with a new application that can be developed from the outset using the block-modelling approach, or with
an existing C/C++ application.  In the latter case, it is necessary to identify the parts that can be
turned into the components of the block-diagram model. 
The programmer then designs an \emph{application model} using a given set of building blocks. These blocks will include both:
\begin{inparaenum}[i)]
\item
basic blocks for encapsulating data, computations, and communication over distributed systems; and,
\item
higher-level \emph{pattern}~\cite{RPL} blocks that encapsulate some general combination of computation, communication and data distribution and synchronisation.
\end{inparaenum}
\emph{Intermediate} C/C++/SYCL code is then generated from this model using
the \TheProject{} generator and the associated library of
building block implementations.
A strong link will also be established between the basic and pattern blocks at the pattern level and the corresponding code blocks and patterns at the C/C++/SYCL level.
% We will also establish a strong link between code blocks and patterns at the C/C++/SYCL level and the corresponding basic and pattern blocks at the pattern level. 
This will enable various static and dynamic analyses on the intermediate
code to verify its safety, security and compliance to necessary
standards. Depending on the results of these analyses, alterations of the
model may be required.  In line with standard industry practice, 
the intermediate code will not modified directly to ensure
safety/security/code compliance, but instead  the results of the
analyses will be reflected back to the model level,
and possible changes to the models will be proposed. After this phase, the intermediate code
is compiled into \emph{binary} code.

The programmer also creates a \emph{system specification}, describing the target hardware, to be compiled into the
\emph{software-defined infrastructure}. We will exploit the \textbf{Erlang} programming language~\cite{Cesarini:2009} to generate software-defined infrastructures.. 
Erlang was designed for distributed-memory systems.
It contains simple and generalised abstractions to encapsulate processes (which can be components written in other languages, such as C/C++) 
that communicate over various types of networks. It also has built-in fault tolerance, work distribution and load balancing mechanisms. 
This Erlang-based software-defined infrastructure will be responsible for
the distribution of computations and data, and for monitoring and tuning the execution of the application. 
The  software-defined infrastructure, together with
the application, will then be deployed to the target platform under the control of
the runtime system. As part of this, a \emph{dynamic optimiser}
will dynamically tune application parameters to optimise for different
required runtime metrics.  This will ensure that the extra-functional
properties and requirements of the application are fully satisfied.

 \begin{figure}[tp]
 \begin{center}
 \vspace*{-5mm}
 \hspace{-0.2in}\includeimage[width=0.7\textwidth,angle=-90]{ModelFeedback.pdf}
 \end{center}
  \vspace{-2cm}
 \caption{Feedback between Block-Diagram Model and the generated C/C++/SYCL Code.}
 \label{fig:modelling}
 \end{figure}
 
\textbf{ Figure~\ref{fig:modelling}} shows the feedback loop between the block-diagram model of an application and the code generated from it, which is used by diagnostics tools to establish security, safety and code compliance. The orange arrows show what is possible using existing technologies, namely:
\begin{inparaenum}[i)]
\item
generating the application model based on the building blocks;
\item
 generating the C/C++ code from this model; and
\item
static analysis of this code. 
\end{inparaenum}
The green arrows show the extensions that will be achieved by the \TheProject{} technologies, namely
\begin{inparaenum}[i)]
\item
 reflecting the results of diagnostics to the application model and buildings blocks; and
\item
and suggesting alterations of the model to ensure compliance of the generated code to standards.
\end{inparaenum}

%% \begin{description}
%% \item \textbf{We must develop new high-level programming
%%     models, tailored to automotive, telecommunications, 3d graphics and IoT domains, that will be usable 
%%     both by normal domain-expert and expert parallel programmers.}

%% \item \textbf{We must develop novel analyses for automatic
%%     discovery of potential parallelism in the applications, and methods
%%     for automatic preparation of the code for introduction of parallelism.}

%% \item \textbf{We must develop mechanisms that will both \emph{detect} possible concurrency bugs
%%   arising from the usage of patterns and, if possible, \emph{automatically repair} the code to eliminate these
%%   bugs.}

%% \item \textbf{We must develop mechanisms for automatic dynamic compilation and mapping of components of the
%%     patterned applications to the target hardware.}

%% \item \textbf{We must develop new decentralised static and dynamic
%%     multi-objective optimisation techniques, adapted to massively-parallel hardware.}

%% \item \textbf{We must develop mechanism for ensuring safety of the parallel code, both at design/implementation stage (\emph{security-by-design}) and at runtime.}

%% \item \textbf{We must develop new analyses to ensure that the code generated using the \TheProject{} toolchain conforms to any required
%% coding standard, increasing confidence in its safety and security.}
%% \end{description}

%% The approach that \TheProject{} takes to address these challenges is visualised in Figures~\ref{fig:approach}, \ref{fig:CodeFlowDiagram} and \ref{fig:InformationFlowMOO}. More details on the workplan that will be used to achieve the \TheProject{} vision and the associated workpackages can be found in Section~\ref{sect:workplan} (Page~\pageref{sect:workplan}).\khcomment{What happened to the model??}

%% First, Figure~\ref{fig:approach} shows the overall approach of \TheProject{}, including the new tools and libraries that will be developed/extended and how they interact to form a coherent whole.
%% % Figure~\ref{fig:approach} shows the approach that \TheProject{} takes to address these challenges, including the new tools and libraries that will be developed/extended and how they interact to form a coherent whole.
%% % Successful completion of the \TheProject{} project will allow
%% % major advances to be made in software development for highly-parallel % mixed-criticality % cyber-physical
%% %  systems.
%% % More detail on the workplan that will be used to achieve the \TheProject{} vision and the associated workpackages can be found in Section~\ref{sect:workplan} (page~\pageref{sect:workplan}).
%% % \khcomment{What happened to the model??}
%% The programmer starts with either a new application, with a set of sequential
%% or parallel components written in C/C++ that needs to be plugged in the future application, or with an existing legacy
%% C/C++ application. 
%% %
%% In the case that we start with a new application, a high level DSL, that contains a library of DSL parallel patterns, is
%% used to design an application and link in the components. The code
%% generator is then invoked that generates the parallel C/C++ code, using the OpenMP and Intel TBB parallel libraries. The code is then
%% passed to the tools that work on the C/C++ level. Since we do not want to change the code generated by the code 
%% generator that corresponds to the model code, we use safety checking and (semi-) automatic repairing only on the C++ components,
%% and we identify alternative patterns or pattern structures that can be used to avoid the identified problems at the model level.
%% % 
%% In the case when we start with an existing sequential C/C++ application, parallelism discovery is first run in order
%% to find the places in the source-code where parallelism can be introduced and the code is annotated so that the programmer
%% can, with the help of the refactoring tool, introduce parallel patterns in the source-code. In this case, we can use safety
%% checking and refactoring tools to alter the resulting C/C++ code, either by fixing possible safety issues or to directly
%% change the pattern structure of the C/C++ code. 
%% %
%% After these phases, in either of the two cases, we end up with a parallel patterned C++ code. This source-code is then compiled to a binary code, possibly with intermediate representation for multiple versions of some components. The code is then run on parallel hardware and is controlled by multi-objective optimisation engine, that monitors the application execution and invokes optimisation decisions, reconfiguring application as it is executing to adapt it to satisfy given extra-functional requirements, such as performance, energy consumption, real time response, dependability etc.

%% %% An overview of the approach taken by \TheProject{} project is shown in Figure~\ref{fig:approach}.

%% Figure~\ref{fig:CodeFlowDiagram} provides the artefacts manipulated in
%% the project and their relations. The figure is vertically split in two
%% parts. On the right is the C/C++ process flow. Here, components are
%% first refactored. This refactoring phase is responsible for both
%% exposing parallelism, and then possibly curtailing it to facilitate
%% mapping, by grouping smaller functions into larger sequential tasks
%% according to criteria such as data locality, common period, common
%% deadline, or load balancing. Once refactoring is completed, static
%% mapping decisions are taken. This step produces the implementation
%% C/C++ code, which includes linker, OS, and run-time configuration. The
%% static mapping may further coarsen the grain of the tasks to reduce
%% scheduling overhead and/or non-determinism. In some applicative fields
%% ({\em e.g.} critical embedded control systems) it may even produce
%% fully static schedules for some resources and application parts that
%% require strict timing predictability. Finally, the implementation
%% C/C++ code is compiled, resulting in the light green parts of the
%% running implementation (and its configuration).

%% The left side of the figure shows the flow in the model domain.  A
%% model includes two parts: the (blue) functional part, described in a
%% dataflow synchronous language, and a (purple) extra-functional part
%% comprinsing both platform-independent requirements that can be
%% annotated on the elements of the functional specification, and a
%% separate specification of platform-dependent aspects, including a
%% description of the target platform. Like in the C/C++ domain, we
%% identify two levels: specification and implementation. Refactoring is
%% performed at specification level, before moving to the implementation.
%% Note that the latter no longer has non-functional requirements, but
%% only non-functional properties providing a full description of the
%% mapping, including all configuration.

%% The C/C++ and the model domains are connected bidirectionally with
%% code generation (purple) and traceability (green) links. These links
%% must preserve the parallel patterns. Thus, parallelism/security/{\em
%%   etc.}  patterns of the model level are directly mapped onto equivalent
%% patterns at the C/C++ level, and most patterns of C/C++ level can be
%% presented at model level. In particular, most refactoring performed
%% at C/C++ level can be represented as refactoring at the model level.

%% \comment{SCCH}{We do not mention security in Figure~\ref{fig:CodeFlowDiagram}.}

 \begin{figure}[tp]
 \begin{center}
 \vspace*{5mm}
 \hspace{0in}\includeimage[width=0.9\textwidth]{MOO_flow.png}
 \end{center}
  \vspace{0cm}
 \caption{Information Flow in Multi Objective Optimiser.}
 \label{fig:MOO_flow}
 \end{figure}


Finally, \textbf{Figure~\ref{fig:MOO_flow}} provides a closer look at the
information flow and decision-making process within the multi-objective
optimisation engine. The first part of the optimisation engine is the
Optimisation Interface that is responsible for retrieving and
pre-processing relevant information for optimisation (e.g., the functional
and extra-functional parameters of the application, the constraints imposed
by the static mapping, the monitoring and actuator parameters as well as
optimisation objectives, the configuration of the software-defined infrastructure). This information is then
utilised by the Predictive Modelling Tool for generating performance
models that relate the optimisation objective with the available
optimisation parameters. Support may also be needed by the Data
Interfaces tool equipped with a Knowledge-Base (KB), which is able to
handle data acquisition, storage and preprocessing during the generation
of the predictive models. The predictive models developed are fed into
the Multi-Objective Optimisation engine for generating optimal settings,
either off-line (during the initialisation phase) or by on-line
adjustments. Also, any time-criticality and static mapping constraints
imposed by the model need to be enforced. During the dynamic adjustment
phase, monitoring information is usually necessary. Moreover, decisions
that are related to application parameters may require just-in-time (JIT)
compilation of the injected intermediate representation.

\pagebreak
\subsubsection*{Key assumptions}

\khcomment{This may not be necessary.}

\TheProject{} makes a number of fundamental assumptions that will be tested and verified in the course of the project,
and which form the basis for a register of technical risk.  The most significant assumptions are:

\begin{enumerate}[{A}1)]
\item
We can devise building blocks that capture data, computation and communication for data-intensive applications, as well as common patterns, in terms of composition of these blocks, that occur frequently in such applications;
\item
These building blocks and patterns can expose the required extra-functional properties and requirements;
\item
It is possible to establish relation between the building blocks and patterns at the model level and the generated C/C++/SYCL that will enable diagnostics performed at the C/C++/SYCL level to be reflected to the model level;
\item
It is possible to automatically detect and then repair a wide range of security, safety and code compliance breaches at the model level;
\item It is possible to abstract common characteristics of distributed hardware systems 
\item It is possible to use Erlang for distribution, monitoring and managing of components written in C/C++ as a part of generated software-defined infrastructures

\end{enumerate}

% \subsubsection*{Transdisciplinary concerns}
% 
% \khcomment{Only if relevant.}

\subsubsection{Positioning of the project}
% * <kevin@kevinhammond.net> 2018-03-28T13:54:41.793Z:
% 
% Needs TRLs and table
% 
% ^ <kevin@kevinhammond.net> 2018-03-28T13:54:58.292Z.
\eucommentary{Describe the positioning of the project e.g. where it is situated in the spectrum from 'idea to application', or from 'lab to market'. Refer to Technology Readiness Levels where relevant.}

In line with the expectations of the ICT-16-2018 call, \TheProject{} aims
to achieve overall Technology Readiness Level (TRL) 5-6 (``technology
validated in relevant environment (industrially relevant environment in
the case of key enabling technologies)'').  Some new technologies will
need to be developed, but we are able to build on a number of related
existing tools and technologies, including: \IBM{}'s software verifier ExpliSAT (TRL 7) and virtual patching technology (TRL 4);
\PRL{}'s QA-Verify, that is already in
commercial use (TRL 9), 
\SCCH{}'s machine learning framework, that is
already being   used in application oriented research projects (TRL 6-7);
\SCCH{}'s RePhrase Mapping Framework PaRL-Sched which has shown significant performance boost in industrial applications (TR 4-5); the
\paraformance refactoring tool (including safety checking mechanisms), that is being taken to market by the
Scottish Enterprise \paraformance Innovation Project (TRL 6-7); \CODEPLAY{}'s SYCL implementation ComputeCPP is currently pre-SYCL 1.2.1 conformance (therefore TRL 7-8) and will be TRL 8-9 with 1.2.1 conformance and commercial versions in mid 2018; 
\INRIA{}'s Heptagon compiler and the Lopht static parallelisation tool (TRL 3-4).  Existing mature tools will be taken in
new and exciting directions, as part of a coherent methodology for
developing highly distributed model-based applications and 
\GOLEM{}s Smart City Monitor platform is already being deployed on real-world application (TRL 5-6) and will be further promoted to TRL 7-8 for deployment in small and medium cities and organisations managing various metropolitan assets (trade centres, buildings, vehicles, water, waste, etc).
%highly parallel model-based or C/C++ software, and new and
This mature and emerging technology will be applied to realistic and representative use cases
taken from commercially important application areas, including the automotive, machine learning and smart cities domains.
% The specific advances that
% will be made by \TheProject{} are described in more detail in
% Section~\ref{sec:novelty} %(page~\pageref{sec:novelty}).
%
Importantly, \TheProject{} includes both tool builders and industrial
end-users.  It follows that validating the overall project results in an industrially-relevant environment (TRL 5-6)
is entirely achievable within the duration of the project. Following the successful completion 
of the project, our exploitation plan shows how the key technologies that will be produced
by \TheProject{} can subsequently be raised to TRL 7 and above, and released for production use.
Many of the tools that are produced will be released as open source throughout the duration of the project, and will be available
for long-term use by the community at large.
% Although, in line with funding requirements, the \TheProject project will not directly develop the more mature existing technologies
% such as QA-Verify,  
Additional major new features will become available to all of the tools that will be exploited in the project, both new ones and mature ones.
This will be beneficial to existing users, and will lead to new opportunities for innovation as described in Section~\ref{sec:innovationpotential} 
(page~\pageref{sec:innovationpotential}).
%
% The key enabling technologies that will be developed in the project are:

\begin{center}
  \begin{tabular}{|p{4.9in}|l|l|}
    \hline
    \textbf{Key Enabling Technology} & \textbf{Current TRL} & \textbf{Final TRL} \\
    \hline
     &  & \\
    \hline PharosN platform running Smart City Monitor & TRL 5 & TRL 6 \\  
    
    \hline Vulnerability Analyzer & TRL 3 & TRL 5 \\  
    
    \hline Virtual Patching Generator & TRL 4 & TRL 5 \\  
    
%    \hline Machine Learning Framework & TRL 6 & TRL 8 \\  
    \hline Machine Learning Framework & TRL 6 & TRL 7 \\  
    
    \hline PaRL-Sched & TRL 4 & TRL 6 \\  
    
    \hline Building Blocks Compliance Modules for MISRA and AUTOSAR & TRL 2 & TRL 5 \\
    \hline \paraformance Safety Checking Module for Distributed Applications & TRL 2 & TRL 5 \\
    \hline
    
%%     \hline PharosN platform running Smart City Monitor & TRL 5 & TRL 6 \\  
    
%%     \hline Vulnerability Analyzer & TRL 3 & TRL 6 \\  
    
%%     \hline Virtual Paching Generator & TRL 4 & TRL 6 \\  
    
%% %    \hline Machine Learning Framework & TRL 6 & TRL 8 \\  
%%     \hline Machine Learning Framework & TRL 6 & TRL 7 \\  
    
%%     \hline PaRL-Sched & TRL 4 & TRL 6 \\  
    
%%     \hline Building Blocks Comliance Modules for MISRA and AUTOSAR & TRL 2 & TRL 7 \\
%%     \hline
    
  \end{tabular}
\end{center}

\noindent
The specific advances that will be made are described in more detail in Section~\ref{sec:novelty} (page~\pageref{sec:novelty}).


\subsubsection{Linked research and innovation activities}
\label{projects}

\eucommentary{Describe any national or international research and innovation activities which will be linked with the project, especially where the outputs from these will feed into the project;}

\vspace{-8pt}
\paragraph{\rephrase (ICT-644235).}
%\vspace{-12pt}

The \rephrase project aims to study the software engineering process as
a whole for heterogeneous parallel machines
using C++.  It considers neglected, but important, issues such as
effective testing, debugging, maintenance and quality assurance for
multicore/manycore machines, and involves major industry players such as IBM.
%
The \rephrase project is focused on the software engineering cycle,
and on the single-node execution environment. 
\rephrase will thus provide a foundation for the work that will be carried
out in \TheProject{}. In particular, we will be able to exploit the
\rephrase requirements capture, testing and debugging technology to ensure that the \TheProject{} technology supports the full lifecycle of data-intensive applications. 
\emph{\TheProject will, however, substantially extend the work that has been done in \rephrase by:
\begin{inparaenum}[i)]
\item
further raising the level of abstraction through using block-diagram modelling in addition to patterns;
\item
targeting distributed, as well as shared-memory systems;
\item
enabling portability over targets through exploiting software-defined infrastructures;
and
\item
developing novel machine-learning based multiobjective optimisation techniques that will simultaneously target multiple business requirements.
\end{inparaenum}}

\vspace{-2pt}
\paragraph{\paraphrase (ICT-288570).}
%\vspace{-12pt}
% \TOWRITE{CB,VJ}{Write about ParaPhrase} 
The \paraphrase project introduced a new structured design and implementation process for
heterogeneous multicore/manycore architectures, in which developers exploit a variety of
parallel patterns to develop component based applications in Erlang and C++. These
component based pattern-applications may then be re-mapped to meet the
application requirements and hardware availability. 
This approach was subsequently exploited by the \rephrase project.
\emph{\TheProject{} builds on the parallel patterns that were originally developed in the \paraphrase project, but extends the concept of patterns
to also cover data distribution and communication as well as computation.
Importantly, \TheProject also extends patterns to encapsulate the required business properties of data-intensive applications, including extra-functional
properties.  Moreover, \paraphrase was restricted to
smaller scale shared-memory parallel systems. \TheProject{} will significantly
extend the range of architectures that can be targeted, using software-defined infrastructures to abstract over actual hardware.}

\vspace{-2pt}
\paragraph{\teamplay (ICT-779882).}
The \teamplay{} project aims to develop new, formally-motivated, techniques that will allow execution time, energy usage, security, and other important non-functional properties of parallel software to be treated effectively, and as first-class citizens. This will be built into a toolbox for developing highly parallel software for low-energy systems, as required by the internet of things, cyber-physical systems etc. The \teamplay{} approach will allow programs to reflect directly on their own time, energy consumption, security, etc., as well as enabling the developer to reason about both the functional and the non-functional properties of their software at the source code level.  The \teamplay{} concept of first-class resources has inspired us to adopt a similar approach in \TheProject{}.
\TheProject{} will be able to exploit the basic structures and mechanisms for resource analysis and certification that are
being developed as part of \teamplay{}.
\emph{\TheProject{} will go well beyond \teamplay{}, however, in considering how to compose and decompose resource information,
and in dealing with complex and scalable distribution patterns.}


\vspace{-2pt}
\paragraph{\paraformance (Scottish Enterprise Innovation Grant).}
The \paraformance{} project is a UK-funded innovation project, whose purpose is to create
a high-growth company of scale, exploiting key research results from the EU \paraphrase and \rephrase
projects.
 This \paraformance{} approach is both effective and practical: \paraformance{} tooling has successfully analysed million-line C++ programs,
 narrowing over 1,000 potential sites of potential parallelism to 29 effective sites.
 This task would take weeks or months of human effort, but the \paraformance{} tool achieves it
 in less than 60 seconds. \paraformance also has built-in support for \emph{safety checking}. %, where it identifies potential race conditions in a code and automatically repairs some of them.
%
\TheProject{} will exploit the safety-checking techniques developed in \paraformance.
\emph{It will go beyond \paraformance by  providing a complete methodology and tool-chain for developing block-model,
by extending the techniques to complex distributed, data-intensive applications, and by incorporating the
possibility of runtime failure.} 

\paragraph{\advanceproject (ICT-248828).}
% \vspace{-12pt}
% * <kevin@kevinhammond.net> 2018-03-28T13:55:29.031Z:
% 
% TeamPlay and others
% 
% ^ <kevin@kevinhammond.net> 2018-03-28T13:55:41.728Z.
The \advanceproject project addressed the dynamic adaptation of concurrent applications 
based on statistical performance feedback for streaming applications.  
It developed new and advanced cost-directed hardware
virtualisation technology to map programs onto emerging hardware
architectures
while simultaneously respecting programmer
expectations and requirements on extra-functional properties, such as
resource utilisation or power consumption.
%
\TheProject{} will be able to exploit some of the results from the \advanceproject project, notably  cost-models to predict the execution time, which will be used in 
the dynamic optimisation framework to build application performance models, and the \TheProject{} dynamic adaptation mechanisms.
\emph{\TheProject{} will, however, go well beyond \advanceproject in allowing easier
application designing using block-diagram modelling modelling systems, considering multiple
target extra-functional properties and considering security and safety of the code.
We will also investigate a much wider range program structures,
more substantial applications and more diverse target architectures.
}

%% \paragraph{REPARA (ICT-609666)}
%% \vspace{-12pt}

%% The REPARA project aims to help the transformation and deployment of new and legacy applications
%% in parallel heterogeneous computing architectures while maintaining a balance between
%% application performance, energy efficiency and source code maintainability.
%% To achieve this objective, the \textbf{REPARA} project uses source code transformations 
%% to multiple programming models through refactoring techniques.
%% %
%% The key elements in \textbf{REPARA} are: automated partitioning of applications, transformation to
%% specific programming models, compilation to reconfigurable hardware, and integration of performance
%% and energy models. All these are integrated through the use of an unified run-time system.
%% %
%% \emph{\TheProject{} will use some of the work done on REPARA, in particular the work on
%% energy and performance models. However, it goes well beyond REPARA in terms of its scale and 
%% infrastructure. It will also provide the flexascaling capabilities and programming 
%% methodology, including pattern discovery and program shaping, that REPARA lacks.}

\vspace{-2pt} \paragraph{\project{Dreamcloud} (ICT-611411).}
%\vspace{-12pt}

The main objective of \project{Dreamcloud} is to enable dynamic resource allocation in manycore embedded and HPC systems while providing appropriate guarantees on performance and energy efficiency.
%
\emph{\TheProject{} will benefit from the rich-functional scheduling and mapping components of \project{Dreamcloud}. However, compared to \project{Dreamcloud}, \TheProject addresses many additional issues involved in developing data-intensive applications, such as high-level designing, safety and security checking, compliance of code to standards and multiobjective optimisations for multiple target metrics.}



%\vspace{-2pt}
%\paragraph{JUNIPER (ICT-318763).}
%\vspace{-12pt}

%The fundamental challenge addressed by JUNIPER is to enable application development using an industrial strength programming language that enables the necessary performance and performance guarantees required for real-time exploitation of large streaming data sources and stored data.
%%
%\emph{\TheProject{} will benefit from the unique MPI-enabling Java technologies, developed by JUNIPER. In particular, the communication layer of the \TheProject platform will be developed based on the Java binding for the OpenMPI communication library.} %, currently developed in the frame of the project.}

\newcommand{\EXCESS}{\project{EXCESS}}
\vspace{-2pt}
\paragraph{\EXCESS (ICT-611183).}
% \vspace{-12pt}

\EXCESS aims to provide radically new energy execution models forming foundations for energy-efficient computing paradigms that will enable significant improvements in energy efficiency for computing systems.
%
\emph{\TheProject{} will exploit parts of \EXCESS monitoring and analysis framework as a part of its
dynamic optimisation technology, but will go well beyond it in considering high-level design
of applications, safety and security checking, compliance of code to standards etc.}

\vspace{-2pt}
\paragraph{\project{ASSUME} (ITEA3 14014).}
% \vspace{-12pt}

The goal of the \project{ASSUME} project is the affordable, standards-compliant
development and verification of highly automated, safety-relevant, and
performance-critical mobility systems. A strong focus is on
development methods for concurrent systems and static verification
techniques.
%
\emph{\TheProject{} will extend this work with the introduction of
  parallel patterns and 
  traceability mechanisms providing model-level visibility to
  lower-level code transformations.  It will also  consider the
  applicability of \project{ASSUME} work in three new fields: automotive, AI and IoT.}

\vspace{-2pt}
%\paragraph{Discovery (EPSRC EP/P020603).}

\paragraph{\discovery{} (EPSRC EP/020631).}
The \discovery{} project is a UK-funded research project, whose purpose is to investigate parallel pattern discovery
in legacy code.  It directly tackles the problems of:
i) determining where patterned parallelism could be introduced within an application;
ii) determining how to shape the application so that it exposes more/better patterned parallelism.
% The \discovery{} project complements most current research on parallel systems, by enabling new opportunities to exploit parallelism. 
Unusually, but importantly, \discovery considers both newly-written applications and legacy sequential and parallel applications.
\emph{The results of the \discovery project are directly relevant to the \TheProject{} project, since they allow discovery of those parts of the C/C++ code that can be encapsulated into components,
 and that can then abstracted over using basic or pattern building blocks. Therefore, \discovery{} will enable us to consider legacy sequential and parallel applications
 and to apply the \TheProject methodology to them. Whereas the \discovery project deals just with the discovery of parallel computation components, \TheProject{} goes well beyond this by 
\begin{inparaenum}[i)]
\item
extending the notion of patterns to also include data and communication;
\item
considering larger-scale distributed applications;
\item 
raising the level of abstraction of the target application through block-modelling techniques;
and
\item
addressing various implementation and deployment issues that are outside of scope of \discovery, such as reliability and fault tolerance.
\end{inparaenum}}

\vspace{-2pt}
\paragraph{Smart Urbana (CPS Labs 644400).}
% \vspace{-12pt}
The \project{Smart Urbana} project
``Enabling municipalities with Cyber-Physical System (CPS) instruments and business models for digital transformation of real time data about urban processes into digital services for the urban communities''
was supported by \GOLEMshort{}. It successfully applied a
Smart City Monitor in 5 medium size  European cities. 
\emph{\TheProject{} builds on the results of the experiment that prove the need for high level modelling support in the building of customised CPS models of urban areas at the highest level of abstraction, 
refactoring of the Smart City Monitor C++ code for processing real time data streams applying machine learning tools and extending the concept indicator chains and object statuses patterns to exploit parallelism,
adding safety and security checks, analyses for compliance to standards and adaptiveness.}




\subsubsection{Overall approach and methodology}

\eucommentary{Describe and explain the overall approach and methodology, distinguishing, as appropriate, activities indicated in the relevant section of the work programme, e.g. for research, demonstration, piloting, first market replication, etc.;}

\TheProject{} comprises six technical work packages: WP2 on high-level modelling languages;
WP3 on components and software-defined infrastructures;
WP4 on runtime systems;
WP5 on multi-objective optimisation;
WP6 on safety and security; and
WP7 on use cases and evaluation.  WP1 covers management, and WP8 covers dissemination and exploitation.
The relationship between the project objectives and these workpackages is shown below

\vspace{-8pt}
\begin{center}
\begin{tabular}{|l|l|l|}\hline
\textbf{Objective} & \textbf{Purpose} & \textbf{Contributing WPs} \\\hline \hline
Objective 1 & Model-Based Design and Development & \textbf{WP2}, \textbf{WP3}, \textbf{WP4} \\\hline
Objective 2 & Pattern-Based Programming Methodology & \textbf{WP3}, \textbf{WP4}, \textbf{WP5} \\ \hline
Objective 3 & Software-Driven Infrastructure & \textbf{WP3}, \textbf{WP4}, \textbf{WP5}\\ \hline
Objective 4 & Optimisations for business requirements of applications & \textbf{WP5}, \textbf{WP6}\\ \hline
Objective 5 & Verifying code compliance & \textbf{WP2}, \textbf{WP3}, \textbf{WP6}\\ \hline
Objective 6 & Demonstrating the \TheProject{} Approach for the Full Software Lifecycle & \textbf{WP2-WP6,WP7} \\ \hline
Objective 7 & Demonstrating Productivity & \textbf{WP7} \\ \hline
Objective 8 & User Community Building & \textbf{WP7, WP8} \\\hline
\end{tabular}
\end{center}

\paragraph*{Work Programme for Objective 1.}

Objective 1 aims to build on the existing techniques for block-diagram modelling and extend them to support
modelling of distributed, data-intensive applications, abstracting over computations, communication and
data. In WP2, we will extend the existing block-diagram modelling systems, such as 
\textbf{Matlab/Simulink}, \textbf{Scade} and \INRIAshort{}'s \textbf{Heptagon}  with both:
\begin{inparaenum}[i)]
\item
\emph{basic building blocks} to encapsulate data and its logical and runtime properties, in terms of, for example
distribution over hardware, structure and interpretation of the data, computations involved in data
analytics and communication between computing and data components;
 and
\item
\emph{pattern building blocks} that will encapsulate commonly encountered combinations and interactions between basic building blocks.
\end{inparaenum}
In WP3, we will also develop new building block
libraries, providing implementations of the new basic and pattern building blocks that will target
heterogeneous systems at different levels of abstraction. In WP4 we will develop sophisticated 
compilation and runtime mechanisms for the generated C/C++/SYCL code. % that will be generated from the application models. 
Collectively, these mechanisms will allow adaptation of the existing and development of new applications at
a very high level of abstraction, using application models, while still generating efficient 
and portable target 
code from these models. % that will be able to adapt to different target hardware platforms.

%DOES NOT SAY ANYTHING ABOUT WP5: fixed
%% But incomprehensible...
\paragraph*{Work Programme for Objective 2.}

Objective 2 aims to develop a new pattern-based programming methodology to capture essential extra-functional
properties. In WP2, we will develop \emph{pattern building blocks} that will abstract over common structures of building blocks and their interactions for data-intensive applications. In WP3, we will develop equivalent intermediate code (C/C++/SYCL) patterns that will correspond to pattern building blocks. In this way, we will further increase the level of abstraction that is offered to \TheProject{} developers.
% enabling  programmers to develop code at
% different abstraction levels. 
These patterns will also support different extra-functional 
requirements, such as the level of security, reliability, robustness and adaptivity.
The compilation, runtime adaptation and monitoring mechanisms that will be developed in WP4, together with
the optimisation mechanism of WP5, will further support generation of efficient code for different architectures and runtime
adaptation of the applications to ensure the business requirements are satisfied. 
% WP5 will enable extra functionalities and optimising such code.
%% Objective 2 is to develop new techniques to enable model-based design and development of
%% new parallel applications. The modelling languages that will be developed in WP2
%% will support development of new applications at the very high level. The techniques for
%% ensuring safety and for transformation between alternative pattern structures that
%% will be developed in WP3 will feed back directly to the modelling level, ensuring semi-automatic
%% optimisation of the application code at the model level. Multi-objective optimisation techniques that will be developed in WP5 will ensure that the developed code is optimised for energy, performance, time predictability, real time response and other metrics of interest, while the security infrastructure in WP6 will ensure the safety of the code generated from the high-level application model.

\paragraph*{Work Programme for Objective 3.}
 Objective 3 aims to develop novel techniques for defining and generating software-defined infrastructures that
 will abstract over the target hardware platforms while still carrying information about execution environment
 and network topology.  It will also develop methods to match the application model with the software-defined infrastructure 
 to allow  applications to adapt to different target platforms. In WP3, we will extend the concept of
 parallel patterns that we have successfully implemented in the \paraphrase and \rephrase
 projects, and allow them to carry additional information about the extra-functional requirements of an application.
  WP3 will also develop mechanisms for specifying characteristics of a target hardware and
 additional properties of the data, data distribution and communication etc.
 These will be used to generate, software-defined infrastructure for executing distributed,
 data intensive-applications,  exploiting the \textbf{Erlang} programming language~\cite{Cesarini:2009}.
 This  provides very high levels of reliability, built-in support for fault tolerance. 
 %% Not C/C++?
 Compilation and runtime-system methods from WP4 will be responsible for generating
 machine code and executing the application 
%% Eh?
% coupled with the software-defined infrastructure, 
 on the actual 
 hardware. Finally, in WP5 we will develop methods for runtime optimisation of both the application 
 parameters and software-defined infrastructure, enabling adaptation to changes both in the available hardware and in the
 application itself.
 
 
\paragraph*{Work Programme for Objective 4.}
Objective 4 aims to produce new software development and optimisation techniques to ensure that all the relevant
business requirements, such as security, performance, safety (reliability) and robustness, of the data-intensive 
applications are met. In WP5, we will develop machine-learning based techniques for:
\begin{inparaenum}[i)]
\item
extracting 
tunable parameters of the application and software-defined infrastructure; 
\item
constructing models that
describe the application's runtime behaviour in terms of the impact on relevant metrics (performance etc.)
of the identified parameter; and
\item
automatic runtime tuning of the identified parameters so that the
business requirements of the application are satisfied. 
\end{inparaenum}
Furthermore, in WP6 we will develop techniques for
dealing with the key business requirements of  data-intensive applications,
%  on shared-memory and  distributed heterogeneous
% environments, 
including dealing with the challenges that are presented by the execution environment for 
security and reliability. The techniques for tuning security and reliability will be exploited by the 
machine-learning mechanisms that will be developed in WP5.

%% does not say anything about wp 3 and 6; fixed
\paragraph*{Work Programme for Objective 5.}
Objective 5 aims to develop automated mechanisms for ensuring the compliance of the code generated from the 
high-level block-diagram models to the required industry standards. % of the automotive and other industries.
In WP2, we will build on and extend existing techniques to determine compliance of (parallel)
C/C++ code with the \textbf{MISRA} and \textbf{AUTOSAR} automotive standards. % in the automotive industry.
% and
%develop methods for semi-automatic repairing of the code to ensure the compliance.
We will also establish the relationship between diagnostics and the
location of their root cause (the block-diagram model or the code
generator).  Changes that are required in the C/C++ code as a result of the
application model will be highlighted as changes in the block-model.
Changes required in the code-generation will be handled in one of two
ways.  The first is improved analysis tool support for generated code to
allow ``filtering'' of issues that are primarily maintenance issues and
therefore do not apply to code that is maintained through the model.  The
second is the creation of a tool that will create a ``Standard Prettified''
version of the source that used for analysis purposes only.
WP3 will define and implement the C/C++/SYCL \emph{components} of data-intensive
applications, corresponding to building-blocks at the model level,
together with an interface that allows the \emph{composition} of these
components into distributed applications. WP6 will then provide static and
dynamic analyses of the code to determine compliance to safety coding
standards.
%% Objective 5 is support the parallel software development methodology by providing
%% refactoring and other tools to support software development and to integrate with
%% widely-used Integrated Development Environments. In WP3, we will develop refactoring,
%% safety checking and parallelism discovery tools that will be supported by the
%% compilation and runtime system mechanisms from WP4. In the WP3 we will also
%% integrate our tools into the state-of-the-art IDEs such as Visual Studio and
%% Eclipse, allowing us to take advantage of the techniques for debugging, remote
%% collaboration, testing and other stages of software development.



\paragraph*{Work Programme for Objective 6.}

Objective 6 aims to demonstrate that the \TheProject{} tools and technologies can support the full software lifecycle.
% of data-intensive applications. 
The tools and techniques that will be developed in WP2-WP5 will specifically target the
\emph{design}, \emph{implementation}, \emph{deployment} and \emph{maintenance} phases of the application lifecycle. 
To address the remaining stages of application development, including requirements capture, debugging, testing 
and verification,  we will exploit the tools and techniques that have been
developed in the course of the EU \rephrase project.  We will demonstrate the approach using
the benchmarks and real-world use cases that will be developed in WP7.

%% Objective 6 is to demonstrate that the \TheProject tools and techniques can cover
%% all levels of software stack. In WP7, we will develop a variety of benchmarks and
%% use cases that will demonstrate the applicability of the \TheProject methodology
%% and tools to both realistic use cases taken from important industrial domains,
%% such as aerospace, graphic processing and telecommunications, as well as synthetic
%% benchmarks that will target specific levels in the software stack. All these
%% benchmarks and use cases will be properly evaluated, demonstrating increase in
%% productivity, reliability and maintainability of software, as well as showing that
%% we can optimise these applications to target different conflicting runtime metrics
%% with the help of optimisation tools from WP5.

\paragraph*{Work Programme for Objective 7.}

Objective 7 aims to demonstrate that the \TheProject{} tools and technologies can improve the performance, security,
reliability and robustness of data-intensive applications, simultaneously improving productivity and
decreasing development and deployment time. In WP7, we will develop realistic use cases from the Smart City, 
automotive, machine learning and IoT domains, using the \TheProject{} block-diagram modelling, software-defined infrastructure, and pattern 
techniques.
Where feasible, we will develop both manually produced versions of the applications and versions that use
the \TheProject{} tools and methodology.  This will allow us to compare the productivity, development and deployment time that
each of these approaches takes in real world system prototypes. 


\paragraph*{Work Programme for Objective 8.}

 Objective 8 is to build a sustainable user community that will ensure long-term
 uptake and commercial success of the \TheProject technology. WP8 will promote
 the use of \TheProject tools and technologies in the wider developer community.
 We will provide suitable documentation for the programming tools and methodology.
%  (which at the moment either does not exist or, for some tools,
%  exists in the from of patchy notes). 
 A web site will be
 designed and implemented via which the user community will be
 supplied with documentation and demonstrator tools. We will also gather feedback.
 This process will start on the first day of
 the project in the form of review of existing materials, and
 it will then progress in pace with the development of \TheProject{} technology.

%% \paragraph*{Work Programme for Objective 8: }

%% Objective 8 is covered by \ref{wp:dissem}, which involves ensuring the
%% long-term dissemination of the \TheProject{} research results,
%% with a view to boosting European competitiveness. Such results
%% can be roughly construed as software, publications, and
%% research progress.
%% %
%% We will build a software repository using the \TheProject{}
%% portal and allow the software to be licensed under the EUPL,
%% maintained at the Open Source Observatory and Repository for
%% European public administrations (OSOR.eu)~\cite{Hollmann08}.
%% %
%% In terms of publications, we will liaise with the FP7 OpenAIRE
%% infrastructure located at the University of Ghent in
%% Belgium~\url{http://www.openaire.eu/}.
%% %
%% Finally, research progress and general public announcements
%% will take advantage of the EU CORDIS
%% service~\url{http://cordis.europa.eu/}.


\draftpage
\subsection{Ambition}

\eucommentary{Describe the advance your proposal would provide beyond the state-of-the-art, and the extent the proposed work is ambitious. Your answer could refer to the ground-breaking nature of the objectives, concepts involved, issues and problems to be addressed, and approaches and methods to be used.}

\subsubsection{Advances Beyond the State-of-the-Art}
\label{sec:novelty}
\label{sect:background}
\label{sect:state-of-the-art}

%\eucommentary{}


\TOWRITE{ALL}{Add sections on your own technologies/update
what's here.}

The \TheProject{} project will revolutionise the design and implementation of
data-intensive applications for distributed hardware platforms by:
\begin{itemize}
\item Extending the existing block-diagram modelling systems to allow easier design and implementation of data-intensive applications for shared-memory and distributed heterogeneous hardware systems, adapted to automotive, AI and internet-of-things (IoT) application domains;
\item Develop novel methods for abstracting over hardware details and generating adaptive \emph{software-defined} infrastructures;
\item Extending the code generators of the block-diagram modelling languages with mechanisms to ensure \emph{compliance} of the generated code to the industrial standards, as well as safety and security;
\item Developing a new methodology for data-intensive applications that target heterogeneous distributed hardware technologies, covering the full life-cycle of the application;
\item Developing new mechanisms for embedding the existing C++ code components into the high-level
  application model obtained using modelling languages, thus allowing programmers to combine the legacy
  sequential or parallel code fragments with the high-level block-diagram application design;
\item Developing novel static and dynamic code analyses for ensuring safety and security of both the application code and of software-defined infrastructures;
\item Develop novel mechanisms for semi-automatic altering of the block-diagram models so that the C/C++ code generated to them conforms to the required safety and security standard, as well as compliance to any other required standards from particular industries, such as \textbf{MISRA} and \textbf{AUTOSAR}
\item Implementing a runtime system that supports compilation and execution of distributed applications across different hardware architectures while monitoring the code execution and gathering performance metrics which will be used for runtime optimisations;
\item Developing novel dynamic and decentralised multi-objective optimisation techniques to dynamically reconfigure and
  optimise the application for a combination of given metrics, such
  as performance, reliability and safety;
%\item Extending the best security code practices to better address parallel code, including lock free concurrency mechanisms;
\item Developing novel mechanisms for efficient detection of vulnerabilities in the code, including detection of patterns in which code side effects may be exploited in software-defined infrastructure; 
%and informing refactoring
  %engine about the vulnerabilities, as well as for considering tradeoffs between security and other metrics;
\item Leveraging the code generation tool to create more secure code by construction and applying best security code practices and adding input validation for untrusted input sources;  
\item Developing new algorithms for building models for detecting untrusted inputs that may utilise vulnerabilities in the code, thus enabling runtime protection for the code generated by \TheProject.  
\end{itemize}

% In
% Sections~\ref{sect:background-first}--\ref{sect:background-last}
% we describe the present state-of-the-art, and in
% Section~\ref{sect:beyondStateoftheArt} we describe how the
% research contributions made by the  \TheProject\ project will
% advance the state-of-the-art. Finally, in
% Section~\ref{sect:existing-tools}, we describe how we will
% develop the tools and technologies that we have produced in
% previous work so that it meets the aims of the \TheProject{}
% project.
% %
% By enabling the technical progress outlined below, and as
% described further in Section~\ref{sec:impact}, we will permit
% significant improvements in designing, implementing, optimising and using parallel software.
% Our technical
% achievements will cement European excellence in parallel systems, and strengthen the European research
% area.




\subsubsection{Block-Diagram Modelling Languages}
\label{sect:background-first}
\label{sect:parallelDSL}

Our vision on the use of modelling languages is firmly rooted in
previous academic and industrial practice, and follows the classical,
general pattern of Fig.~\ref{fig:relations}, which describes the 3
pieces of information defining an embedded implementation problem.
\begin{figure}[h]
 \begin{center}
 \includeimage[width=0.6\textwidth]{figRelations1.pdf}
 \end{center}
 \caption{Specification of an embedded implementation problem}
 \label{fig:relations}
 \end{figure}

%\begin{figure}[htb]
%  \begin{center}
 %   \scalebox{1.2}{\input{images/figRelations.pdftex_t}}
%% Please do not uncomment this without first checking in the file!!
 %   \caption{Specification of an embedded implementation problem}
 %   \label{fig:relations}
 % \end{center}
%\end{figure}

For the \emph{functional specification}, we will use industry-standard
dataflow synchronous languages such as 
Scade (\url{http://www.esterel-technologies.com/products/scade-suite/})
and discrete-time
Simulink (\url{https://www.mathworks.com/products/simulink.html}),
as well as the Heptagon language developed by \INRIAshort. 
There are many implementations of such high-level models, both in
industry and in academia. Industrial tools such as Simulink
Real-Time (\url{https://www.mathworks.com/products/simulink-real-time.html})
or Scade Studio allow the synthesis of real-time embeddable code
directly from high-level specifications.
%
Academic solutions usually go further, tackling optimisation problems
involved in deriving implementations that satisfy complex
extra-functional requirements. Of particular interest for \TheProject
is previous work on multiprocessor, real-time and fault-tolerant
implementations. A major issue for parallel implementations of
data-flow specifications has been the automatic synthesis of 
efficient communication and synchronisation protocols~\cite{syndex,ocrep,weakendo}.
%
%Multi-processor/distributed/multi-core parallel
%implementation of data-flow specifications has been a major research
%topic for some time now. One major issue here has been the automatic
%synthesis of efficient/optimal communication and synchronization
%protocols \cite{syndex,ocrep,weakendo} when the allocation of
%computations to processors is fixed.
%
%The allocation and scheduling problems are usually considered in a
%real-time context that offers the metrics allowing the definition of
%the optimization problem. Metrics are usually classical concepts such
%as latency, throughput, or power consumption\footnote{Or less-known
%  ones, such as the number of preemptions.} and the objectives can be
%the respect of requirements, the optimization of some metric, or a
%combination of both \cite{lopht1,lopht2,lopht3}.
The resource allocation problem is usually considered in a real-time
context, using metrics (such as latency, throughput, power consumption
and the number of preemptions) to define the (possibly multiobjective)
optimisation problem~\cite{lopht1,lopht3}.
%The allocation and scheduling problems are usually considered in a
%real-time context that offers the metrics allowing the definition of
%the optimization problem. Metrics are usually classical concepts such
%as latency, throughput, or power consumption\footnote{Or less-known
%  ones, such as the number of preemptions.} and the objectives can be
%the respect of requirements, the optimization of some metric, or a
%combination of both \cite{lopht1,lopht2,lopht3}.
%
Among the non-real-time requirements, fault tolerance and criticality
(a measure of how important a given component is for the correct
functioning of a system) are most important ones. It must be ensured
that more critical parts will be the last ones to break, which 
involves a mix of design methodologies to i) increase assurance
levels for more critical parts (according to industry standards such
as ISO 26262, DO178C, or Common Criteria); and, ii) ensure fault
isolation and containment between components of different
criticality, by means of space/time partitioning. Fault tolerance
extends these requirements by demanding that no single point of
failure exists. Previous work by \INRIA teams cover allocation, 
scheduling, and the synthesis of communication protocols under 
partitioning constraints \cite{lopht1}, automatic
synthesis of fault containment protocols for the isolation of
components of different criticality \cite{erts2}, or the automatic
introduction of redundancy to ensure desired system-level reliability
\cite{tolerant}.
%
%% Among the non-real-time requirements considered as part of allocation
%% and scheduling problems, several are of particular interest for
%% Rephorm. Previous work of the partners and elsewhere has considered
%% two related concepts: fault tolerance and criticality. Criticality is
%% a measure of how important a given component is for the correct
%% functioning of a system. To ensure graceful degradation, it must be
%% ensured that more critical parts of a system break last. Ensuring this
%% involves a mix of design methodologies meant to (1) increase assurance
%% levels for more critical parts (according to industry standards such
%% as ISO 26262, DO178C, or Common Criteria) and (2) ensure fault
%% isolation and containment between components of different
%% criticalities, by means of space/time partitioning. Fault tolerance
%% extends these requirements by demanding that no single point of
%% failure exists. Previous work by the project partners and their teams
%% include allocation, scheduling, and the synthesis of communication
%% protocols under partitioning constraints \cite{lopht1}, automatic
%% synthesis of fault containment protocols for the isolation of
%% components of different criticalities \cite{erts2}, or the automatic
%% introduction of redundancy to ensure desired system-level reliability
%% \cite{tolerant}.
%
This work covers a range of target architectures, ranging from bare 
metal homogeneous and heterogeneous multi-cores\cite{erts2,lopht3} 
to architectures featuring complex
RTOSs and interconnect providing real-time configurability
capabilities \cite{lopht1}.
%% This work by the partners has covered a large variety of target
%% architectures, ranging from bare metal heterogeneous or homogeneous
%% multi-cores\cite{erts2,lopht3} to architectures featuring complex
%% RTOSs and interconnect providing real-time configurability
%% capabilities \cite{lopht1}.

%% All these advances: extensions of data-flow languages supporting
%% the various transformations.

%% All these advances: extensions of data-flow languages suporting
%% the various transformations.

%% Weak points:
%% - correct-by-construction, not bi-directional, whereas in many
%% cases changes made by engineers at C/C++ level must be
%% made visible at model level
%% - integration with parallelization, through common patterns.
%% Needs on the parallelization side, code refactoring, model as an
%% image of well-formed C code.

%% Other approach: BIP - refinement done neither in data-flow, nor in C/C++ code.


\TheProject{}\ will advance the state-of-the-art in modelling languages in
the following directions:
\begin{itemize}

\item \textit{Novel basic and pattern building blocks for data-intensive applications.}
We will extend the set of building blocks supported by block-diagram modelling systems with building blocks to encapsulate data, communication and computation, together with their extra-functional \emph{properties}. We will also develop novel \emph{pattern building blocks} that will comprise multiple basic building blocks and their communication and synchronisation. These patterns will encapsulate commonly encountered computations, together with associated data management, in the data-intensive applications.


\item \textit{Bi-directional traceability link with the generated C/C++ code.}
  While it is common practice that the C/C++ implementation code is
  obtained (manually or automatically) from functional specification
  formalisms, the reverse traceability link is much less studied. In the
  best cases, the execution/simulation of the implementation can be
  visualised at model level, and implementation errors can be traced
  back to model constructs. In \TheProject, we will go beyond this
  and define interfaces to link the specification model and the 
  generated C++ code, together with parallel patterns and refactorings
  at both levels and thus allowing model-level reasoning on
  implementation-level properties.
%% \item \textit{Bi-directional traceability link with C/C++ level.}
%%   While it is common practice that the C/C++ implementation code is
%%   obtained (manually or automatically) from functional specification
%%   formalisms, the reverse traceability link is less studied. In the
%%   best cases, the execution/simulation of the implementation can be
%%   visualized at model level, and implementation errors can be traced
%%   back to model constructs. Our objective here is to go beyond this,
%%   and allow (most) code refactoring and mapping performed at C/C++
%%   level to be represented through refactoring of the model and
%%   annotation with mapping information. This is only possible through
%%   the definition of well-defined interfaces which must cover not only
%%   the link between the specification model and the C/C++ code, but
%%   also between parallel patterns and refactoring transformations at
%%   both representation levels. Such a strong traceability link should
%%   allow model-level reasoning on implementation-level properties. It
%%   also enables engineers with different expertise access at different
%%   application levels. Artefacts on both levels must be visible.
  
\item \textit{Representation of parallelisation choices.} Modelling
  languages such as \textbf{Simulink} and \textbf{SCADE} follow a data-flow paradigm,
  meaning that specifications are usually highly concurrent. To allow
  efficient parallel implementation, however, model-level concurrency 
  must be refactored to both \emph{discover and expose} the parallelism 
  inside the components (usually written in C/C++) and to \emph{alter
  the parallel structure} by, for example, grouping smaller components 
  to obtain new coarser-grained, sequential components. In \TheProject,
  we will provide mechanisms to represent both pattern parallel structure
  and optional extra-functional, runtime features such as mapping choices 
  \emph{at the model level}. In this way, we will leave to the programmer
  a choice of the level of complexity in the model itself, allowing either
  encapsulating all the parallelism properties in the model or
  delegating some (or most) of them to the compiler/runtime system.
%% \item \textit{Representation of parallelization choices.} Modelling
%%   languages such as Simulink and SCADE follow a data-flow paradigm,
%%   meaning that specifications are usually highly concurrent. To allow
%%   implementation, model-level concurrency must be refactored through:
%%   \begin{itemize}
%%   \item Discovery of parallelism inside model-level components that
%%     are initially opaque, through analysis of the C/C++ business code
%%     implementing the components.
%%   \item Restriction and control of potential parallelism to match the
%%     capabilities of the implementation platform. Typical
%%     transformations here are the grouping of several components
%%     (possibly concurrent) into one sequential task, or the
%%     identification of data-parallel patterns that can be provided
%%     efficient implementation using OpenCL.
%%   \end{itemize}
%%   Our focus here will be less on the refactoring methods and
%%   algorithms, and more on the model constructs allowing the
%%   representation of the parallel patterns and mapping choices
%%   involved.
  
%% \item \textit{Representation of mapping choices.} We want to allow the
%%   representation, at model level, of most mapping choices. In
%%   particular, we want to represent all static allocation and
%%   scheduling choices, so that the model contains all the information
%%   needed to generate the implementation. Implementation correctness
%%   reasoning can then be performed, at least in part, at model level.
  
\item \textit{Support for mixed criticality systems with mixed
static and dynamic implementations.} Complex embedded
  control systems usually have mixed criticality, containing both 
  safety-critical and non-critical parts. \TheProject will 
  allow sharing of resources by these two kinds of parts,
  ensuring in the process that lower criticality parts can 
  never affect the behavior of a higher criticality ones.
  In order to achieve this, we will develop both static and dynamic
  implementations of the model components and parallel constructs
  (such as patterns), allowing less optimal and flexible but more
  time-predictable (static) parts to be mixed with highly optimised
  but possibly unpredictable (dynamic) ones.
%% \item \textit{Support for mixed criticality systems.} Complex embedded
%%   control systems are usually mixed criticality systems, containing
%%   applications of different criticalities, ranging from
%%   safety-critical to non-critical. These application need to share
%%   platform resources in such a way as to ensure that a lower
%%   criticality application can never affect the behavior of a higher
%%   criticality one. When two applications of different criticality
%%   communicate, special interface components are needed to ensure the
%%   (space/time) isolation between them.
  
%% \item \textit{Support for mixed static/dynamic implementations.} For
%%   modelling languages such as Simulink and Scade both static and
%%   dynamic code generation schemes have been studied. Code generation
%%   for safety-critical avionics applications has mostly followed static
%%   code generation approaches. We are interested here in mixed
%%   criticality applications where applications of different
%%   criticalities use different execution mechanisms, {\em e.g.} static
%%   for the safety-critical applications and dynamic for the
%%   lower-criticality ones. To match the complex needs of case studies,
%%   we will study varying guarantees in terms of safety and
%%   predictability, depending on implementation type.

\end{itemize}


\subsubsection{Software-Defined Infrastructures}
\label{sect:SDIs}

Providing proper level of scalability and fault tolerance requires modern systems to be designed and executed using distributed architectures. This is mandatory if the system is to survive hardware failures. Real-time or on-demand scalability is also based on horizontal scaling which implies designing the system as a set of cooperating services/components which are not embedded into a single process and do not have to work on a single node. Designing and building such distributed, scalable and reliable systems is inevitably burdened with additional effort. First of all, the problem has to be split into individual elements, able of running in parallel, which then have to be integrated with each other with proper interfaces and protocols. Secondly, the system has to be properly deployed on available hardware in order to use its capabilities efficiently. The second aspect has received far less attention over the last decades. Currently it is constantly gains importance due to growing scale and complexity of contemporary systems. The problem of proper deployment of system elements is closely related to the problem of understanding its dynamics. It turns out that in many cases the assumptions of the design are far from the features of running system. Bottlenecks and SPOFs can emerge in unexpected fragments in reaction to incoming traffic or character of processed data. Different approaches to the problem are being used, including advanced monitoring \cite{ Akkoorath:2016}, intuition engineering \cite{vizceral} or chaos engineering approach \cite{basiri2016chaos}.
%
The approach called \emph{Software-Defined Infrastructures} (SDIs) can become a successful  solution to the problems of distributed applications deployment and maintenance. The general concept of SDI is to declaratively define the requirements of the system components. This allows automated selection of proper environment for the system and also its automated modification on runtime. 

\paragraph{The Erlang Programming Language.} Erlang is a strict, impure, functional programming language with support for first-class concurrency. It was developed in the 1980s by Ericsson with the specific goal of easing the programming of distributed systems by providing suitable abstractions to model \emph{processes} that exchange messages. It provides explicit message passing, but is implicit in terms of task placement and synchronisation. Additionally, it proves transparent fault-tolerance support, with automatic checkpointing and restarting of processes that fail, as well as foreign-function interface to execute components written in other languages. It has also been shown that Erlang achieves good scalability to large-scale distributed systems~\cite{release}. This all makes it an ideal candidate as a target language for software-defined infrastructures, since the Erlang abstractions will ensure that the distributed code is highly portable and easily scalable. 

Our ambition within the \TheProject{} project is to advance the state-of-the-art in SDI solutions in the following directions: 
\begin{itemize}
% VJ: I merged the two contributions into one
\item \emph{Develop a new visual language for defining SDIs.}  This language will allow representation of components of hardware system, together with their dependencies, in the form of a diagram. This will allow seamless integration of the process of defining software-defined infrastructures into the overall methodology for block-diagram modelling, since both the applications and the target hardware systems will be modelled using similar visual methods. 
	
\item \emph{Implement novel SDI Composer for generating SDIs based on their visual representation.} The SDI Composer will be implemented using the Erlang technology \cite{Cesarini:2009}. This provides several key features required for the SDIs, including autonomous processes model for massive concurrency, native distribution mechanisms with transparent inter-node communication, transparent fault handling and code updates performed on runtime. 
%Together with several libraries and tools for distributed databases management and advanced messaging it forms a very good basis prototyping a novel approach to the distributed systems deployment. 

%\item \emph{Develop new methods for monitoring the execution of distributed applications.} These methods will allow specifying parts of the system/application that shall be monitored using the same visual language that is used for specifying software-defined infrastructures. This will allow us to monitor the system in real-time and display the relevant information about it in visual and human-understandable way. In addition, the monitoring mechanisms will support collection of different kinds of runtime metrics and will feed information to the \TheProject{} multi-objective optimisation framework.
\end {itemize} 


The SDI Composer will be responsible for deploying the application on a suitable hardware.  The hardware will be explicitly defined in the model or defined as available for the Composer. In the second case, the Composer will inspect the available hardware and analyse the model in order to determine the possibility of running the application. It will generate possible variants of deployment, which can become the subject for optimization. The hardware available for the Composer will require only basic configuration. A lightweight process of the Composer will be run on each computing node. It will allow incorporating vast majority of existing computing hardware, from efficient cluster nodes to small embedded, ARM-based devices. The Composer will also accept nodes, which will not host the component, provided that other means of starting and monitoring of managed processes is provided.  


\subsubsection{Static and Dynamic Code Analysis}
\label{sect:codeAnalysis}

\paragraph{Compliance to Standards}
In safety related industries, compliance to the coding standards is
of utmost importance, as it guarantees that a large number of problems that are usually
very hard to detect are avoided. These problems include not only safety, but also security problems. For example, MISRA coding standard ({\url{https://www.misra.org.uk}}), which is
widely used in automotive industry, has strict requirements for the use of potentially
hazardous constructs such as global variables and pointers, limiting the way in
which these can be use but also avoiding common problems related to e.g.~pointer arithmetic. 
The latest MISRA amendment specifically addresses security issues.
Compliance to standards is even more important if we adopt model-based approach with
generation of the target C/C++ code, as we have to ensure that the generated code conforms
to the desired standards. At the same time, we have to follow the standard practice in industry,
which is that once the target code is generated by the tool-chain, it cannot be modified further. 
Industry standards assume that auto-generated code is compliant to the industry coding 
standard. In reality, this is usually not the case. While it is theoretically possible to 
use static analysis tools for analysis of code 
compliance, such as the \PRLshort QA-C and QA-C++, on the 
generated code, there is no way to reflect back the results of these checks back to the original model
from which the code was generated. This would have a significant impact on the automatic
code generators, as it would allow generation of certifiable code, compliant to
whatever industry standard we are interested in. This is completely new concept that need to be developed during this project.

%\paragraph{Pattern Discovery}
%There has been some research on analysis of the application codez
%to discover which parts are amenable to
%parallelisation and what parallel patterns could be applied
%there. Hammacher et al.~\cite{Hammacher} propose a tool for detecting
%%dependencies in Java applications, using dynamic dependency graphs,
%and suggesting hotspots for parallelisation. Ferenc et
%al.~\cite{ferenc2005design} describe a code analysis based approach to
%detect general patterns, and enhance this by Machine Learning based
%discrimination between closely related patterns and filtering of false
%candidates. Molitorisz~\cite{Molitorisz} describes a tool for
%automatic discovery of parts of the sequential application code that
%contain master-worker and pipeline patterns, and the refactoring and
%performance tuning of these parts to introduce
%parallelism. There is also some work on dynamic analysis mechanisms
%for detecting generic parallel patterns, using profile-guided 
%parallelisation~\cite{franke2}.
%In the \ParaPhrase project, we have developed
%a source-code analysis tool that can discover instances of
%simple parallel patterns, such as task farms, in Erlang applications~\cite{erlangPatternDisc}.
%In the \RePhrase project, we have developed mechanisms for discovering
%pipleine parallelism in the sequential C++ code~\cite{rephrasePatternDisc}

\vjcomment{We need to mention tools from ParaPhrase, RePhrase and ParaFormance.}

\begin{figure}
\begin{center}
\begin{tabular}{ll}
%\hspace{-0.75in}
\includeimage[scale=0.5]{ParaFormance.jpg}
%\vspace{-25mm}
\end{tabular}
\caption{\paraformance Safety Checking.}
\label{fig:paraformance}
\end{center}
\end{figure}

\paragraph{Safety Checking}
There has been some work on using static analysis to discover
parallelism bottlenecks, and provide help to the programmer to reshape
the program in accordance with such analysis. Most previous work has
been done in the context of object-oriented concurrent programs,
within the X10 project~\cite{x10extract}, and in
Java~\cite{reentrancy}. These aim at introducing concurrency and
specifically trying to provide thread safety and/or
re-entrancy. However, the simple special cases considered are not
integrated into a general approach, and it is unclear how much they
work in practice.  Moving away from concurrency, some older
work~\cite{prog-families} aims to classify (sequential) programs at
the software design level into families of programs, and then shape
them into the closest of such families. This has been extended, for
example, to shape programs for pervasive programming
applications~\cite{Sadjadi:05}. In the \paraphrase{} project, we have
developed several program shaping transformations for Erlang programs,
such as transforming data representation between lists, binary data
and hash tables~\cite{erlangShaping}. In the \rephrase project, we improved
on the state-of-the-art mechanisms for detecting race conditions, with
a special focus on lock free queues, by developing techniques to eliminate
some false positives from the commonly used race detectors, such as
Helgrind~\cite{rephraseRaceDetection}. This work has been incorporated
into the Eclipse development platform as a part of the \textbf{ParaFormance}
project (\textbf{Figure~\ref{fig:paraformance}}).



\TheProject will advance the state-of-the-art in static and dynamic 
code analysis in several ways:
\begin{itemize}
\item \emph{Compliance of the code generated from the high-level models.}
As we have mentioned before, there is currently no way to ensure that
the code generated from the block-diagram modelling languages 
is compliant to the standards
used in specific industries that we are targeting. In \TheProject, we will
develop new static analysis feedback mechanisms for the code generated by \textbf{Simulink}
and \textbf{Heptagon} code generators, that will ensure compliance to \textbf{MISRA} and other
relevant standards, thus enhancing these code generators and adding 
additional level of confidence in the code that they produce.

\item \emph{Reflecting the results of code compliance analysis to the
high-level application models.} One of the main novelties of \TheProject
from the modelling perspective will be development of mechanisms to 
reflect the results of the compliance analysis of the target code
detected issues and proposed fixes, back to the original model from which
the code was generated. This will enable us to suggest changes to the
model itself that will make the generated code compliant to the standards,
without directly modifying the generated code.% (which is not allowed by industries standards). 

%\item \emph{Discovery of domain-specific patterns.} Most of the state-of-the-art
%work is focused on detecting generic parallel patterns, such as farms and 
%pipelines. In \TheProject, we will extend the pattern discovery 
%mechanisms, combining static and dynamic mechanisms, to also cover more 
%abstract, domain-specific patterns that will be developed over the course 
%of the project. %We will also develop novel methods to combine static and 
%dynamic pattern discovery mechanisms, using advantages of both of these 
%approaches to discover a wider range
%of parallel patterns. 

\item \emph{Discovery of safety violations in distributed applications.} To the best of our knowledge, there are currently no tools that can discover race conditions in distributed applications, such as the ones that will be targeted by \TheProject{}. We will, therefore, extend our current tooling for detecting such conditions to cover also applications distributed over multiple computing nodes that do not share memory. This will require development of completely new types of analysis compared to the existing state-of-the-art.

\item \emph{Automatic repairing of certain conditions that can prevent 
correct parallelisation.} So far, most of the research in safety checking 
was focused on detecting race conditions in parallel code. There is, however, 
not much work done in \emph{automatic elimination} of these conditions. 
While not all conditions that lead to violation of safety of both parallel and distributed  code 
can be automatically repaired, \TheProject will identify large classes of problems, 
such as using global counters for indexing into arrays, that appear frequently
in sequential code and develop novel analysis mechanisms that both detect and
automatically repair these problems.
\end{itemize}


\subsubsection{Runtime Systems for Heterogeneous Distributed Systems}
\label{sect:compilers}

%The overall aim and ambition for the runtime systems targeted is by \TheProject{} is to enable a non-intrusive way for existing large-scale applications to easily adapt at run-time to execute efficiently on new heterogeneous platforms (with various types of accelerators) and to facilitate a distributed execution. To achieve this aim we'll apply advanced optimisations at run-time (inside the SYCL runtime - existing code can be transformed into a form (SPIR, PTX, GCN, SPIRV etc) that can be managed by the SYCL runtime) to facilitate efficient execution on the platforms present during execution. The distributed runtime will enhance the open SYCL standard by enabling existing programs to run on different nodes in the cloud. The ambitions for this project is to lay the groundwork to include distributed execution in the SYCL standard and open up new research on SYCL-based distributed programming models.

There have been numerous efforts to democratise parallel programming 
based on the popular C++ language and its Standard Template Library (STL). 
Backed by an important community effort for more than five years, SYCL is a Khronos specification for heterogeneous computing built on top of OpenCL and C++. SYCL 1.2 was released in May 2015, and the current SYCL 1.2.1 specification was published in November 2017. With these specifications available, Codeplay is releasing their ComputeCpp Community Edition. Other implementations of SYCL exists from Xilinx for FPGA.
Many other heterogeneous programming models also exist, including HPX, KoKKos, Raja, C++AMP, HCC, Boost.Compute, and CUDA. The C++11/14/17 International Standards have introduced new tools for parallel programming, and the ongoing standardisation effort is developing additional features that will enable support for heterogeneous and distributed parallelism into ISO C++ 20/23. These include executors, futures, affinity, co-routines, and networking support. 

% C++ AMP standard~\cite{AMP} extends C++ with explicit data-parallel 
% construct and \texttt{array\_views} which provide functions for memory 
% transfers. However, C++AMP requires the programmer to use wrappers 
% for each memory allocation and to use the aforementioned views instead 
% of the original C++ data types for memory synchronization.
%VJ - not relevant if we don't consider heterogeneity
%SYCL~\cite{sycl} and Concord~\cite{concord} are recently developed 
%high-level interfaces that integrate GPU programming into C++.
%However, SYCL still requires explicit memory management using buffers.
%Concord uses the advanced shared virtual memory (SVM) features from OpenCL 2.0 
%to provide a transparent memory handling, limiting portability and excluding,
%for example, Nvidia GPUs.
%
The next major C++ standard, C++17 which has just been ratified includes support for
parallel patterns in the form of Parallel Standard Template Library~\cite{parallelSTL} which enables Parallel Algorithms  from STL such as important parallel patterns such as reduce and transform as well as new ones such as parallel\_for\_each. There are also execution policies that enable parallel or parallel with vectorisation. 
Parallel STL is implemented by SYCL and many others. 
First implementations of the parallel STL are provided by Microsoft~\cite{CodePlexPSTL} 
and in ParallelSTL~\cite{ParallelSTLTLutz}. However, they are still very restricted
in terms of the threading model and architectures that they support. For example, the C++ Standard ParallelSTL is only defined to support CPUs as there is no concept of off-node memory as yet in the current C++ Standard, though several Study Groups are working on adding distributed and heterogeneous computing within ISO C++. The Codeplay SYCL implementation  is called ComputeCPP and it enables support for distributing to accelerators and demonstrates speedup compared to pure CPU. This proposal will enhance it for distributed nodes, and also add that as a possible proposal for enhancement both for the C++ and Khronos SYCL Standards.

% Just-in-time compilation (JIT) is an approach where the binary code is produced during the execution of the application, at the point where certain functionality of the application (e.g.~some computation kernel) is invoked. It usually involves multiple stages of compilation, producing possibly several version of the intermediate code before the final binary version of the function or component is produced. It was pioneered by Taha~\cite{MSP}, first introduced in functional languages MetaML~\cite{MetaML} and MetaOcaML~\cite{MetaOcaML} and then made available in Mint~\cite{Mint} for Java, LMS~\cite{LMS} for Scala, and Terra~\cite{Terra} for Lua~\cite{Lua}. Multi-staged compilation offers large potential for various runtime optimisations of the code. Some recent approaches, such as LambdaJIT~\cite{LambdaJIT} support just-in-time compilation and multi-staging for C++ code. 
%LambdaJIT~\cite{LambdaJIT}, for
%example, support JIT compilation of the GPU code, which is generated from
%C++ lambda expressions used in the algorithms from the standard C++ library.
%However, this approach has limitations, mostly in terms of capturing variables
%by lambda expressions, limiting staging of the lambda expressions. 
% The Lightweight Modular Staging~\cite{LMS} framework for Scala is built on top of the Delite~\cite{Delite} project which simplifies the development of domain-specific languages (DSL) for parallel processors. Multi-staging can be used in DSLs to target specific accelerators, such as GPUs. 
%generate efficient GPU code. Delite is, however, limited to the developed DSLs
%and only supports generation of the CUDA code.


%Large potential  in optimizing programs is promised by JIT and multi-staging, but they are still not used in many-core programming. Multi-stage programming was pioneered by Taha~\cite{MSP}, first introduced in functional languages MetaML~\cite{MetaML} and MetaOcaML~\cite{MetaOcaML} and then made available in Mint~\cite{Mint} for Java, LMS~\cite{LMS} for Scala, and Terra~\cite{Terra} for Lua~\cite{Lua}. 


%in C++17 ??? the next envisaged major C++ standard. This Parallel Standard Template Library (parallel STL) proposal [? ], developed by NVIDIA, Microsoft and Intel, aims to add parallel implementations to the C++ Standard Template Library for patterns which have been part of the C++ standard for more than two decades. Large potential  in optimizing programs is promised by JIT and multi-staging, but they are still not used in many-core programming. Multi-stage programming was pioneered by Taha~\cite{MSP}, first introduced in functional languages MetaML~\cite{MetaML} and MetaOcaML~\cite{MetaOcaML} and then made available in Mint~\cite{Mint} for Java, LMS~\cite{LMS} for Scala, and Terra~\cite{Terra} for Lua~\cite{Lua}. 









%% \end{itemize}

\paragraph{Monitoring of Parallel Applications}
Performance monitoring is very important for tuning and 
selecting implementation variants, and especially for multi-objective
optimisations that will be developed in \TheProject. Several tools 
exist that support the collection of runtime information and use 
of this information to improve runtime predictions. The most widely 
used are \emph{Open$\vert$SpeedShop}~\cite{schulz2008open}, 
\emph{TAU/ParaProf}~\cite{shende2006tau},
and \emph{VampirTrace}~\cite{muller2007developing}. 
Performance information is often collected into repositories of 
performance profiling data and models, e.g. by
\emph{PerfDMF}~\cite{huck2005design} and 
\emph{PerfTrack}~\cite{karavanic2005integrating}. 
There also exist several libraries that allow collection of runtime
information such as data accesses, cache behaviour and performance
of computational cores~\cite{papi,pmlib}. This libraries can be used
as a basis for tools for monitoring application behaviour and
detecting any anomalies in terms of performance or energy consumption.
, and \emph{Prophesy} \cite{wu2001design}.
As part of the \paraphrase project, we have previously developed a
set of protocols and tools for collecting performance and profiling
information for patterned applications~\cite{vip},
and as part of \advanceproject, we developed a statistical framework
for predicting performance of streaming applications.
This work will be generally exploitable in \TheProject{}.

\TheProject{} will advance the state-of-the-art of runtime system and monitoring for parallel and distributed runtime system in several directions:

\begin{itemize}
\item \emph{Novel runtime optimisations of the binary code.} We will extend the SYCL runtime system to support transformation of the existing code into different forms (SPIR, PTX, GCN, SPIRV etc.), facilitating efficient execution on the code on the target platforms during execution.
\item \emph{Extend the SYCL runtime system to target distributed systems}. We will enhance the open SYCL standard by enabling existing programs to run on different nodes in the cloud and other kind of distributed system. The ambitions for this project is to lay the groundwork to include distributed execution in the SYCL standard and open up new research on SYCL-based distributed programming models. This would enable SYCL to target a very wide range of systems in a portable way, from single-node CPU-only systems, to single-node heterogeneous systems combining possibly several different types of accelerators to, finally, distributed heterogeneous systems.

\item \emph{Develop new methods for monitoring the execution of distributed applications.} These methods will allow specifying parts of the system/application that shall be monitored using the same visual language that is used for specifying software-defined infrastructures. This will allow us to monitor the system in real-time and display the relevant information about it in visual and human-understandable way. In addition, the monitoring mechanisms will support collection of different kinds of runtime metrics and will feed information to the \TheProject{} multi-objective optimisation framework.
\end{itemize}


%The overall aim and ambition for the runtime systems targeted is by \TheProject{} is to enable a non-intrusive way for existing large-scale applications to easily adapt at run-time to execute efficiently on new heterogeneous platforms (with various types of accelerators) and to facilitate a distributed execution. To achieve this aim we'll apply advanced optimisations at run-time (inside the SYCL runtime - existing code can be transformed into a form (SPIR, PTX, GCN, SPIRV etc) that can be managed by the SYCL runtime) to facilitate efficient execution on the platforms present during execution. The distributed runtime will enhance the open SYCL standard by enabling existing programs to run on different nodes in the cloud. The ambitions for this project is to lay the groundwork to include distributed execution in the SYCL standard and open up new research on SYCL-based distributed programming models.

\subsubsection{Static, Dynamic and Decentralised Optimisation}
\label{sect:optimisation}

During the lifetime of an application, target platforms on which it is executed as well as extra-functional requirements, e.g., in terms of performance, energy consumption and QoS specifications, might change. Rewriting and recompiling the application separately for each
combination of target platforms and metrics is infeasible. We need mechanisms that enable applications to \emph{adapt} to the changing requirements and to derive an optimal configuration with respect to a given metric, or a combination of possibly 
conflicting metrics. Many applications have extra-functional parameters that can be dynamically changed without altering the functionality of the application itself. Such parameters may include \emph{problem-specific parameters} (e.g., QoS requirements and service-levels), \emph{parallelisation-specific parameters} (e.g., parallelisation structure) and \emph{runtime-specific parameters} (e.g., scheduling and mapping parameters). Optimal tuning of these parameters requires expert knowledge of the internal properties of the application, as well as the characteristics of the available hardware. 
To this end, the goal of \TheProject is to provide an infrastructure that automatically 
adjusts the application- and runtime-parameters both offline (statically) and online (dynamically), in order to optimise a (possibly multi-objective) optimisation criterion. 

% This objective is mostly related to the area of scheduling and resource management for parallelized applications. Below, we provide a classification of related research directions that touch on our objective spanning from a coarse-scale optimisation methodologies towards more fine-scale frameworks (either static or dynamic). % In particular, we classify the related literature to a) \emph{Coarse-scale and hierarchical resource management}, b) \emph{Parallel-Task-Graph Scheduling}, c) \emph{Application-parameters adjustment}, and d) \emph{Thread- and memory locality optimisation}. 

\paragraph{Coarse-grained resource management.}
Optimising resource management in many-core systems is a necessary task for guaranteeing an {\it efficient} performance over a large number of applications running on the same platform. Resource adaptivity of parallel application has been studied at different levels. 
\emph{Coarse-grained tuning} of application resources, such as the number of processors on which the application is executed, has been studied in context 
computational grids and clouds~\cite{gridSchedOverview,gridSelfAdaptive,DBLP:conf/pdp/AldinucciCDVKDLT08}, %\cite{adaptiveApp}
and it has been shown to have benefits on performance, thermal management, reliability and fault tolerance~\cite{gotzinger_role_2016,singh_survey_2017}. When the number of tasks increases, the necessary computational overhead to efficiently map these tasks becomes extremely complex, since a multi-criteria objective might be needed (e.g., heat distribution on the chip \cite{matsumoto_investigations_2010}, mixed-criticality \cite{saraswat_task_2009}). To this end and mostly in the context of task-graph scheduling, there have been several attempts towards \emph{decentralisation} of the problem of optimising resources (as in the Lagrangian decomposition of \cite{wildermann_multi-objective_2014} or the hierarchical optimisation of \cite{quan_hierarchical_2016}). Both approaches assume a-priori knowledge of the performance function of the optimised applications, and online measurements/prediction updates are not considered. The \TheProject{} team has developed decentralised optimisation schemes for coarse-grain resource allocation when only performance measurements are available \cite{chasparis_design_2016,MagECRTS,chasparis_reinforcement-learning-based_2015}.


\paragraph{Fine-grained resource management.}
\emph{Fine-grained tuning}, within a single processor chip, has been studied in context of task ordering and task mapping to reduce the application makespan~\cite{wang_task_1997}.
Most of the existing approaches are static in nature, e.g.~model-based approach that uses a precedence graph to optimise mapping of tasks to resources~\cite{zhang_joint_2016}, simulated annealing approach in~\cite{marcon_comparison_2008} and integer-linear-programming in~\cite{javaid_design_2009}. More recent work has moved towards \emph{dynamic scheduling} techniques, where placement decisions are made at runtime. For example, Klug et al.~\cite{Klug11} describe a tool that checks the performance of each of the available thread-to-core bindings and searches for an optimal placement, employing possibly prohibitively expensive exhaustive search. Task migration has also been recently introduced in the on-chip context as an effective way to balance different system characteristics at 
runtime \cite{holmbacka_task_2014}. Broquedis et al.~\cite{Broquedis10} combine 
the problem of thread scheduling with scheduling hints related to thread-memory affinity issues. While the main
concern for these approaches was performance, additional objectives can also be
incorporated, e.g.~fault tolerance and 
reliability~\cite{wang_optimizing_2011,matsumoto_investigations_2010,saraswat_task_2009}.
There are, however, several issues with this line of work including difficulties in retrieving exact affinity relations at runtime (i.e., information complexity), the need for expensive runtime monitoring, failure to consider irregular application behaviour and the coarse granularity of rescheduling decisions. Instead, learning-based optimisation techniques, using ongoing performance measurements, seem more appropriate for managing information complexity and for managing uncertainty. The \TheProject{} team has developed relevant learning-based techniques in the context of allocating threads and memory in many-core systems~\cite{chasparis_efficient_2017} (\emph{PaRL-Sched} scheduler). % and in mapping to heterogeneous architectures~\cite{cec2014}.


\paragraph{Application-parameters adjustment.} 

The area of a combined adjustment of both resource allocation parameters (above) as well as application parameters is rather young. The need for such combined optimisation becomes evident when the number of tasks and the availability of resources vary over time. This is already recognised in real-time applications \cite{Bini11}, and in cloud computing \cite{guerout_mixed_2017}. Given the need for meeting (possibly hard) QoS specifications, dynamic monitoring of the application performance metrics and optimisation of its  parameters is inevitable. An example of a combined use of optimisation and feedback was developed by \cite{Arz11,Bini11} however a look-up table is required that provides the QoS achieved at different amount of CPU resources. The need for \emph{efficient} online adjustment of both QoS parameters and (coarse-level) resources has led to a novel learning-based methodology in \cite{chasparis_design_2016,MagECRTS,chasparis_reinforcement-learning-based_2015} developed by the \TheProject team. 

\paragraph{Multi-objective optimisation.} When combining resource-management adjustment criteria with application-specific requirements, a set of possibly conflicting criteria emerges (e.g., energy efficiency and reliability in heterogeneous 
systems~\cite{wang_optimizing_2011,zhang_joint_2016}). To this end, recently there have been several efforts towards \emph{multi-objective optimisation} approaches. Due to the fact that the aforementioned resource management problems constitute NP-hard 
combinatorial problems \cite{garey_computers_1979}, metaheuristic approaches 
(such as stochastic-local-search methods \cite{Hoos05} or genetic algorithms) 
are often utilised. For example, an approximate algorithm has been proposed in \cite{wu_new_2006} for solving fuzzy multi-objective linear programming problems with fuzzy parameters in both objective functions and constraints, and a decision support system (DSS) has been developed based on this algorithm. Focusing on metaheuristics, we could see that a number of efforts have been made to adapt the artificial bee colony (ABC) algorithm for multi-objective optimization problems. For example, in \cite{akbari_multi-objective_2012}, a multi-objective artificial bee colony (MOABC) algorithm is proposed, which features a grid-based approach to adaptively assess the Pareto front maintained in an external archive. The \TheProject team has experience in the design of metaheuristics for multi-objective NP-hard combinatorial problems (such as bin-packing and cutting-stock problems) \cite{chasparis_evolutionary_2017}. 
% 
\TheProject will advance the state-of-the-art in multi-objective optimisation of applications in the following directions:
\begin{itemize}
\item \emph{Hierarchical Resource Optimisation.} 
\TheProject will develop hierarchical optimisation algorithms that will operate on two levels. On the \emph{coordination} level, we will develop mechanisms for coarse-grained adaptivity of the resources on which the application is executed, taking into account precedence constraints imposed by components and the patterns that comprise the application. On the \emph{component} level, we will tune the thread and data mapping within the components to the given goal.

%We distinguish between the following optimization layers: a) \emph{Higher/Coordination-level optimisation}, and b) \emph{Lower/Component-level optimisation}. In the first (higher) optimisation level, we are concerned with a coarse-scale adaptivity of the available resources in the required components by also taking into account possible precedence constraints imposed by the component interrelation and the coordination parallelization pattern. Decisions concern both application and parallelization parameters (e.g., QoS requirements, parallel pattern) and runtime resource allocation (e.g., mapping of resources, migration, data locality). 

\item \emph{Combined application- and runtime-parameters optimisation.} 
\textbf{}The above described multi-objective optimisation approaches ignore the potential benefits of the combined adjustment of both application and runtime parameters. 
\TheProject will exploit feedback information from the monitoring 
infrastructure to guide the dynamic adjustment of both types of parameters 
through a measurement- or learning-based approach. We
already have experience in the development of such multi-dimensional optimisations 
\cite{chasparis_design_2016,chasparis_reinforcement-learning-based_2015},
which will be extended in \TheProject{} to also consider multi-objective 
optimisation criteria.

%The aforementioned literature in parallel-task-scheduling ignores the potential benefit of the combined adjustment of both application (e.g., parallelisation features) and runtime parameters (e.g., mappings). In particular, we will exploit feedback information from monitoring to guide the dynamic adjustment of both types of parameters through a measurement- or learning-based approach. The proposal team has experience in the development of such multi-dimensional optimisation as demonstrated in the combined QoS- and resource-adjustment process for real-time applications in \cite{chasparis_design_2016} and generalised tasks in \cite{chasparis_reinforcement-learning-based_2015}. The \TheProject{} team will advance these techniques to also consider multi-objective optimisation criteria.

\item \emph{Novel predictive modelling and static (stochastic-local-search-based) optimisation.} 
\TheProject will advance the state-of-the-art in task scheduling and mapping
parallel-task-scheduling by developing a static optimisation framework
for multi-objective application- and runtime-parameter selection, supported
by a predictive modelling infrastructure. We will utilise stochastic-local-search 
(SLS) methods which are appropriate for such NP-hard combinatorial problems. 
The optimisation will further be supported by advanced machine-learning tools 
for predicting the performance (objective) functions and for minimising complexity 
(through causal-discovery or inference methods). The proposal team has experience in 
the development of both SLS methods~\cite{chasparis_optimization_2016,chasparis_evolutionary_2017}, 
and predictive modelling (as a basis of structural learning) \cite{kosorus_relevance_2013}. % for parallel applications~\cite{cec2014}. 


%We will advance parallel-task-scheduling by developing a static optimisation framework supported by a  predictive modelling infrastructure. The static optimisation addresses application- and runtime-parameter selection under a multi-objective criterion. Given the complexity of the problem we will utilise stochastic-local-search (SLS) methods which are appropriate for such NP-hard combinatorial problems. The optimisation will further be supported by advanced machine-learning tools for predicting the performance (objective) functions and for minimising complexity (through causal-discovery or inference methods). The proposal team has experience in the development of both SLS methods, for addressing real-world resource-allocation problems \cite{chasparis_optimization_2016,chasparis_evolutionary_2017}, and predictive modelling for parallel applications \cite{europar2014}. % in the \textbf{ParaPhrase} project. 

% In the context of predictive modelling, in our previous work in the \textbf{ParaPhrase} project, we have investigated the use of machine-learning methods for discovering near-optimal instantiation of application components (i.e. the number and the type of each component) and their placement~\cite{europar2014}. In \TheProject{}, we will extend and enhance these techniques to also consider data locality and placement. 

\item \emph{Novel Learning-based dynamic and decentralised optimisation.} 
\TheProject will develop novel learning-based optimisation techniques for decentralised optimisation. These techniques are appropriate for managing information complexity (e.g.,~when dealing with a large number of possible thread/memory bindings) and uncertainty (arising, for example, from irregularities of the applications), and have additional benefit in being able to periodically adjust their learned behaviour based on ongoing measurements. We will extend our previous work on task mapping/scheduling~\cite{chasparis_design_2016,chasparis_pinning_applications_2017} 
and distributed convergence to Pareto optimal outcomes~\cite{ChasparisAriShamma13_SIAM,ChasparisShammaRantzer15}
to provide i) combined adjustment of application parameters; ii) 
decentralisation/decomposition of the learning process; and, 
iii) multi-objective optimisation criteria. 

%Learning-based optimisation techniques seem more appropriate for managing information complexity (i.e., when dealing with a large number of possible thread/memory bindings), for managing uncertainty (i.e., irregularities in applications behaviour), and can periodically adjust their learned behaviour based on ongoing measurements. % Such a scheme will periodically evaluate the performance of the parallelised application and provide a mapping of its components to the underlying hardware objects through processing of prior performance measurements. 
%The proposal team has developed relevant learning-based techniques in the context of allocating computing bandwidth among time-sensitive applications~\cite{chasparis_design_2016}, 
%mapping parallelised applications to heterogeneous architectures~\cite{chasparis_pinning_applications_2017,VJ} and distributed convergence to Pareto optimal outcomes \cite{ChasparisAriShamma13_SIAM}. This work will be extended in \TheProject{} to consider a) combined adjustment of application parameters, b) decentralisation/decomposition of the learning process to reduce computational and communication complexity, and c) multi-objective optimisation criteria. 

\end{itemize}

\subsubsection{Security}
\label{sect:security}
Application security is becoming increasingly important in modern computer systems.
Current practice is for security to be ensured at runtime, using efficient monitoring
techniques coupled with dynamic runtime checks such as ISR~\cite{isr}, ASLR~\cite{aslr}
and CFI~\cite{cfi}. These methods are, however, expensive in terms of computational
complexity and energy consumption. As most of the security vulnerabilities come from
code defects, bugs and logic flaws, the most cost-effective way to ensure security is
to follow the secure code best practices~\cite{OWASP} and eliminate vulnerabilities
before code is deployed. This is usually done with static and dynamic code analysers.
Static code checkers, such as Appscan Source~\cite{AppScan} and Coverity~\cite{Coverity}
%and Klocwork~\cite{Klocwork}, 
are based on techniques for quality checking that
have been extended to cover security vulnerabilities in the code. The main drawback of 
these methods is their high rate of false positives they produce. Techniques based on
formal methods, such as model checking and symbolic interpretation, do not report false 
positives, but have potential problem with scaling. AFL~\cite{AFL} uses use fuzz testing 
to dynamically profile the code to expose potential security vulnerabilities. 
None of the above tools includes special algorithms for detecting vulnerabilities 
resulting from parallel execution. Parallelism, due to undeterministic nature of the
application execution, presents many additional problems from the security perspective. These problems have to be dealt with efficiently
in future systems.


Including third party libraries into the application code presents additional
security risks, as these libraries may include intentional or unintentional
vulnerabilities resulting in unwanted behavior or data leakage of sensitive 
information. For example, an open source JavaScript library used in a website 
may contain malicious code that collects data and sends it to the third party. 
%Another example is binary packages which contain vulnerability which is detected by crackers and compromise users??? machines. 
The state of the art solutions to this problem is to use security testing, static 
analysis and dynamic analysis, such as sandboxing~\cite{jsand}, to detect 
vulnerabilities and malicious packages before integrating them into 
the application code. % is proposed to detect malicious JavaScript code, in addition, they 
%suggest wrapping the components to control access to security sensitive operations. 
In~\cite{Cova}, a combination of anomaly detection and emulation systems is built to detect malicious code. OWASP~\cite{OWASP} presents the risks in using third party 
packages, how to determine if these packages are vulnerable or contain malicious code 
and how to deploy these packages. %This problem is not met only in JavaScript packages, it is in many other 3rd party packages, especially, Android 3rd party libraries. 
Other possible solutions to the problem of integrating third party libraries are
based on analysis of the libraries, building control flow graph, looking for 
copies of buggy code, and profiling packages~\cite{Hanna, XinSun}.%~\cite{Hanna, XinSun, Nora, Backes}.
These solutions miss many vulnerabilities or malicious dormant code as a result of 
obfuscation, packing, and the ease in rebuilding new variants of the malicious code, 
or just the rapid release of new buggy version of packages.

There are cases in which it is not beneficial to fix a vulnerability in the 
software or in a third party code integrated into the software. For example, 
in cases where the patch is hard to deploy or where fixing the vulnerability 
has significant impact on performance. In fact, 99\% of attacks are believed to exploit known and fixed vulnerability~\cite{GartnerVulnerability} for which the fix was not deployed. One famous example is the WannaCry attack that was based on a fixed and updated CVE.
To protect the software in these cases,
\emph{virtual patching} may be used. %Virtual patching is a security policy 
%enforcement layer which prevents the exploitation of a known vulnerability. 
A virtual patch is a set of rules that implements complex logic to prevent 
malicious traffic from reaching the application. 
%The virtual patching mechanism will usually be integrated in the organization firewall. 
The challenge is to come up with a set of accurate rules that filter 
exactly the malicious inputs without blocking other inputs. This work is 
currently done manually, and it would heavily benefit from integrating a 
predictive model to automatically detect inputs that may utilize the 
vulnerability.




\TheProject{} will advance the state-of-the-art of security of parallel code in the following directions:
\begin{itemize}
%\item \emph{Novel security code practices for parallel code}. We will extend the best security code practices
 % to better cover parallel programming, while also taking into account performance penalties. We will also
 % prove the effectiveness of the suggested practices.
\item \emph{Incorporating security into code generation algorithms.} We will improve the code generation algorithms
  by incorporating security code practices where applicable and validate the resiliency and security of the
  code after generation. special attention will be given to vulnerable code gadgets that may run with extended privileges. These vulnerable
gadgets may be used to attack the infrastructure using side channel attacks (e.g. Spectre and Meltdown). We may add more
  security checks beyond looking for code vulnerabilities. For example, we may automatically identify inputs
  that may come from untrusted data sources, and add input validation where missing.
\item \emph{Security aware integration with third party code.} We will build tools and methods for detecting
  vulnerable libraries and check that the parallel libraries that we are targeting with implementation of patterns at the C/C++/SYCL level
  are safe. Our methods will look for known blocks of malicious code or vulnerabilities already discovered
  in open source code and will exclude libraries that contain these blocks or similar code from being used.  We will also identify when
a vulnerable version of a library is used and issue an alert to upgrade to an updated version.
%% VJ: put somewhere else  
%In case we detect vulnerabilities in open source during the \TheProject project we will report them to the community to help preventing cyber-attacks on applications which uses vulnerable libraries.
%\item \emph{Tradeoffs Between Security and Performance.} New parallel coding paradigms such as lock free
%  synchronization that enhance performance may introduce more opportunities for malicious attacks.
 % In \TheProject we will explore the trade-offs between security and performance. We will use the multi-objective
 % optimisation techniques together with security tuning mechanisms to achieve the balance between performance,
  %energy consumption and other extra-functional metrics on one side and the level of security on the other side.
\item \emph{Novel Mechanisms for Runtime Protection.} We will develop novel mechanisms for run time protection
  that will be able to mitigate security vulnerabilities left in the code. We will use quality assurance techniques
  for identifying vulnerabilities, such as Fuzz testing and static code analysis, to generate data that can be
  used for creating predictive models for untrusted inputs. These models may be used to filter out malicious behaviours
  when the application is not patched yet or contains vulnerabilities because of performance considerations.
\end{itemize}

%\begin{figure}
\begin{table}
\begin{tabular}{|p{2cm}|c|c|p{4cm}|p{6cm}|}
\hline \hline
\textbf{Use Case} & \textbf{Domain} & \textbf{Data Scale} & \textbf{Target Hardware Systems} & \textbf{Business Requirements} \\
\hline Smart City Monitor & IoT & Very Large & HPC and multicore microcomputers & Reliability, security, performance, compliance to standards \\
\hline ADAS & Automotive & Large & Small-Scale Distributed Systems  & Compliance to standards, performance, security \\
\hline TensorC & AI & Small & Shared-Memory Systems & Performance, performance predictability \\
\hline \hline
\end{tabular}
\caption{Overview of the \TheProject{} Use Cases}
\label{fig:usecases}
\end{table}

%\pagebreak
\subsubsection{Use Cases}
\label{sect:applications}
\label{sect:background-last}

Table~\ref{fig:usecases} gives an overview of the use cases that will be developed over the course of the \TheProject{} project.

\paragraph{Smart City Monitor (\GOLEMshort and \JMOICshort)} is one of the novel large scale IoT applications developed on PharosN platform. It was applied in Horizon 2020 project Smart Urbana (2017): Enabling municipalities in 5 European small and medium cities of five countries with cyber-physical system (CPS) instruments and business models for digital transformation in real time. The system is licensed and running in several cities, including Jelgava (Latvia), to support new smart management of resource efficiency and quality of life in urban areas. Its unique features allow municipalities, local utility providers and citizens obtaining holistic perception of their cities as big and increasingly dynamic systems and access rich set of novel information services online and mobile apps. The Smart City Monitor use case offers demonstration of the project results in a variety of innovative  applications for smart sustainable governance, big data collection from diverse data sources such as buildings, schools, public utilities, transportation services, etc and its transformation into the rich set of urban information services available in real time.   

\begin{figure}
\begin{center}
\begin{tabular}{ll}
%\hspace{-0.75in}
\includeimage[scale=0.5]{Jeglava.jpg}
%\vspace{-25mm}
\end{tabular}
\caption{Smart City Monitor Example.}
\label{fig:smartCityMonitor}
\end{center}
\end{figure}

The complexity of the urban CPS model and its scale and the diversity of optional smart applications and controls require effective reliable and secure high performance software components running in distributed interlinked smart systems within the Smart City Monitor architecture enabling digital transformation of big data streams for visual monitoring, analytics and predictions and operational controls  while supporting relevant compliance with relevant international standards.

However, the implementation of such large scale applications brings the need of using massively parallel hardware and high performing software and faces a number of challenging issues, such as:
\begin{enumerate}[i)]
\item automatic evaluation of each individual process, presented as a data stream, using most effective machine learning algorithms (including its automatic selection depending of the physical nature of the process)
\item effective organisation and parallelisation and optimisation of the real time processing of the dependency chains between different performance indicators that take place upon the receipt of each new data piece in multiple data streams; 
\item addressing computational complexity of the sustainability calculations and predictions for each of the system objects caused by large number of the system components in the urban CPS model;
\item need for real-time response of certain components, e.g. by controlling the performance of actuators in different geographic locations connected over WiFi, LoRa, cellular and cable networks. 
\item high level of system security at each operational level of stakeholders (linkages between system distributed modules, core engines in cloud and on premises, authorised user access from common devices, use of  Open Source SW components, its updating and maintenance, etc).
\end{enumerate}

The \TheProject project directly addresses these issues that are of key importance for this and many other industrial PharosN applications dealing with inherent complexity of real world CPSs. We will apply the \TheProject technology to various PharosN components enabling increased parallel performance, reliability, security, adaptivity to different target HW platforms both in HPC and multicore microcomputers (Raspberry PI and alternatives, Intel Compute Card, etc). The expected results will allow reduced time and costs for system implementation and validation the due to use of high-level block diagrams and reduced energy consumption for running the large scale distributed IoT applications in the cities. The project results will be demonstrated in the concrete examples of Smart City project in Jelgava city (\textbf{Figure~\ref{fig:smartCityMonitor}}) by \JMOICshort{} and \GOLEMshort{}.
\paragraph{TensorC (\CODEPLAY)} Machine learning is a hugely growing discipline with a rapidly increasing number of applications in computer vision, speech recognition, control automation and other areas across different industries and research domains. More and more machine learning- related tasks, applications, designs (in particular various neural network designs) and frameworks are leaving the "lab" and are required to be executed on different types of (heterogeneous), often resource-constrained mobile platforms in the cloud with different performance characteristics which require adaptations or customised versions of the used algorithms and data formats suited for those platforms. To achieve performance the machine learning applications, in particular neural networks, must take advantage of parallelisation and hardware acceleration requiring the code to be easily targeted to graphics processing units (GPUs) and other accelerators.

To address this challenge, in this use case the \TheProject{} tools are used in particular to generate optimised implementations of highly popular convolutional neural networks from higher-level descriptions targeted for specific platforms. The generated neural network models will be benchmarked and their higher-level description updated/extended/improved to better reflect the performance characteristics of the targeted platforms.
\paragraph{Advanced Driver-Assistance Systems - ADAS (\PRshort)}is one of the fast growing systems to help the driver in the driving process. ADAS cameras and sensors required processing of the huge information during very short time. Any mistake in processing this information is safety critical issue. There are two main issues that need to be addresses: (i) To assure that level of the software quality it is necessary for the software to be compliant to the industry coding standards. Currently most of the code for this software is automatically generated and as result is not compliant to the industry standards. Applying \TheProject technology for semi-automatic fixes of the generated code we are planning to achieve required level of compliance. (ii) Processing of the large amount of data in the real time required parallelisation of the data processing from the cameras and sensors - applying parallelization technique that will be developed in \TheProject will address this issue as well. 
This use case will be using real code and model provided by Magna Electronics Inc. (USA) to \PRshort on confidential base. The use of real data will allow us to demonstrate the results of the project on the real software for safety critical industry and will provide a very good show case to promote this technique in the industry.

\subsubsection{Key Technologies Used in the \TheProject{} Project}
\label{sect:key-technologies}

\paragraph{\SCCHshort{} Machine Learning Tools.}
\label{sec:mlpp}
\emph{mlpp} is a C++ library with interfaces to several other languages. It 
is currently under heavy development. So far, it contains algorithms for 
linear regression, time series analysis, feature selection and causal 
dependency discovery. Its focus is on building interpretable models. It is hosted 
on SourceForge (\url{https://sourceforge.net/projects/ml-pp/}), and released 
as Open Source (GPL license); paid licenses for commercial use will be provided
with the first mature release.
%
\emph{mlpp} collects algorithms developed and implemented in data analysis 
projects undertaken by \SCCHshort{} for diverse customers. It will be used
in implementing performance prediction systems in WP4, and extensions necessary
for that will feed back into further development of \emph{mlpp} and thus be
publicly available.

\paragraph{\SCCHshort{} \RePhrase Mapping Framework.} The RePhrase project has developed the \emph{PaRL-Sched} C++ library for Linux operating systems, which provides run-time resource allocation (i.e., computing bandwidth and memory) for parallelized applications reported in a number of publications \cite{chasparis_pinning_applications_2017,chasparis_efficient_2017}. It utilizes novel learning-based optimisation techniques developed by the proposal team \cite{chasparis_stochastic_2017_acc}, and it has demonstrated a significant increase in processing speed in real-world applications (such as the Ant-Colony-Optimisation algorithms) \cite{chasparis_pinning_applications_2017}. It will be used in implementing dynamic resource allocation of hardware resources and it will further be extended in the context of the \TheProject{} project to accommodate decentralised optimisation as well as multi-objective criteria.

\paragraph{\IBM{} Security Vulnerability Detection Technology.}

IBM developed vulnerability detection technologies based both on static and dynamic methods. IBM developed Beam for static analysis of C/C++ code and extended it to look in particular for security vulnerabilities with a low ratio of false positive. In addition, IBM developed the ExpliSAT tool which uses symbolic interpretation for more precise program analysis including checks for security vulnerabilities such as buffer overflow detection. During the \RePhrase{} project, ExpliSAT was extended to analyse parallel programs. To complement the static methods IBM uses also Fuzz testing technology that detects security vulnerabilities dynamically using genetic algorithms. In order to achieve accurate and scalable security vulnerability detection, IBM is now working on combining the static and dynamic analysis vulnerability detection tools.     

\paragraph{\SAshort{} \paraformance Refactoring and Code Analysis Tools.}
\khcomment{VJ/CB to update, please}


The \paraformance Refactoring tool was originally developed for the EU FP7 \ParaPhrase{} project, and later extended and enhanced in both \RePhrase{} and the Scottish Enterprise innovation project, \ParaFormance{}. 
\paraformance is a tool-chain designed to \emph{democratise} parallel programming by allowing software developers to quickly and easily write parallel software. \paraformance enables software developers to find the sources of parallelism within their code, automatically (through user-controlled guidance) through a process of \emph{pattern discovery}. \paraformance also offers refactoring support to allow parallel patterns to be introduced directly into the source code of the application, inserting the parallel business logic. \paraformance also has integrated safety checking features, to not only enable  the parallelised code to be thread-safe, but also the checking of sequential code, eliminating potential sources of parallelism errors that occur, such as race conditions and deadlocks. Finally, \paraformance is able to repair some of the parallelism errors detected by the safety checking to automatically make the application thread safe. \paraformance will be extended in \TheProject{} by adding safety checks for the distributed code, in addition to the existing mechanisms for checking the code that is executed on shared-memory machines. We will also extend the tool with features to check for \emph{security} of the code. 

\paragraph{\INRIA{} Heptagon Language and Compiler.}
Heptagon (\url{http://heptagon.gforge.inria.fr}) is a
synchronous dataflow language, compatible with the Lustre
language and SCADE 6 modelling environment (ANSYS - Esterel
Technologies), with a syntax allowing the expression of 
control structures (e.g., switch or mode automata).
It is also very close to the subset of Simulink/Stateflow 
associated with discrete block-diagram modelling.
Heptagon is also a research compiler, whose aim is to facilitate 
experimentation. The current version of the compiler 
allows expression and compilation of array values with modular
memory optimisation and expression of parallelism and mapping 
annotations. 
Heptagon was initially developed in the Parkas (INRIA/ENS) and Ctrl-A 
(INRIA/LIG) research teams, later joined by the Aoste team (INRIA).

\paragraph{\INRIA{} Lopht mapping and code generation tool.}
Lopht \cite{lopht1,lopht2,lopht3,erts2,dasia} is a tool for the
efficient compilation of systems with complex functional and
non-functional properties. Starting from a dataflow synchronous
functional specification, a description of the target platform, and a
set of non-functional requirements, Lopht produces fully mapped,
correct-by-construction implementations, under the form of
statically-mapped, ready-for-compilation C code including, depending
on the target, gcc attributes and the linker script defining memory
allocation \cite{lopht3}, or the configuration of a real-time
operating system (RTOS) such as VxWorks653 and/or that of specialised
communication networks such as TTEthernet
\cite{dasia,lopht1}. Correctness includes here both functional
aspects, and non-functional ones (the respect of requirements).  To
ensure efficiency, Lopht uses fine-grain platform and application
models. To ensure scalability, Lopht uses fast allocation, scheduling,
and code generation heuristics drawing inspiration from multiple
fields: real-time scheduling, optimised compilation, and synchronous
language analysis and implementation. Lopht aims at providing error
traceability, which allows the use of a trial-and-error design style
aimed at maximizing engineer productivity.
%
Lopht is now fully integrated as a back-end of the Heptagon
compiler. The Heptagon language and compiler have been extended to
allow the source-level annotation of the dataflow synchronous
functional specification to allow the representation of all
platform-independent non-functional requirements, and in particular
real-time ones.

\paragraph{QA-Verify.} QA-Verify is the Quality Management System for Industrial-Scale Software Projects
developed by \PRshort. QA-Verify combines the analysis strength and depth of QA-C and QA-C++ with 
team-sharing collaboration and broader quality management concepts, extending the functionality of 
language analysis and also extending the audience beyond the traditional core of developers and 
quality professionals to encompass other key stakeholders, such as development leads, architects, 
project managers, senior management and even external customers and suppliers. It has many 
thousands of users worldwide. The client-server architecture and web-based interface complements 
our developer desktop deployments, adding role-based views of project quality from ground level 
to tree-top. The key features and benefits are sharing software quality data across key stakeholders,
collaborative code inspections, verification of compliance to coding standards, meaningful
software quality metrics, increased visibility and reduced risk, improved predictability and
detection and prevention of code defects. \textbf{QA-C} and \textbf{QA-C++} are components that
provide static analysis for C and C++, respectively, and provide a comprehensive suite of 
features to help enforce a wide range of coding standards and to find bugs in new and legacy
code. SGS-TUV SAAR has certified QA-C and QA-C++ as usable in the development of safety related 
software for the key safety critical standards, IEC 61508, ISO 26262, EN 50128, IEC 60880 and 
IEC 62304, enabling customers to achieve product certifications to these standards more easily 
and in less time. In \TheProject, we will extend the \PRshort set of tools with mechanisms
to ensure safety, and apply our code analysis to ensure security and compliance to standards
of the code generated from dataflow modelling languages. We are going to develop a new tool to read and interpret the model etc. and then provide the mechanisms for filtering the issues. Providing the comparison between code snippets will also require significant changes to QA-Verify and QA-GUI interface.

\paragraph{PharosN platform by GOLEM}is new IoT software and tools for interactive development of customised high level models of complex cyber-physical systems of systems and  its running.  The platform allows making the variety of CPS models such as {Smart City Monitor} as well as industrial equipment, machinery, buildings, trade centres, factories, various assets, districts, cities, regions, etc.  It realises the latest proof-of-concept of novel Intelligent Sustainable Systems that are capable to evaluate and predict the sustainability status of the target CPS and can advise and support its optimised operations based on its vast linkages to relevant multiple processes and objects in both physical and virtual worlds. The running CPS model enables real time digital transformation of big data streams from a large number of diverse data sources into assisting intelligence providing holistic information services  for effective management, monitoring and controls of the target application system sustainability and operational efficiency. PharosN has the scalable distributed event-driven client-server architecture realised as backend Linux container software developed in C++ and incorporating  major open source software components (docker, postgresql, apache http, nginx, frameworks QT5 and poco and mqtt, etc). The CPS model builder tool clients run under MS Windows and Linux. The PharosN supports rapid prototyping, linking and integration with diverse data sources including IoT, sensors, video cameras, drones, robotics, actuators, webs, mobiles, etc as well as automation  systems  (SCADAs, ERP, MES, etc) and various virtual models of systems and processes running as third party applications.
In \TheProject, we will refactor the platform components such as real time logical processors for data streams and object statuses to ensure reliable run-time dynamic resource allocation for parallelized applications by automatic identification of parallelism sources within the code, inserting the parallel logic and integrated safety checking, novel learning-based decentralised optimisation techniques and applying the code analysis to ensure security and compliance to standards. The results will be benchmarked and validated in the real life application in the project pilot in Jelgava Smart City.  

\paragraph{SYCL compiler ComputeCPP by Codeplay.} ComputeCpp\texttrademark  is Codeplay's implementation of the SYCL\texttrademark open standard. The SYCL standard from the Khronos\texttrademark Group is for developers who want to take software written using C++ single-source programming models like CUDA\copyright~or C++AMP and port to a wide range of OpenCL\texttrademark devices. The current early-access ComputeCpp Community Edition Beta release provides pre-conformance SYCL v1.2.1 support for AMD and Intel\copyright~OpenCL GPUs and CPUs. Further operating system and device support is on its way. This version will let developers work on acceleration of open-source C++ software such as TensorFlow, Eigen and the C++ 17 Parallel STL.
%
Since the ComputeCPP compiler appears as a standard C++ compiler the integration of ComputeCPP into existing projects is seamless and non-intrusive enabling the project to be built as before. %SYCL features are only included if they are used by the project.

\subsubsection{Innovation Potential}
\label{sec:innovationpotential}
\label{innovationpotential}

\eucommentary{Describe the innovation potential which the proposal represents. Where relevant, refer to products and services already available on the market. Please refer to the results of any patent search carried out.}

\khcomment{CB to provide information on the overall scale of the market, and potential competition.  Others to fill in with information
about their own expertise.  Refer to exploitation plans where necessary.}

There is significant market potential in the areas that are targeted by the \TheProject{} technologies.
The global AI market is expected to grow by over 25\% each year up to 2026, reaching a forecast size of around \$30Bn (\url{https://tinyurl.com/ydydusug}).
Within this, machine learning is expected to grow by over 44\% each year, reaching a forecast size of around \$8Bn by 2022 (\url{https://tinyurl.com/y7jh6cy9}).
Similarly, the market for big data processing is expected by grow from \$33.5Bn to \$92.2Bn, driven by e.g. the
huge volumes of data that will be generated by smart cities (a \$2.5 Trillion market by 2027, \url{https://tinyurl.com/yaeou4ju}), 
AI, and the general Internet-of-Things (\url{https://tinyurl.com/ycdcjvlj}).
The global automotive software market is similarly projected to grow to \$11.47Bn by 2021 (\url{https://tinyurl.com/ycsclruy}).
The  demands on performance and data analytics capability that is being imposed by these and other applications mandates the use of 
increasing levels of parallel computing hardware, integrated into data centres and large scale distributed systems.
The global high performance computing (HPC) market is expected to grow from \$32.11Bn in 2017 to \$44.98Bn by 2022,
with the server solutions segment expected to hold the largest market size (\url{https://tinyurl.com/y8uvweoo}).
Finally, the  growth in data volumes, much of which may be personal/sensitive, increasing concerns with privacy in the wake
of recent scandals with misuse of personal social data (e.g. by Cambridge Analytica), concerns about external control of devices such as autonomous vehicles,
and the introduction of new regulatory frameworks to control the use
and processing of data, including the GDPR, means that security is a major and increasingly present concern,
that cannot be ignored in any significant computing solution.
The global cybersecurity market was valued at \$137.85Bn in 2017 and is expected to grow to \$231.94Bn by 2022 (\url{https://tinyurl.com/y8urg28s}).
Collectively, this creates clear market opportunities for \TheProject{} technologies.

% market size
Within this space, the \TheProject{} approach offers significant potential to promote innovation.
The market for block-diagram modelling and development of data-intensive
applications is correspondingly huge. \textbf{MATLAB/Simulink} alone has
more than 3 million users, with the revenue of 900 million USD, and that
is just one of the systems that uses block-diagram modelling. In addition
to this, there are approximately 6.4 million C and C++ programmers in
enterprises of all sizes, globally. This creates an addressable market
whose value is measured in the billions of \euros. This is the market for
tools to enable software developers to create efficient, secure, safe,
bug-free data-intensive code for shared-memory and distributed hardware
systems. Competition in this space is currently fragmented, with low market
penetration. Programming even shared-memory homogeneous systems is
currently very expensive because it is extremely difficult to do and only
an estimated 2\% of all programmers have the required skills. The
situation becomes far worse when the code needs to target heterogeneous
systems or distributed systems of homogeneous or heterogeneous nodes. By
enabling existing expert programmers to become more
productive and by enabling the remainder to develop code for
shared-memory and distributed systems, the technology developed in
\TheProject{} addresses a significant and growing business opportunity.

There is a desperate need for software development tools that can deal
efficiently and effectively with emerging classes of hardware systems,
especially where these are distributed and/or comprise different
processor classes (e.g. Graphics Processing Units [GPUs] used for
general-purpose computing). Such systems are appearing everywhere from
embedded systems and mobile platforms to enterprise/high-performance
computing settings. The decreasing cost of multicore hardware has made
computationally powerful distributed systems to be cheaper than ever to
produce. This has also led to the inclusion of such hardware in the offer
of the leading cloud services providers. Amazon EC2 nowadays sells
virtual machines that contain multicore CPUs and one or more GPU
accelerators for very cheap price. This has also had a consequence in
changing \emph{business requirements} of applications. While performance,
reliability, safety and security are becoming more important than ever,
they are much harder to establish for the code that targets potentially
parallel, cloud-like systems than for sequential code executed in private
environment.

However this change in hardware and business requirements for applications has not been matched by similar changes in software development technologies.  As a consequence, code development for parallel and distributed systems is slow, painful, error-prone \textbf{and, above all, expensive}; and production of distributed software can be highly buggy and very hard to maintain. Furthermore, porting such applications to different target systems is currently mostly manual work, which requires significant restructuring and rewriting of applications. This creates a clear need for improvement and major long-term market potential for \TheProject tools and technologies.

%% Research undertaken as part of a business innovation survey in 2016 at \USTAN{} of 209 software developers, showed that:
%% \begin{itemize}
%% \item 	there are 4.4 million C++ and 1.9 million C developers globally
%% %\item 34\% EMEA, 34\% APAC, 23\% NAM and 9\% LAM
%% \item 89\% think it is important that their software uses multiple cores efficiently
%% \item 44\% identify parallel coding as a cause of problems faced in development
%% \item 23\% of C/C++ programmers said they are likely or very likely to buy the proposed \ParaFormance{} product developed by \USTAN{} at \pounds{}1,000/year (with 18\% saying they would buy more than one copy)
%% \item a \pounds{}1 billion market opportunity today
%% \end{itemize}

%% \TheProject{} addresses the business problem associated with these technical issues.  Businesses pay premiums to recruit and employ the best programmers, yet cannot keep up with the demand of data-intensive applications, and cannot exploit memory and computing potential of modern hardware systems.  Even the best programmers make mistakes: finding bugs in parallel code is extremely difficult, time consuming and therefore expensive.
%% In addition failures of production systems can impact both software system suppliers, their employees and customers using the applications.      
%% Competition in the market includes manual production of distributed data-intensive applications using low-level libraries such as MPI, Intel Parallel Studio (optimisation tools), Silexica (hardware design), Critical Blue (consultancy and optimisation tooling), and Rogue Wave Software (debugging tools). Existing tools and techniques are time-consuming to use, highly error-prone and require significant knowledge and expertise in specific hardware platforms.  We therefore expect that our tools will be highly attractive to a wide range of potential customers at both the enterprise and smaller scales. Potential benefits are in faster time-to-market, reduced debugging and maintenance effort, easier deployment of software to a variety of hardware platforms, and fewer runtime bugs. Manual approaches are very error prone and costly, where programmers typically invest weeks or months of development effort.  Other software tools are targeted at very specialist programmers with a very low-level and expert skill-set.  Non-specialist programmers would require extensive training to learn to use these tools. Furthermore, the assistance given is often limited and difficult to understand. 

%% Model-based code generation especially in context of software-defined
%% infrastructures presents a large number of challenges for tool chains and
%% runtimes as information at the model level needs to be propagated
%% effectively down to the tool chains that process the code for a variety
%% of different heterogeneous platforms. Addressing these challenges will
%% lead to a lot of innovative solutions and commercially exploitable
%% results in runtime systems and platform support. As already outlined in
%% WP4, some work is expected to extend existing standards such as SYCL with
%% new features such as distributed execution. The results of \TheProject{}
%% will help to reuse more existing code quicker and maintain performance on
%% new (heterogeneous) platforms and with less (or without) modifications
%% (therefore more cost-efficient) through improved runtimes and better
%% platform support (also targeting low energy consumption and
%% safety-critical).

%\begin{itemize}
%\item Market analysis: number of programmers, demand, willingness to use new tools.
%\item
%Need for parallelism, heterogeneity, low-energy computing.
%\item
%Scale of application areas: IoT etc.
%\item
%Cost of bugs etc.
%\item
%Current skills level
%\item
%Existing companies developing tools for parallel programming.  Intel, Silexica, CriticalBlue, ...
%\item
%Advantages over the competition.
%\end{itemize}

% ---------------------------------------------------------------------------
%  Section 2: Impact
% ---------------------------------------------------------------------------

\draftpage

\clearpage
\section{Impact}
\label{sec:impact}

\TODO{Look at this once the rest of the project is together.}

\eucommentary{Describe how your project will contribute to:\\
o the expected impacts set out in the work programme, under the relevant topic;\\
o improving innovation capacity and the integration of new knowledge (strengthening the competitiveness and growth of companies by developing innovations meeting the needs of European and global markets; and, where relevant, by delivering such innovations to the markets;\\
o any other environmental and socially important impacts (if not already covered above).}


\subsection{Expected Impacts}

\eucommentary{}


% The table below summarises how the \TheProject{} project will achieve impact in the areas expected by the Work Programme.

\khcomment{We don't necessarily have to tackle all these
impacts head-on,  but we should have a good story.}

\begin{longtable}{|p{125pt}|p{320pt}|}%\hline

\hline \textbf{Expected impact}&

\textbf{How will \TheProject{} achieve this impact?}\\\\ \hline
\endfirsthead

\multicolumn{2}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous
page}} \\ \hline \textbf{Expected impact}&

\textbf{How will \TheProject{} achieve this impact?}\\\\ \hline
\endhead

\hline \multicolumn{2}{|r|}{{Continued on next page}} \\ \hline
\endfoot

\hline \hline
\endlastfoot

\textit{Impact 1} \par 
\textbf{Increased capacity of the European software industry to exploit the capabilities of software-defined infrastructures at middleware and application layer.}
&
%\begin{description}
% \item[1]
\noindent
\TheProject{} directly addresses this impact by targeting \textbf{software-defined infrastructures} at both
the application and middleware layers.
%  by raising the level of
% abstraction in both developing the end-user applications, by exploiting
% and extending the block-diagram modelling technology, and in describing
% complex target hardware platforms, by automatically generating
% software-defined infrastructures. 
At the \emph{application level}, we
will exploit building blocks and patterns to encapsulate data, communication and
computation, so allowing programmers to capture structure of both
compute-intensive and data-intensive applications. Novel code-generation
techniques will allow the automatic generation of efficient C/C++ code.
The generated code will be extended with SYCL constructs to allow implementations to
transparently, portably and flexibly target heterogeneous systems. 
The generated code will be safe, secure and will be shown to
comply to the required coding standards.
At the \emph{middleware level}, we will
automatically generate software-defined infrastructure that will be responsible for
the automatic deployment, monitoring and tuning of the application, while at the
same time dynamically optimising the required extra-functional
properties. Block-diagram modelling, pattern-based program development and
software-defined infrastructure will collectively relieve the programmer of the burden
of dealing with low-level deployment and tuning issues while still permitting the
generation of highly optimised and portable code.  In this way, the European software
industry will gain an increased capacity to target the exciting new capabilities offered
by software-defined infrastructures.

\vspace{10pt}
\noindent \emph{\TheProject aims related to this impact:}
\textbf{Aims 1, 2 and 3. Objectives 1-6.}
\vspace{10pt}
\\ \hline
\vspace{10pt}
\textit{Impact 2} \par
\textbf{Expand research and innovation potential in software technologies while overcoming fragmentation in the European supply base, optimising investments and use of resources to yield multi-domain software-based products and related software services.} & 
\noindent 
\TheProject{} directly addresses this
impact. Section~\ref{sect:state-of-the-art}
(p.~\pageref{sect:state-of-the-art}) describes how we will expand
European \textbf{research capacity} by advancing the state-of-the-art in
many areas of software technologies.  Researchers will be
empowered to tackle major future challenges in software technologies.
Section~\ref{innovationpotential} (p.~\pageref{innovationpotential})
describes the corresponding long-term \textbf{innovation potential} that
\TheProject{} will create.  In order to \textbf{optimise investments and
  use of resources} and \textbf{overcome fragmentation}, where possible,
we will make use of existing technologies and research results that have
been developed and/or enhanced in previous EU projects and/or by European
industry and academia (e.g. Erlang, SYCL, QA-Verify, SimuLink,
FOCUS\khcomment{need to mention some IBM tech here...}, mlpp, etc.), as
well as taking advantage of % widely-used % industry-standard
technologies that are widely used in industry (e.g. C/C++. block-diagram
modelling, pattern-based programming etc.).
% Rather than developing a completely new technology from scratch, we will
%  instead mostly build on the existing, widely-used technologies such as block-diagram modelling, pattern-based programming, code compliance diagnostics, security checking and verifying mechanisms and dynamic multiobjective optimisations.
We will extend and adapt these technologies to support the development of complex large-scale data-intensive applications
for the emerging generation of distributed heterogeneous computing systems.
We will also ensure their interoperability as part of a coherent software development methodology.
% We will for data-intensive applications and distributed heterogeneous systems, bringing them together and allowing their interoperability in a coherent software development methodology. In this way, we will reduce fragmentation in the European supply base. Finally, 
The technologies that we will use are naturally \textbf{multi-level} and \textbf{multi-domain}.
We will demonstrate this in the course of the project by considering the needs of use cases taken from three widely varying application domains
and with different needs and requirements: artificial intelligence (deep learning), smart cities (IoT), and the automotive sector.
% As will be demonstrated over the course of the project, our technologies will not be tied to a specific application domain, but will be general enough to accommodate applications from diverse domains such as automotive, machine learning and internet-of-things.

\vspace{10pt}
\noindent \emph{\TheProject aims related to this impact:}
\textbf{Aims 1, 2, 3 and 4. Objectives 1-8.}

\vspace{10pt}
\\
 \hline
\end{longtable}

%\draftpage
% \subsubsection*{Improving Innovation Capacity}
%\draftpage

\pagebreak
\paragraph*{Improving Innovation Capacity}
\noindent
\TheProject{} will significantly improve long-term innovation capacity by enabling 
%  permitting significantly more productive development of applications for distributed computing systems.
% This will allow 
European companies and individuals to develop complex and innovative new software and systems for a variety
of commercially-important and emerging markets, including artificial intelligence, smart cities and automotive,
in a much more productive and effective way.
% , including and internet-of-things. 
Software-defined infrastructures and dynamic adaptivity mechanisms will make it possible to exploit new hardware platforms
at much lower cost, and requiring much less system expertise, than is presently the case.
Dynamic, decentralised multi-objective optimisations will make it possible to transparently manage performance, security, reliability and other extra-functional properties and requirements of the applications on a wide range of homogeneous, heterogeneous, shared-memory and distributed platforms. 
This will bring a major boost to European markets in software, hardware and all kinds of smart devices.
Section~\ref{sec:innovationpotential} (page \pageref{sec:innovationpotential}) describes the market opportunities and potential for innovation 
in more detail.

%\subsubsection*{Societal Impact}
\paragraph*{Societal Impact.}
\noindent
\TheProject{} will have a major impact on society through its focus on delivering high-quality, highly-secure, highly efficient, highly robust,
highly scalable and lower cost software systems.
This will meet future societal needs for safe and secure complex data-intensive applications.
\TheProject will enable  rapid and low-cost software development a variety of new and emerging hardware platforms, 
automatically generating highly-efficient target code while simultaneously eliminating and/or reducing complete classes of common, catastrophic software bugs.
By embedding security and safety as a key part of the design process, \TheProject{} will ensure trust, privacy, and security
\emph{by design} and \emph{by construction}.
Collectively, the \TheProject{} approach will permit the development of lower-cost systems that are more scalable, more reliable, more secure and easier to maintain,
and that will easily and automatically adapt to new classes of hardware, including ones that integrate features of cloud computing, high-performance computing
and fog/edge computing.  This will be done more effectively, more productively and more rapidly than is possible with current state-of-the-art software
development techniques.

%\draftpage
\subsubsection*{Possible Barriers to Achieving the Expected Impacts and Associated Mitigations}

\newcounter{barrier}

\begin{longtable}{|p{125pt}|p{320pt}|}%\hline

\hline \textbf{Possible Barrier}&

\textbf{Mitigation}\\ \hline
\endfirsthead

\multicolumn{2}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous
page}} \\ \hline
 \textbf{Possible Barrier}&

\textbf{Mitigation}\\ \hline
\endhead

\hline \multicolumn{2}{|r|}{{Continued on next page}} \\ \hline
\endfoot

\hline \hline
\endlastfoot


\addtocounter{barrier}{1}
\noindent
\emph{Barrier \thebarrier.}
\par \emph{\TheProject{} block-diagram modelling and software-defined infrastructures cannot be used in realistic use cases.}

&
\noindent
Key technology from \TheProject{} is already used on large scale
applications from different domains. Block-diagram modelling is already
widely used in the automotive and other industries.  Erlang is widely
used for constructing highly-reliable large-scale distributed systems,
especially in the telecommunications industry. The FP7 \paraphrase
project has shown how Erlang can be used to manage large-scale distributed AI and industrial applications
on high-performance computing systems.
% in building middlewares for coordinating and managing execution of large-scale scientific and industrial applications on high-performance distributed systems%Additionally, in the previous EU projects such as \textbf{SCIEnce} and \textbf{ParaPhrase} we have demonstrated usability of functional programming languages \textbf{Haskell} and \textbf{Erlang} in building middlewares for coordinating and managing execution of large-scale scientific and industrial applications on high-performance distributed systems.
%% Not relevant?  KH
% Other tools and technologies that will be used in \TheProject{}, such as \textbf{QA-Verify} and the \paraformance{} tools have been extensively tested on large-scale C/C++ code. 
\\ \hline
\addtocounter{barrier}{1}
\emph{Barrier \thebarrier.} \par
\emph{\TheProject{} software-defined infrastructures are not flexible enough to be used as middleware for the required target architectures.}
&
\noindent
One of the main aims in the design of Erlang programming language, which will be the target language in which the software-defined infrastructures will be generated in \TheProject{}, was precisely portability to different distributed architectures. Erlang has generic portable constructs for distribution and synchronisation of work over multiple computing and data nodes and monitoring and fault tolerance are integral parts of its design. Furthermore, Erlang Virtual Machine which executes the actual compiled Erlang code was, similarly to JVM, designed for portability. Therefore, by targeting Erlang for software-defined infrastructure, we are ensuring production of code that can target vastly different shared-memory and distributed target platforms.
%% Implementation risk rather than impact barrier
%% \\ \hline
%% \addtocounter{barrier}{1} 
%% \emph{Barrier \thebarrier.} \par
%% \emph{Incompatibility of the \TheProject{} technologies at the application and middleware layer}
%% &
%% \TheProject{} will use different technologies and target languages for different parts of its methodology -- block-diagram modelling for design of the applications, C/C++ combined with SYCL as an intermediate language for diagnostics and Erlang for generating software-defined infrastructures. While it is possible that we will not be able to integrate all these different technologies into a coherent software-development methodology and tool chain, that is highly unlikely, as we already have experience in using Erlang as a middleware for coordinating large-scale computations, components of which are written in C/C++ and other languages, as a part of the successful EU \paraphrase{} project (page~\pageref{projects}). 
\\ \hline
\addtocounter{barrier}{1}
\emph{Barrier \thebarrier.} \par
\emph{\TheProject{} technology cannot be applied to different application domains.}
& 
All \TheProject{} tools technologies are completely generic and have been applied in multiple application domains.
% No part of the proposed \TheProject{} methodology and tool-chain is specific to any particular industry.
Block-diagram modelling is widely used industrially, including for engineering, aeronautics, radio
and automotive applications.
%  and, in particular, in automotive industry that \TheProject{} is targeting.
Our code analysis tools have been used on general C/C++ code. As part of
the EU \paraphrase{} and \rephrase{} projects, we have demonstrated their
applicability to use cases from a number of different domains,
including railway diagnostics, medical imaging, deep learning, weather prediction,
high precision machining, and complex simulation. Finally, SYCL has also been used on
applications from various domains, including machine learning and
automotive applications. All this gives us high confidence in the generality of the
\TheProject{} approach.
\\ \hline\addtocounter{barrier}{1} \emph{Barrier \thebarrier.} \par
\emph{Industry cannot adopt the \TheProject{} Technologies.} &
\TheProject{} incorporates five companies (\IBMshort{}, \CODEPLAYshort{},
\PRLshort{}, \GOLEMshort{} and \SCCHshort{}), who will provide feedback
on the suitability and usability of the tools and technologies that will
be developed both for their own use and those of their customers. User community activities are specifically designed to
engage with industry and to smooth the adoption path, including public
tutorials, webinars and documentation. \TheProject{} targets widely-used
development platforms, hardware and software frameworks. Steps will be
taken to make it easy to access and use the tools: we will provide
open source software solutions where available; and we will provide links to proprietary tools
from the project website,
where these form part of the \TheProject{} toolchain.
\end{longtable}

%\draftpage
\subsection{Measures to Maximise Impact}

The impact of \TheProject{} will be maximised by:
\begin{inparaenum}[i)]
\item
demonstrating the utility of the \TheProject{} technology and tools on real-world use cases taken
from three commercially important application domains: artificial intelligence, smart cities (IoT) and automotive;
\item
incorporating security, safety and correctness concerns from the outset;
\item
ensuring that business requirements are met, including scalability, reliability and robustness;
\item
covering the full software lifecycle, including requirements, implementation, testing, debugging and deployment;
% \item
% promoting the use of \TheProject{} technologies and tools through dedicated user community building activities;
\item
engaging directly with and taking feedback from the user and developer community, including specific user community development
activities;
\item
building on and using technologies that are widely used commercially, including C/C++ and Erlang, and using respected high-quality tooling
from \IBMshort{}, \PRLshort{} and \CODEPLAYshort{};
% \item
% targeting two very widely-used standard development languages, C and C++;
%% \item
%% targeting a range of widely-used libraries and parallelism frameworks, including pThreads, openCL, SYCL,  openMP, Intel's TBB, Microsoft's PPL,
%% Cilk and the C++17 parallel STL;
\item
demonstrating the utility of the \TheProject{} technology and tools through deploying a variety of 
use case applications;
% \item
% developing a coherent set of methods that will allow patterned approaches to be utilised
% throughout the entire development process;
%% \item
%% integrating with widely-used development environments, including Microsoft's Visual Studio and Eclipse;
\item
promoting new coding standards to developers and their management teams;
%% \item
%% considering widely-used processor architectures, including Intel/AMD, ARM and IBM Power;
%% \item
%% engaging with the ISO C++ standards committee;
\item
deploying an exploitation plan that links with the strategic needs and internal
development plans of the companies and other organisations that are involved in \TheProject;
\item
publishing results extensively, both in technical areas, and through press releases,
radio interviews, news items and the general technical press;
\item
engaging with relevant networks of excellence, such as HiPEAC and MARioNET, to ensure widespread dissemination of
project results and uptake by third parties;
\item
exploiting nationally-funded opportunities for the formation of spin-off companies and for commercial
exploitation of research results;
\item
utilising open development and dissemination mechanisms as far as practical
and consistent with the needs of good exploitation; 
and
\item
teaching the achieved results in advanced Masters and PhD courses that are offered by the academic beneficiaries. 
\end{inparaenum}
%
The following section describes these measures in more detail.

\subsection{Dissemination and Exploitation of Results}
\label{sect:dissemination}

\eucommentary{Provide a draft 'plan for the dissemination and exploitation of the project's results' (unless the work programme topic explicitly states that such a plan is not required). For innovation actions describe a credible path to deliver the innovations to the market. The plan, which should be proportionate to the scale of the project, should contain measures to be implemented both during and after the project.
Dissemination and exploitation measures should address the full range of potential users and uses including research, commercial, investment, social, environmental, policy making, setting standards, skills and educational training.
The approach to innovation should be as comprehensive as possible, and must be tailored to the specific technical, market and organisational issues to be addressed\\
o Explain how the proposed measures will help to achieve the expected impact of the project. Include a business plan where relevant.\\
o Where relevant, include information on how the participants will manage the research data generated and/or collected during the project, in particular addressing the following issues:\\
o What types of data will the project generate/collect? o What standards will be used? o How will this data be exploited and/or shared/made accessible for verification and re-use? If data cannot be made available, explain why.
o How will this data be curated and preserved?}

\khcomment{A serious plan is needed here.}

\subsubsection{Draft Dissemination Plan}

We will focus on propagating our results both to the computer science
research community and to potential users of the \TheProject{} technology.
We will do this through a mixture of high-quality publication, presentations
and direct engagement with the user community.
%
The main research communities that we expect to target are:
parallel programming, compilation, optimisation, runtime systems, machine learning and security.
%
We anticipate that the primary users of our technology will be:
programmers and developers who need to develop complex parallel applications with time, energy
and security constraints in a variety of application domains.

\paragraph{Scientific Publications:}  The main routes to good scientific dissemination are
through peer-reviewed publication, and through presentation of results at key scientific events.
\TheProject{} partners will therefore aim to produce high-quality \emph{peer-reviewed
research publications} in relevant leading
conferences, technical workshops and journals.
We will build on the existing good publication records of the \TheProject{} partners,
aiming to produce a sizeable volume of good quality publications in the course of the project. 
%
\noindent
The conferences that we propose to target include:
\khcomment{Update these.}

\begin{quote}
\textbf{ICSE:} International Conference on Software Engineering;
\textbf{CAV:} Computer Aided Verification;
\textbf{TACAS:} Tools and Algorithms for Construction and Analysis of Systems;
\textbf{ICSM:} International Conference on Software Maintenance;
\textbf{IC2E:} IEEE Intl. Conf. on Cloud Engineering;
\textbf{CCGRID:} International Conference on Cluster Computing and the Grid;
\textbf{IPDPS:} International Parallel and Distributed Processing Symposium;
%% \textbf{EURO-PAR:} International Conference on Parallel processing;
%% \textbf{PPoPP:} International Conference on Principles and Practice of Parallel Programming
%% \textbf{ICPP:} International Conference on Parallel Processing;
%% \textbf{ParCo:} International Conference on Parallel Computing;
\textbf{SC:} Super-Computing;
% \textbf{PACT:} International Conference on Parallelism in Architecture, Environment and Computing Techniques;
\textbf{PDP:} Euromicro International Conference on Parallel, Distributed and Network-Based Computing;
% \textbf{HLPP:} International Symposium on High-Level Parallel Programming;
\textbf{EUROMPI:} European MPI Users' Group Meeting;
\textbf{ICML:} International Conference on Machine Learning;
\textbf{CGO:} International Symposium on Code Generation and Optimisation;
\textbf{POPL:} ACM Symposium on Principles of Programming Languages;
\textbf{PLDI:} ACM Conference on Programming Language
    Design and  Implementation;
%\textbf{GPCE:} ACM Conference on Generative Programming    and Component Engineering;
%\textbf {ICFP:} ACM International Conference on Functional Programming;
\textbf{SPLASH/OOPSLA:} ACM SIGPLAN conference on Systems, Programming, Languages and Application;
\textbf{ECOOP:} European conference on Object-Oriented Programming;
%\textbf{TFP:} the International Symposium on Trends in Functional Programming;
\textbf{CDNSC:} Cyber Defence \& Network Security Conference;
\textbf{ICSIC:} International Cyber Security \& Intelligence Conference;
\textbf{CyberTech Israel};
\textbf{European Security Summit};
\textbf{Information Security Europe};
\textbf{Ignite};
%\textbf{FHOC:} the International Workshop on Functional High-Performance Computing;
%\textbf{SCALA:} the Scala Workshop.
\textbf{GECCO:} ACM Genetic and Evolutionary Computation Conference;
\textbf{ICCS:} International Conference on Computational Science.
% \textbf{Lambda Days:} International Conference on Functional Programming and Emerging Technologies.
 \end{quote}

\noindent
We also propose to target relevant high-impact journals such as:
\begin{quote}
\emph{the ACM Transactions on Software Engineering and Methodology (TOSEM)}, 
\emph{the IEEE Transactions on Software Engineering (TSE)}, 
\emph{the Journal of Systems and Software (JSS)},
\emph{Concurrency and Computation: Practice and Experience (CCPE)},
\emph{IEEE Transactions on Parallel and Distributed Systems (TPDS)},
\emph{Future Generation Computing Systems (FGCS)},
% the \emph{International Journal of Parallel Processing (IJPP)},
% the \emph{ACM Transactions on Parallel Computing (TOPC)},
the \emph{ACM Transactions on Computer Systems (TOCS)},
\emph{Journal of Parallel and Distributed Computing (JPDC)},
% \emph{Parallel Processing Letters (PPL)},
\emph{Software: Practice \& Experience}, 
\emph{Journal of Cybersecurity},
\emph{International Journal of Cyber-Security and Digital Forensics (IJCSDF)},
\emph{The Journal of Cyber Security and Information Systems},
% \emph{IEEE Transactions on Evolutionary Computation},
\emph{Journal of Computational Science (JOCS)}.
% the \emph{ACM Transactions on Programming Languages and Systems (TOPLAS)},
% \emph{International Journal on Applied Mathematics and Computer Science (AMCS)}
\end{quote}

\paragraph{Project Web site:}  
A crucial component of the \TheProject{} dissemination strategy is a
high-quality project website. The public section will provide ample
and consistent information about all aspects of the \TheProject{}
project, with the goal of positioning the \TheProject{} website as a
prime information source for relevant scientific and technical
information.  The vast majority of the \TheProject{} deliverables are
public, and full access to these will be provided.  The web site will also contain lists of publications and links
to open-access repositories; copies of technical reports and white
papers; a news feed; technical documentation; downloadable software
and pre-installed virtual machines; video demonstrations; online
tutorials; information about project partners; copies of
presentations, podcasts and other material; data and results; plus
links to the Horizon 2020 programme in general and to related 
Horizon 2020 research projects in order to highlight the role played by \TheProject{} within the
broader EC research framework.

\paragraph{Project News Feed:}  We will set up open public mailing lists/twitter/facebook accounts that will
be used to communicate project news and results to interested parties, whether they are scientists, academics, developers
or the general public.  This news feed will be highlighted on the project web site.

\paragraph{Developer and User Community:} We will engage with the broader
developer and user communities by a series of focused activities
that will include the organisation of dedicated user community workshops, presentations at
 developer and other conferences, 
 the production of posters, delivering tutorials, staffing booths, and providing hands-on
 guidance in the use of our tools and technologies etc.  This direct engagement will be
 supported by the production of video demonstrations, training materials, tutorials and documentation that can
 be accessed through the project web site.  We will also aim to produce white papers and slide sets that
 can be used to explain the benefits of the \TheProject{} approach to prospective users, both developers
 and managers.
 %\SPINORshort will use its on-going activity in trade shows and conferences to promote
 %the latest enhancements of their Shark 3D technology made by \TheProject. This is a natural part of
 %\SPINORshort activities of continuously strengthening the Shark 3D brand of keeping on the cutting edge. For example, in 2015 the managing director of SPINOR was on a panel on the World Animation & VFX summit, a leading B2B conference for the media industry, about virtual reality. As tools and middleware developer, most of the whole media industries are the target audience, including the video game, film, broadcasting, and simulation markets. Specifically, SPINOR plans to make not only all customers of SPINOR aware of the new technologies, but aims to make a significant part of the virtual reality industry aware of the new technologies.
 
\paragraph{Expert Working Groups and Networks of Excellence:}
In order to ensure good dissemination to the research and development
community, including industrial researchers, we propose to disseminate
our project results through the most relevant scientific/technological
networks and working groups, including HiPeac,%  the TACLe Cost Action on Timing Analysis, the Ercim DES Working Group, 
IFIP Working Groups 2.11 \& 10.3, and other EU and national projects,
as well as national groups such as the UK's network on manycore computing.
\SAshort{} is a full member of HiPeac and a charter member of IFIP working
group 2.11, and has been heavily involved in activities organised by these and other expert groups.
These provide a high-level interface between academic and industrial interests, and a valuable
melting pot for ideas and new technologies. 

\paragraph{Standardisation Committees:}
% \khcomment{We should mention any committees that we are members of. Remove this section if not relevant.}
This proposal is tied to Standards from the beginning, through the end and beyond. It leverages the key leadership position of several members, acting as senior leaders, officers, working group experts, proposal authors, collaboration with other industry and academic experts.
%% Doesn't make sense?
% Current C++ Standards only support CPU, though it is adding parallel programming support starting with C++ 11, enhanced through C++ 17 with parallelSTL. Yet it is still lacking many things that can support Heterogeneous computing, which has been already explored by SYCL and OpenCL. 
%
%
\CODEPLAYshort{}, \SAshort{} and \PRshort{} are members of the ISO C++ Standards Committee, the leading body
for standardisation of the C++ programming language. \CODEPLAYshort{} member Michael Wong holds a senior leadership position
on this committee, including chairing SG12 on low latency, SG5 on transactional memory, and is a member of the C++ Directors Group. 
\CODEPLAYshort{} and  \PRshort{} are members of the MISRA C and MISRA C++ Standards Committees, the leading bodies for standardisation of the C and C++ languages for the automotive industry. 
Other standards that they have contributed to include the Khronos Safety Critical standard and the HSA Foundation software, as well as OpenCL, OpenCL SC and Vulkan. 
%
\GOLEMshort{} is an active member of the Focus Group on Data Processing and Management to support IoT and Smart Cities and Communities (FG-DPM) at International Telecommunication Union (ITU).
%% Too much C++ focus.  Needs to move away from just C++/parallel.
% Consortium members are also very involved in relevant academic and industrially-focused conferences,
% including CPPCon, Meeting C++, ACCU, C++ Users Conferences, C++Now, LLVM developer meetings, AutoSens, HiPEAC, and Supercomputing. 


\paragraph{General Scientific and Technical Community:}
We will further engage with the general scientific and technical community
 by participating in relevant workshops, conferences and clustering events (including ones which
 will develop our technologies beyond the bounds of our own research communities), by engaging with the
 different EC-sponsored CORDIS information channels, by presenting at trade
 fairs, and through other dissemination activities.  We will use materials such as presentations,
 papers,  posters, demonstrations and the project web site to do this.


\paragraph{Education:} We will target 
the educational communities through the production of relevant educational materials, that will
also have a training benefit. Young researchers, software
 developers and application programmers will learn how to
 use our software technologies in their
 respective fields. This is aligned with recent EC
 initiatives on the subject such as ``Increasing the Attractiveness
 of Science, Engineering \& Technology Careers''.
 The academic consortium partners will integrate Bachelor,
 Master, Diploma, and PhD students into the \TheProject{}
 project whenever possible, e.g.~through PhD and MSc theses, student
 projects, academic courses, and research seminars. 
 We will also take advantage of opportunities to engage with broader
 audiences through guest lectures at other institutions and summer schools etc.
 These students will
 carry the methodology and techniques of the \TheProject{}
 project into their future work places in the ICT industry.
 In that way the \TheProject{} vision and the project
 results will disseminate into many research groups and
 companies working in the field of security.

% \paragraph{General Communication:} We will adopt a good general communication strategy
% aimed at maximising the outreach of the \TheProject{} project to the public at large.  We will issue
% regular press releases describing relevant events and research results, conduct
% radio/TV interviews, adopt an open policy to disseminating our research results through use of appropriate
% repositories for publications and the project web site, use public media such as
% \emph{twitter} to communicate project results, engage with public lectures and seminars,
% write general articles for newsletters, newspapers etc. as described in \ref{wp:dissem}.

\begin{quote}

\emph{Although all these dissemination activities
will be centrally coordinated, the full and active participation of
all partners is expected. Key project representatives from
the various \TheProject{} academic and industrial
partners will also arrange specific meetings
with scientific, commercial, industrial, and/or
governmental representatives to facilitate public
engagement.}
\end{quote}

\subsubsection{Draft Exploitation Plan}
\label{sect:exploitation-plan}
\vspace{-12pt}

All partners will have the right to use foreground IPR for
the purposes of the project. In order to ensure that all
contributions are recognised, exploitation plans will be
shared with the consortium as a whole. Partners will not,
however, have the right to veto or delay exploitation
unless their own IPR is directly involved.


\horizontalline

\subsection*{Draft Exploitation Plan for \IBMshort{}}


\begin{wrapfigure}{R}{4cm}
\vspace{-1.4cm}
\hfill \includeimage[width=4cm]{logos/ibm.jpg}
\vspace{-0.6cm}
\end{wrapfigure}

\IBMshort{}'s role in this project is to lead the effort of making the
code generated by \TheProject{} more secure, and adding mechanisms to add
dynamic protection when there is an indication for higher risk.
\IBMshort{} established the \IBMshort{} security unit which offers
solutions in security intelligence, endpoint detection, security
vulnerability detection, penetration testing, malware analysis and more.
The \IBMshort{} research lab in Haifa is working closely with the \IBMshort{}
security unit that brands and incorporate innovations into their products, for
example the security products of Guardium, Trusteer and Xforce.
\IBMshort{} has been named by Gartner as a leader in a number of security
fields. Participation in \TheProject{}, including close collaboration
with the use case partners and their real-world code, which may include
security vulnerabilities, or may need to generate code in a secure way,
will help guide the next generation of security products.  In addition,
Israel has a rapidly emerging security community including hundreds of
companies and start-ups developing and building innovative security
solutions. \IBMshort{} is the main sponsor of the major security
conference in Israel, CyberTech
(https://www.cybertechisrael.com/node/520), and actively helps incubating
new start-ups in the \IBMshort{} accelerator. We will use \IBMshort{}'s
connections in Israel security community to present \TheProject{}
innovative ideas.

The \TheProject{} project will give \IBMshort{} an opportunity to develop and exploit security solutions in the area of security vulnerability detection and virtual patching. \IBMshort{} research may exploit these outcomes:
\begin{itemize}
\item
Internally to enhance the security and quality of \IBMshort{} products; 
\item
As a service, by closely collaborating with \IBMshort{} security services;
\item
By integration into \IBMshort{} security offerings of IBM Xforce;
\item
Externally as part of the \IBMshort{} security offerings and/or as a cloud service on one of \IBMshort{}'s platforms (e.g. \url{https://www.ibm.com/cloud-computing/bluemix/})
\end{itemize}
\vspace{-6pt}

\horizontalline

\subsection*{Draft Exploitation Plan for \PRshort{}}

\begin{wrapfigure}{R}{3.8cm}
\vspace{-1.5cm}
\hfill \includeimage[width=4cm]{logos/PRL.png}
\vspace{-1.5cm}
\end{wrapfigure}

In \TheProject, \PRshort{} plans to address the following important problems that will enhance the safety and security of generated C++ code:
\begin{itemize}
\item Expand coverage for static and compliance analysis to support C++ '14 and '17 together with implementation of support for possible parallel patterns implementation problems. This implementation will allow our customers to use the latest language versions and will also help with analysis of implementation for parallel patterns for this project and for customers who wish to adopt parallel patterns.

\item Develop methods required and implement conversion of the static analysis diagnostics into a DSL that will be used to modify generated C++ code by amending models via this approach. This is extremely important for our customers (especially in the automotive, rail, and aviation industries) as auto-generated code is part of their development process and their certifications do  not allow them to modify the code manually,
since such code would be non-compliant. Providing mechanisms for the amendment of the DSL models for code generators will allow our customers to address this big issue. This is a completely new approach to solve the problem which our customers have been awaiting for a long time. It will have a critical impact on the safety and security of software in safety-critical industries.

\item Develop new functionality for review of the model and C/C++ and categorise the diagnostics, that groups related diagnostics within a specific category.

\item Develop visualization component that displays side-by-side ``current code'' and ``modified code'' for the PRQA Framework and QA-Verify software.

\item Expand \PRshort{} coverage for security analysis.
\end{itemize}

\PRshort{} plans to implement these new functionalities into
\PRshort{}'s commercial software.  This will make our customers' software
safer, more secure, and compliant to relevant industry standards. Also
these functionalities will affect the development and implementation of
new standards for industries. This is extremely important with the
current rapid development of the C++ programming language. \PRshort{} also
plans to update their existing High Integrity C++ standard to make new
functionality publicly available for the wide range of programmers who
are using this standard (more than 30,000 developers).

Finally, \PRshort{} plans to use the use case provided by Magna Electronics Inc. (USA) as a show case for
the automotive industry to demonstrate the technique developed in \TheProject{} for automatically generated code improvements and expand \PRshort{}'s market into this area. 

%\vspace{6pt}

\horizontalline

\subsection*{Draft Exploitation Plan for \SCCHshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/SCCH.jpg}
\vspace{-0.8cm}
\end{wrapfigure}

In general, \SCCHshort{} plans to increase within \TheProject{} its know-how to efficiently develop and deploy data and
computationally intensive applications. More precisely, \SCCHshort{} expects to acquire knowledge that allows to apply
the developed \TheProject{} technology to a broad class of massive and computationally expensive data analysis and
computer vision applications.

%\textbf{Exploitable for which Products and Activities.} \SCCHshort{} has a tradition of running application oriented projects with
%industrial partners in the area of intelligent data analysis including computer vision. As these applications are characterized
%by a growing demand on computational speed and data throughput the planned knowledge gain supports
%\SCCHshort{}???s long-term strategy facing the requirements of data analysis systems in terms of performance, heterogeneity
%of hardware platforms and productivity of the software engineering process. Sectors of application include a)
%data intensive applications employing machine learning based computational models for process analysis, process
%optimization, prediction systems (e.g. weather forecast), and fault diagnosis employing computationally demanding
%algorithms and b) computer vision applications like quality inspection, object recognition and tracking.

\textbf{General Exploitation Strategy.} \SCCHshort{}, which is organized as a non-profit GmbH (Ltd), is an independent research
institution (RTO) with a focus on applied software science and the mission to transfer cutting edge research from
academics to industry by running contract research projects. Through its role as an applied RTO, \SCCHshort{} pursues a strategy
to closely collaborate with world-leading academic partners to establish and further develop key technologies together
with its industrial partners. This is organized by conducting common applied research with its industrial partners,
as well as strategic research that is aimed at scientific publications, PhD theses and patents. Following this general strategy,
\SCCHshort{} exploits its benefit from the \TheProject{} project by increasing competences of the well-established research
domains at \SCCHshort{} like data analysis systems and computer vision. Hence, \SCCHshort{} considers the \TheProject{} project
as an opportunity to increase know-how in the efficient development of parallel applications within its network of
industrial beneficiaries: a whole class of applications should be solvable more efficiently and therefore many partners
from \SCCHshort{}'s industrial network will profit from the \TheProject{} project. The scope of applications include large
scale fault diagnosis, automation of defect analysis in extremely complex technical systems, decision support for
process and production engineers, and applications in computer vision like object recognition and quality inspection.
These are only enabled by the possibility to employ computationally demanding methods which is offered by the
\TheProject{} technology like deep learning, robust high dimensional regression and classification methods, and
advanced graphical model-fitting and model calibration techniques.

%\textbf{Exploitation for Contract Research.} 
\textbf{Industrial Exploitation.} \SCCHshort{} runs a large portion of its application oriented research together with a
network of industrial partners within research programs of the Austrian Research Promotion Agency (FFG) (\url{http://www.ffg.at/comet}) 
like COMET, ICT of the Future, and Production of the Future. Currently this network
consists of more than 30 companies. In total the volume of projects based on the contributions of these companies was about
24 person years of work. The current outlook for the upcoming period 2019 to 2022 show an increased demand in
applied research for data analysis and computer vision systems which amounts roughly to a total volume of 30 person years of work. The envisioned
applications during this period are characterised by a growing demand on computational speed and data throughput.
Hence any acquired fundamental knowledge gained in the development of data-intensive analysis applications can
be exploited in those applied research projects and thus transferred from academics to industry (which is \SCCHshort{}'s
mission). Furthermore it will strengthen the data analysis, machine learning and computer vision competencies and
thus will increase the contract research volume.

\textbf{Scientific Exploitation.} In parallel to its contract research, \SCCHshort{} also needs to conduct more fundamental research,
as in the \TheProject{} project, in order to keep in contact with world-leading research. \SCCHshort{}'s success
is also measured by its scientific output, including e.g. the number of publications, PhD and Masters theses.

\horizontalline

\subsection*{Draft Exploitation Plan for \GOLEMshort{}}

\begin{wrapfigure}{R}{4cm}
\vspace{-1.4cm}
\hfill \includeimage[width=4cm]{logos/golem-logo.jpg}
\vspace{-0.8cm}
\end{wrapfigure}

As an industrial SME partner, GOLEM will actively exploit the new RDI and IPR results of the \TheProject project by the implementation of new more effective, reliable and safe versions of Smart City Monitor platform prototypes that are installed in small and medium cities, utility providers and various industrial enterprises in the EU and elsewhere. It will increase company know-how and capacity to apply the newly developed \TheProject technology to a broad class of complex and computationally intensive applications running large custom CPS models for Smart Cities, Factories of the Future and Industry 4.0, Environment, Energy, Water, Waste, Transportation, Buildings, Health and others. The novel technology requires real time analysis of large number of big data streams from IoT, and calculation of custom indicators and the waves of complex indicator dependency chains and multiple object models running as concurrent threads in multicore processors, and the computing of ongoing statuses of multiple system objects as well as their combinations, and prediction of such statuses in the some future. Finally the applications shall calculate and implement various API controls and other actions based on dynamically defined rules and predictions of optional impacts on system sustainability along with strong requirements for security, safety, reliability and conformance to standards. 

\TheProject results would significantly increase GOLEM potential and competitiveness in developing cutting-edge innovations and bringing the practical digital transformation IoT solutions that are more safe and secure, cost effective in maintenance and compliant to the industry standards to new level of sophistication and intelligence for the international market.  
The exploitation opportunities are effectively scaled up by the company active participation in the EIP SCC, standardization activities by ITU Focus Group ``Data Processing Management for IoT and Smart Cities'' and diverse H2020 projects that require modelling of complex cyber-physical systems that link physical and virtual worlds. In addition GOLEM has 25 years' solid history of outsourcing support for the \emph{United Nations Industrial Development Organization} (UNIDO) in implementation of its advanced technology in various international projects for sustainable inclusive development in green metropolitan areas, industries and circular economy. These contacts both in private and public sectors provide excellent, relevant, and high-level channels enabling \GOLEMshort with effective exploitation of \TheProject results both during and beyond the duration of \TheProject.


\horizontalline

\subsection*{Draft Exploitation Plan for \CODEPLAYshort{}}

\begin{wrapfigure}{R}{3.8cm}
\vspace{-1.4cm}
\hfill \includeimage[width=4cm]{logos/codeplay.png}
\vspace{-1cm}
\end{wrapfigure}

\CODEPLAYlong{} supplies 30\% of the worlds AI processor vendors. The most
widely-used AI framework today, \emph{TensorFlow}, requires users to download
\CODEPLAYlong{}s SYCL implementation (\texttt{ComputeCpp}) when building on
non-NVIDIA platforms. \TheProject will leverage this market strength in
tools for heterogeneous processors to drive adoption of the \CODEPLAY
tools. \CODEPLAY needs to be able to provide a roadmap to supply an
updated set of standards-based tools for safety critical applications to
its customers. However, this is only possible if there is an
industry-wide adoption of open standards, so that there is an entire
ecosystem of tools, processors, processes, validation suites and services
available for automotive and medical customers. Without independent
validation of the tools, or open standards that define how the tools
should interoperate, the industry cannot take on the open ecosystem
approach that Codeplay supports.

To bring the tools to market will require extra investment to productize
the technologies after the project ends. The funding to enable this is
only possible if there is evidence of market adoption of an
open-standards approach to safety critical intelligent sensing. \CODEPLAY
will use the \TheProject{} project to demonstrate how this open approach
can be adopted throughout the industry supply chain, as well as to bring
about industry consensus to define the open standards. That will then
unlock further funding to bring \CODEPLAYs tools to market, as well as
enable other companies in the market to provide the services and
independent validation required to enable market success.

\CODEPLAYlong{} will directly exploit the results of the \TheProject{}
project in its commercial tool chain offering, in particular in its SYCL
implementation \texttt{ComputeCPP}. The run-time optimisations, such as improved
JIT processing (better scheduling of kernel compilations as well as
dynamic optimisations of large systems of kernels with a large number of
dependencies) inside a SYCL runtime, are expected to to generate a
significant performance improvement which will greatly benefit \texttt{ComputeCPP}. The
ability to support different platforms is also expected to lead to better
performance, particularly of the runtime but also of kernel execution on a
variety of devices.
%
The support for static mapping will improve the integration of SYCL into
existing projects, in particular controlling optimisations performed by
the SYCL implementation (runtime and compilation) from a higher level
application/model.

The distributed runtime implementation is expected to be made available
in source form (using appropriate licenses, perhaps even open source) to
researchers and commercial adopters to enable them adapt or fine-tune the
runtime and underlying processes for their purposes. Codeplay will build
upon the distributed runtime for further research on inter/intra-node
programming models.
%
\CODEPLAY also expects some of the results from this research to feed
into the standardisation of SYCL, and even Vulkan, OpenCL and
C++. Specifically, SYCL does not currently have distributed support, but
is aimed specifically for heterogeneous computing. We would take the
distributed results from this project and adapt it for further
standardisation in Khronos SYCL, while also building it into ISO C++
where we are leading the effort at adding heterogeneous and distributed
computing through Study groups SG14, and SG1.
%
We would further take the safety critical aspects and inject our findings
into MISRA, C++ Core Guidelines, and C Safety Critical
Group. Furthermore, we would use the patterns tested through this project
to be libraries that can be added to future C++ Standard Library, as well
as supported by SYCL. These will form valuable additions for future
standardisation of distributed and heterogeneous frameworks.

\horizontalline

\subsection*{Draft Exploitation Plan for \SAshort{}}

\begin{wrapfigure}{R}{2cm}
\vspace{-1.4cm}
\hfill \includeimage{logos/st-andrews-logo.jpg}
\vspace{-0.9cm}
\end{wrapfigure}

\SAshort{} will work to actively exploit the foreground IPR that it
produces in the course of the \TheProject{} project, both in
collaboration with industrial partners as part of the overall plan for
exploitation that is described above, and through its own independent exploitation
routes. 
During the course of the \TheProject{} project,
we envisage the production of new and innovative type systems and tools for
reasoning about time, energy and other properties at the source level.
Our intention is to release these tools and type systems on an open source basis during
the course of the project, so allowing their widespread exploitation.
However, they will also form the basis for commercial exploitation as part of a major
ongoing development effort in tools for parallelisation at \SAshort{}.
%
We have already successfully obtained \pounds{537K} in funding from the UK
Government (\paraphrasing{}) to investigate commercialisation and
innovation opportunities arising from previous EU- and UK-funded work
in parallelisation tools for heterogeneous multicore/manycore
machines.  We currently have a team of full-time developers working on the production of new refactoring-based tooling
that will be integrated into widely-used IDEs, including Eclipse and Microsoft's Visual Studio.
This tooling will support the complete development process for heterogeneous parallel systems,
including debugging, performance visualisation, collaboration, error checking, etc.
%We expect to form a new startup company to sell the tooling in early 2018, with
%the goal of achieving a multi-million Euro turnover within five
%years of company formation.  This new company will be in a position to
%directly exploit the new research results and foreground IPR that will arise from
%\TheProject{}, as part of its ongoing business development plan.  
\SAshort{} have incorporated a spin-off company, \textbf{ParaFormance Technologies LTD}, registered with Companies House in the UK in October 2017 as Company Number \textbf{SC579101}. 
Our goal is to achieve a multi-million Euro turnover within five years of company formation, building a
high-growth European company of scale.
In addition to applying for patents, etc., relevant foreground IPR developed by \SAshort{} on \TheProject{} can be offered to the spin-off company
or other companies in order to commercialise the project results.
Scotland
provides an excellent, vibrant base for commercial exploitation, with
specific support available from the Scottish government and other
sources to incubate and grow new
businesses. % , as well as through the University.
In Edinburgh
alone, % Edinburgh University has actively supported the formation of
162 start-up/spin-off companies have been formed from academic
research since 2007, and two new science parks are currently being
developed.  Edinburgh TechnoPole is a new 126-acre science and
technology park that offers flexible accommodation to new and growing
high-technology businesses.  At St Andrews, the proposed new Eden business
campus will provide an excellent base for growing the new startup company.

% % but we will also work with the industrial partners to further exploit these
% % tools and any other foreground intellectual property that we will produce in the
% % course of the project.
% % We are currently engaged in a pre-commercialisation project (\paraphrasing), funded
% % by Scottish Enterprise, that aims to develop new tooling for heterogeneous \mcore systems
% % and will use this to enable direct commercial exploitation of the \TheProject{} results.
% % Where joint exploitation through an existing company is not possible,
% % \SAshort{} will actively investigate the formation of a spin-off company
% % to commercialise the project results, and
% We will apply for patents etc. as necessary and appropriate to ensure that we can effectively exploit.
% We also foresee possible direct exploitation through high-level
% consultancy related to the use of the systems and tools that we will produce.
% %
% Scotland provides an excellent, vibrant base for commercial
% exploitation, with specific support available from the Scottish
% government and other sources. % , as well as through the University.
% In Edinburgh alone, % Edinburgh University has actively supported the formation of
% 162 start-up/spin-off companies have been formed from academic research since 2007, and two new
% science parks are currently being developed.  Edinburgh TechnoPole
% is a new 126-acre science and technology park that offers flexible
% accommodation to new and growing high-technology businesses.
\SAshort{} has excellent connections within the UK and Scottish communities.
\textbf{Prof. Kevin Hammond} is the 
recently-appointed Director of SICSA (\url{http://www.sicsa.ac.uk}), the pan-Scottish pooling agreement for Informatics and Computer Science,
which groups the 14 Scottish universities.
SICSA offers direct support for entrepreneurial and knowledge exchange activities,
and has established close links with the new Scottish Innovation Centres in 
Big Data,
Sensor Networks,
Digital Health, 
and Energy,
as well as with trade bodies including ScotlandIS, with Informatics Ventures, and with the Scottish Digital Skills Partnership.
These contacts provide excellent, relevant, and high-level contacts with industrial, public-sector and other potential end-users
of the \TheProject{} tooling at all scales
from small businesses to multi-national conglomerates.
% To achieve this, SICSA works
% closely with, an organisation which aims to nurture
% an entrepreneurial ecosystem to exploit computer science knowledge and
% expertise.
We will engage with these, and other bodies such as the EU TETRACOM project to exploit our results
effectively and efficiently both during and beyond the duration of \TheProject.

\paragraph{Teaching.}
\SAshort{} aims to exploit the results of the \TheProject{} project
throughout its teaching profile.  It offers advanced undergraduate,
MSc, and PhD level courses in parallel programming, programming language design
and implementation, concurrency, image processing, data analytics, artificial intelligence and verification.
All of these courses will be able to benefit from the \TheProject{}
results on compilation and parallelisation.  Results will be integrated into
the material that is taught, they will be used to inform and adapt the
curriculum, and they will be used to design practical coursework exercises,
essays and other forms of assessment.
In addition, large-scale full-year individual and/or group
projects will be offered on topics that are relevant to \TheProject{}.
These topics will include, for example, parallel compilation, runtime optimisation,
timing and energy analysis etc., and will be available at either undergraduate or MSc level
or both.
We will also seize opportunities to direct PhD and EngD student research towards
areas that are of interest to \TheProject.  The proposed work is a close
fit to the research interests of several academic staff at \SA{}, and
we have already started to explore how e.g. 
how to use our refactoring techniques to introduce parallelism in large-scale software,
how new patterns of parallelism can be automatically
identified from within existing a code-base, and how legacy software can be transformed so that
it is parallelisable.
Finally, where relevant, we will give open seminars and
run courses for industry on e.g. using the tools and techniques that will be developed in the \TheProject{} project,
understanding patterns of parallelism, exploiting many-core architectures, and dealing with complex time/energy etc. optimisation problems.

\pagebreak
\horizontalline
\subsection*{Draft Exploitation Plan for \INRIAshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{4cm}
\vspace{-1.4cm}
\hfill \includeimage[width=4cm]{logos/logo-inria.jpg}
\vspace{-1cm}
\end{wrapfigure}


% Paragraph taken from:
% https://intranet.inria.fr/Vie-pratique/S-informer-Communiquer/Discours-de-presentation-d-Inria
The mission of \INRIAshort, the French National Institute for computer
science and applied mathematics, is to {\em promote scientific
  excellence for technology transfer and society}. With its open,
agile model, \INRIAshort is able to explore original approaches with its
partners in industry and academia and provide an efficient response to
the multidisciplinary and application challenges of the digital
transformation. Committed to assisting innovators, \INRIA provides the
ideal conditions for fruitful relations between public research,
private R\&D and industry. Inria transfers its expertise and research
results to startups, SMEs and major groups in fields as diverse as
healthcare, transport, energy, communications, security and privacy
protection, smart cities and the factory of the future. Inria has also
fostered an entrepreneurial culture that has led to the creation of
120 startups. The PARKAS and AOSTE teams of Inria, in particular, have
a successful history of scientific excellence and technology transfer.
% Recent prize of Marc ?
%
The objectives of \INRIA{} within \TheProject{} fully fit its mission and
previous record of the two teams involved -- PARKAS and AOSTE.
We will:
\begin{itemize}
\item Develop parallel patterns, refactoring methods, and tools for
  the modelling, synthesis and mapping of next-generation control and
  monitoring embedded applications onto low-power manycore and
  heterogeneous hardware. Specific challenges addressed by \TheProject{},
  materialised in extensions of the state-of-the-art tools \textbf{Heptagon}
  and \textbf{Lopht}, will include:
  \begin{itemize}
  \item The efficient implementation of computationally-intensive
    applications, which require the use of complex code refactoring
    and mapping. 
  \item A formally sound link to C/C++ level providing model-level
    visibility to lower-level code transformations.
  \item Improved handling of extra-functional properties such as fault
    tolerance and isolation for timing predictability and security.
  \end{itemize}
  
\item Demonstrate the methods and tools on {\em automotive} case
  studies. Following successful (and ongoing) industrial
  collaborations, the \textbf{Heptagon} and \textbf{Lopht} tools have come to provide
  realistic solutions to domain-specific problems of the avionics
  community, some of which have been already successfully transferred
  to industry. In the perspective of continued industry transfer,
  where cross-discipline mapping tools are sorely needed, it is now
  important to extend our methods and tools to cover the specific
  needs of the automotive field. They include, but are not restricted
  to:
  \begin{itemize}
  \item A more generalized use of Mathworks \textbf{Simulink/Stateflow} at
    various stages of the design flow.
  \item A different set of standards, and in particular \emph{AUTOSAR}, which
    specifies in detail the execution mechanisms of the OS and the set
    of artefacts that need to be produced, and \emph{ISO 26262}, which defines
    functional safety objectives.
  \item The cost envelope, much lower in automotive than in avionics,
    which in particular imposes the use, to a greater extent, of
    commercially available off-the-shelf (COTS) hardware and
    subsystems.
  \end{itemize}
  Increased automation would provide clear benefits to this field
  (cost reduction, reduced time-to-market, improved safety and
  security). We expect to demonstrate safe and efficient mapping of
  next-generation embedded control and monitoring applications onto
  COTS hardware including an ARM multi-core and a next-generation
  low-power and heterogeneous manycore, under real-world automotive
  industrial scenarios.

\end{itemize}
% Thus preparing the way to their industrialization 

\horizontalline

\subsection*{Draft Exploitation Plan for \AGHshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{2.5cm}
\vspace{-1.45cm}
\hfill \includeimage[width=2cm]{logos/agh.jpg}
\vspace{-0.4cm}
\end{wrapfigure} 


\AGHlong is one of the best technical universities in Poland. We are involved
in the wide community of companies and research units potentially
interested in the results and products of the \TheProject{} project. We will
actively promote and apply the results in various fields, including
industrial applications, R\&D projects, research, scientific
publications, conferences and teaching.

Being located in the new ``silicon valley'' still forming around the old,
beautiful city of Krakow, a former Capital of Poland, the \AGHlong{} has
many great opportunities to directly promote developed
solutions. \AGHshort{}, in particular the Department of Computer Science
actively cooperates with industrial partners, including companies like
Motorola, CISCO, Delphi, IBM, AKAMAI, SAMSUNG, Erlang Solutions Poland
and many others that are located in the vicinity of Krakow and in other cities in
Poland. \AGHshort{} leads a significant number of R\&D projects, which
pose great opportunities for possible applications. \AGHshort{} will also
promote using the results of the \TheProject project in future R\&D
proposals submitted by the University.

The results and products of the \TheProject{} project will be presented on various IT
conferences and events taking place in Poland. This will include
scientific conferences, like FedCSIS, PPAM or National Conference on
Software Engineering and more technical conferences, such as \emph{LambdaDays}. The
results also will find applications in various scientific research
projects in different domains, which often require tools for simplifying
the development and increasing the effectiveness of software.

\AGHshort{} will actively disseminate the research outcomes in the Polish
scientific networks that are focused on computing aspects, e.g. the Polish Society of
Artificial Intelligence, the Polish Computer Science Society and the
community around the AGH Cyfronet Supercomputing Centre.

\AGHshort{} will also exploit the results of the project in educational
activities. The results will be presented on several courses related to
parallel and distributed programming. We will also verify different applications or
extensions of the solutions within project classes, MSc and PhD
theses, aiming to extend the impact of the project by writing common
publications with our students and encouraging them to join the project
community.

\horizontalline

\subsection*{Draft Exploitation Plan for \ JMOIC{}}
\vspace{-6pt} 

\begin{wrapfigure}{R}{2.2cm}
\vspace{-1.1cm}
\hfill \includeimage[width=2cm]{logos/JMOIC.jpg}
\vspace{-0.7cm}
\end{wrapfigure}


\JMOIClong
% Jelgava Municipality Operative Information Centre 
 provides 24/7 services to citizens, public utility providers and city
administration. It includes monitoring of the city public area, different
kinds of infrastructural objects (street lights; pumping stations;
meteorological stations; intelligent traffic light management, smart
parking, fire, energy efficiency, schools, public security and buildings,
etc), municipality infrastructure maintenance, measures for risk
prevention and mobilising operational support for civil protection
assistance in the event of major disasters (natural and man-made
disasters, acts of terrorism and, technological, environmental accidents
etc.) in the whole Jelgava region including peri-urban areas.  \JMOIC
applies leading European ICT/IoT solution Smart City Monitor for
obtaining quantifiable holistic information about urban processes and its
sustainability assessment in real time to support smart sustainable
governance. The Jelgava Smart City project implementation requires
connecting large number of diverse urban data sources both from smart
sensors (energy-water-traffic meters, temperature, humidity, CO2, etc) as
well as databases and spreadsheet files. The secure and reliable
processing and transformation of this data into trustful real time
information for decision makers and \JMOIC operators are of key importance
for prompt and preventive actions.  \JMOIC will take part in the
\TheProject{} providing real world operational environment for evaluation
of the project results, its improvements and demonstration. As industrial
use case \JMOIC expect effective project results improving Smart City
Monitor capacity to run large scale city model linked to multiple real
time data streams and ensuring high performance, safety-critical, secure,
and distributed heterogeneous computing to support effective information
services for citizens, local businesses and administration.  The project
results will be demonstrated to numerous Jelgava partners in multiple
cross-border projects such as the {ECHO} project ``The Baltic everyday
accidents, disaster prevention and resilience'', ``Liquidation of
Ecological Catastrophes and Pollution in the Territory of the Lielupe
River Basin'' and various Baltic states cross border cooperation
programs. It will contribute to the ongoing project ``Thermal Energy
Resource Modelling and Optimisation System'' (THERMOS H2020 723636).

\horizontalline


\subsubsection{Knowledge Management and Protection}
\vspace{-12pt}

\eucommentary{Outline the strategy for knowledge management and protection. Include measures to provide open access (free on-line access, such as the 'green' or 'gold' model) to peer-reviewed scientific publications which might result from the project}

Before the project starts, all project partners will agree on explicit rules concerning IP ownership, access rights to any
Background and Results for the execution of the project and the
protection of intellectual property rights (IPRs) and confidential
information as part of the Consortium Agreement.
As part of the Consortium Agreement, in order to ensure a smooth
execution of the project, the project partners will agree to grant each other
royalty-free Access Rights to their Background and Results for the
execution of the project. The Consortium Agreement will define further
details concerning the Access Rights after the duration of the project 
with respect to Background and Results.

\paragraph{Dissemination and Communication:}
While fully taking into account issues of potential exploitation and IPR ownership by project partners
as governed by the Consortium Agreement,
the project aims to provide good general access to its research results.
Balancing access with cost, the project will therefore generally adopt a ``green'' model to open access for publications,
but has included funding to support targeted ``gold'' open access for key publications.
The academic partners all maintain suitable institutional repositories, which will allow public access to research papers produced in the
course of the project, perhaps with some moratorium.  Some publishers (e.g. the ACM) also provide links that allow
free access to their publications from authors' home pages, and this will be exploited wherever possible.
% y, where publisher charges are not excessive, the project will consider ``gold'' open access to key project publications.
%
Furthermore, and perhaps of most significance, the project website will provide free,
open and publicly searchable access to all the public deliverables, to technical reports, data and results, to software tools
and libraries, to white papers and also to all
the other non-confidential documents that are generated in the course of the project.  
% The material released in this way will represent the vast bulk of the scientific
% and technical output of the project.

%\subsubsection*{Intellectual Property Rights}

\pagebreak
\paragraph{IP Ownership.}

Results shall be owned by the project partner carrying out the work
leading to such Results. If any Results are created jointly by at least
two project partners and it is not possible to distinguish between the
contributions of each of the project partners, such Results, including
inventions and all related patent applications and patents, will be
jointly owned by the contributing project partners. In order to further
the competitiveness of the EU market, and to enhance exploitation of the
Consortium Results, each contributing party shall have full own freedom
of action to exploit the joint IP as it wishes, and further the goals of
the consortium. To promote this effort the contributing party will have
full own consideration regarding their use of such joint Results and will
be able to exploit the joint IP without the need to account in any way to
the other joint contributor(s).Further details concerning jointly owned
Results, joint inventions and joint patent applications will be addressed
in the Consortium Agreement.

\paragraph{Transfer of Results.}

As Results are owned by the project partner carrying out the work leading
to such Results, each project partner shall have the right to transfer
Results to their affiliated companies/organisations without prior notification to the
other project partners, while always protecting and assuring the Access
Rights of the other project partners.  Such use of Results will encourage
competitiveness of the EU market by creating broader uses of the Results
and opening up the markets for the Consortium's Results in all markets.

\paragraph{Open Source and Standards.}

A central aim of this consortium is to provide benefit to the European community.  Some of the project partners may be either using Open Source code in their deliverables or contributing their deliverables to the Open
Source communities. Alternatively, some of the partners may be contributing to Standards, be they open standards or other. Details concerning open source code use and standard contributions will be
addressed in the Consortium Agreement.

The base technologies being developed by the academic partners within the duration of the project will be published under Open Source Licenses except where specified under Intellectual Property Management to allow the broader community to benefit from the outcome of this project.

The major validated project results  will be contributed to the key international standardisation bodies  such as ISO, ITU and others where the consortium members take part in. 


\paragraph{Data Management Plan}
The primary research data that will be produced by the project will be the performance results that are reported in
various research publications.  This data will predominantly be scientific,
without confidentiality restrictions, and it will
therefore be made available through the project website, in line with the agreements that will be
made in the Consortium Agreement.  As far as possible, 
this data will be recorded in a human-readable form, such as plain ASCII text
or XML.  Where this is not possible, converters will be provided to make the data accessible to other researchers
in a human-readable form. 
The research data on the project website will be associated with the relevant research publications. 
We have budgeted for adequate disk and processing capacity to allow for the expected access to this data.
The project website will be maintained after the end of the project, but to ensure long-term continuity
and value, data will also be transferred to the \SAshort{} institutional data repository.

\draftpage
\subsubsection{Communication Activities}
\label{sect:comm-activities}

\eucommentary{Describe the proposed communication measures for promoting the project and its findings during the period of the grant. Measures should be proportionate to the scale of the project, with clear objectives. They should be tailored to the needs of various audiences, including groups beyond the project's own community. Where relevant, include measures for public/societal engagement on issues related to the project.}

As described in the draft dissemination plan above, and in the description of~\ref{wp:dissem} (page~\pageref{wp:dissem}),
the \TheProject{} project aims to communicate itself and its findings intensively to various communities.
Research publications and presentations will aim to target various groups of academic and industrial researcher,
including parallel programmers, big data researchers, and programming language
designers, and will add scientific weight and credibility to our findings.  Press releases and news articles will be used to communicate project results and major
project life events (start, finish, key milestones) to both a technical and general audience.
We will also take advantage of opportunities as they arise for radio/TV interviews, public seminars and general articles
in both the technical and non-technical press.
The project website will be used to provide open access to project results, public deliverables,
software tools, technical reports, white papers, (video) tutorials, podcasts etc., and will serve
as a key resource for those wishing to use the project results, whether they are acting as an academic researcher, scientific, commercial or independent
software developer, public sector worker,  educator or private individual.
By making research results public in this way, we especially aim to engage with the software developer
community, who may not normally have access to academic papers and reports.
We will disseminate information about our tools and standards directly to customers, aiming to
increase engagement with an already motivated group of developers/users.
We will run open technical workshops that will showcase our work to interested parties.
These will generally be co-located with major networking events, such as the annual HiPeac
conference and the HiPeac spring/autumn gatherings.
We will also engage with relevant industrial/developer conferences, grass-roots meetings, workshops etc.,
producing poster and demonstrations as necessary to communicate with the broader developer community
and especially with project managers and decision makers.
Finally, we will actively participate in standardisation activities through e.g. the ISO C++ standard committee and ITU FG-DPM to support IoT and Smart Cities and Communities,
aiming to influence development and awareness of the \TheProject{} results.

\clearpage

% ---------------------------------------------------------------------------
%  Section 3: Implementation
% ---------------------------------------------------------------------------


\clearpage
\section{Implementation}

\subsection{Work Plan --- Work Packages, Deliverables and Milestones}
\label{sect:workplan}

%% \eucommentary{Please provide the following:\\
%% \begin{itemize}
%% \item
%% brief presentation of the overall structure of the work plan;
%% \item
%% timing of the different work packages and their components (Gantt chart or similar);
%% \item
%% detailed work description, i.e.:
%% \begin{itemize}
%% \item
%% a description of each work package (table 3.1a);
%% \item
%% a list of work packages (table 3.1b);
%% \item
%% o a list of major deliverables (table 3.1c);
%% \end{itemize}
%% \item
%% graphical presentation of the components showing how they inter-relate (Pert chart or similar).
%% \end{itemize}
%% }

\begin{figure}[tp]
\begin{center}
\vspace{-5mm}
\begin{tabular}{ll}
%\hspace{-0.75in}
\includeimage[scale=0.5]{RePhorm2Pert.pdf}
%\vspace{-25mm}
\end{tabular}
\caption{Overview of the \TheProject{} Workpackage Structure and Dependencies (PERT chart)}
\label{fig:wps}
\end{center}
\end{figure}

\subsubsection*{Overall Structure of the Work Plan}

The work plan is broken down into 6 technical workpackages as shown
in \textbf{Figure~\ref{fig:wps}}: WP2 deals with block-diagram modelling and compliance diagnostics; WP3 deals with C/C++/SYCL components and software-defined infrastructures;
WP4 deals with the compilation and runtime system; WP5 deals with the definition and optimisation of key extra-functional properties
of performance etc.; WP6 deals with security and safety aspects; and \ref{wp:dissem} deals with overall requirements, metrics, benchmarks, use cases
and evaluation of the project.
In addition, there is one management work package (WP1) and one
general dissemination work package (\ref{wp:dissem}). The Gantt chart on
Page~\pageref{fig:gantt} illustrates the timeline for the
various tasks for these work packages, including inter-task
dependencies.

%\newpage

\input{deliverables}

\bigskip\bigskip
\addtocounter{subsubsection}{1}
\addcontentsline{toc}{subsubsection}{\protect\numberline{\thesubsubsection}Work
Package List}
\fbox{\begin{minipage}{\textwidth}\begin{center}{\Large\bf
        Work package list} % (full duration of project)}
  \end{center}
  \end{minipage}}

\bigskip\bigskip

\begin{tabular}{|p{1.2cm}|p{9cm}|p{0.8cm}|p{1.35cm}|p{1cm}|p{0.9cm}|p{0.9cm}|}
\hline
{\bf Work \mbox{package} No} & {\bf Work package title} &
{\bf Lead \mbox{partic.} no.} &
{\bf Lead short name} &
{\bf Person months} & {\bf Start month} & {\bf End month} \\\hline 

\newcounter{wp}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{USTAN}{24}{1}{36}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{INRIA}{72}{1}{35}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{SA}{61}{1}{34}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{CODEPLAY}{48}{1}{34}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{SCCH}{46}{1}{34}

\addtocounter{wp}{1}\workpackageentry{\thewp}{IBM}{77}{1}{34}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{GOLEM}{84}{1}{36}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{USTAN}{44.5}{1}{36}

{\textbf{Total}} & & & &
\textbf{\large 456.5}&
&
\\\hline
\end{tabular}


%% Covered above.
%\newpage
%% \subsubsection*{How the Work Packages will Achieve the Project Objectives}
%% \label{sssec:how_the_work_packages_will_achieve}

%% \TOWRITE{KH}{This needs to explain that we're actually going to meet the   objectives.  Needs to be done after objectives and WPs.}

%% The project objectives (Section~\ref{sect:objectives},
%% page~\pageref{sect:objectives}) and the corresponding work
%% packages that contribute to achieving those objectives are:

%% \begin{center}
%% \begin{tabular}{|l|l|l|}\hline
%% \textbf{Objective} & \textbf{Purpose} & \textbf{WPs} \\\hline \hline
%% Objective 1 & XX & \textbf{WP X} \\\hline
%% Objective 6 & Application-focused Research/Development Methodology & \textbf{WPX} \\\hline
%% % Objective 7 & User Community Building & \textbf{WP X} \\\hline
%% Objective 8 & Open Science & \textbf{\ref{wp:dissem}} \\\hline
%% \end{tabular}
%% \end{center}

%% \paragraph*{Work Programme for Objective 1: }

%% Objective 1 is covered by WPX, which will ...

%% \paragraph*{Work Programme for Objective 6: }

%% Objective 6 is covered by WPX, which will evaluate our work
%% with applications provided by industrial and academic partners.
%% Application areas that we will consider comprise: ...


%% % \paragraph*{Work Programme for Objective 7: }

%% % Objective 7 is covered by WP7, which will promote the use of
%% % \TheProject{} tools and technologies in the wider community.

%% To promote the technologies developed in the project we will
%% create user-level documentation for the programming tools
%% (which at the moment wither does not exist or, for some tools,
%% exists in the from of patchy notes). A web site will be
%% designed and implemented via which the user community will be
%% supplied with documentation and demonstrator tools and feedback
%% will be gathered. This process will start on the first day of
%% the project in the form of review of existing materials, and
%% then it will progress in pace with the technology development.

%% \paragraph*{Work Programme for Objective 8: }

%% Objective 8 is covered by \ref{wp:dissem}, which involves ensuring the
%% long-term dissemination of the \TheProject{} research results,
%% with a view to boosting European competitiveness. Such results
%% can be roughly construed as software, publications, and
%% research progress.
%% %
%% We will build a software repository using the \TheProject{}
%% portal and allow the software to be licensed under the EUPL,
%% maintained at the Open Source Observatory and Repository for
%% European public administrations (OSOR.eu)~\cite{Hollmann08}.
%% %
%% In terms of publications, we will liaise with the FP7 OpenAIRE
%% infrastructure located at the University of Ghent in
%% Belgium~\url{http://www.openaire.eu/}.
%% %
%% Finally, research progress and general public announcements
%% will take advantage of the EU CORDIS
%% service~\url{http://cordis.europa.eu/}.


\landscape

\subsubsection*{Work Plan Timing: GANTT Chart showing Task Dependencies and Information Flows}


%\vspace{-0.7in}
\centerline{\hbox to \columnwidth{\hss%
    \includeimage[scale=0.9]{RePhorm2Gantt.pdf}
\hss}}
\label{fig:gantt}
\vspace{-1in} % Fool LaTeX into avoiding unnecessary page break
\endlandscape

\newpage
%\bigskip\bigskip\bigskip

%% Set up the milestone numbers.
\input{milestones}

\fbox{\begin{minipage}{\textwidth}\begin{center}\Large\bf List of Milestones
  \end{center}
  \end{minipage}}
\label{sect:milestones}

\bigskip

\khcomment{Added MS.  Would be better to use milestone refs from milestones.tex and deliverable refs from deliverables.tex...}
\newcounter{ms}
\renewcommand{\thems}{MS\arabic{ms}}
\begin{minipage}{\textwidth}
\begin{center}
 \begin{tabular*}{\textwidth}{|p{1.5cm}|p{8.3cm}|p{1.2cm}|p{0.6cm}|p{4.2cm}|}  \hline
 \textbf{MS No.} & \textbf{Milestone name} & \textbf{Related WPs} & \textbf{Est. date} & \textbf{Means of
   verification} \\ % (success criteria below)} \\ % (deliverables shown here + success criteria below) \\
\hline
MS1 & Initial Requirements Analysis & WP7 & M3 & D7.1 \\
\hline 
MS2 & Initial C/C++/SYCL Components & WP3 & M9 & D3.1 \\
\hline
MS3 & Initial \TheProject{} Infrastructure & WP2--WP6 & M10 & D2.1, D3.2, D4.1, D5.1, D6.1 \\
\hline
MS4 & Initial Benchmarks and Use Cases & WP7 & M11 & D7.2 \\
\hline
MS5 & Updated Requirements Analysis & WP7 & M12 & D7.3 \\
\hline
MS6 & Initial Security and Safety Compliance & WP6 & M14 & D6.2 \\
\hline
MS7 & Initial Model-Level Compliance and Diagnostics & WP2 & M18 & D2.2 \\
\hline
MS8 & Refined Optimisation Infrastructure & WP5 & M20 & D5.2 \\
\hline
MS9 & Refined C/C++/SYCL Components and Patterns & WP3 & M21 & D3.3 \\ 
\hline
MS10 & Refined Block-Diagram Modelling, SDIs, Runtime System and Safety/Security Mechanisms & WP2--WP4, WP6 & M22 & D2.3, D3.4, D4.2, D6.3 \\
\hline
MS11 & Refined Benchmarks and Use Cases & WP7 & M23 & D7.4, D7.5 \\
\hline
MS12 & Final Requirements Analysis  & WP7 & M24 & D7.6 \\
\hline
MS13 & Final Safety and Security Compliance  & WP6 & M30 & D6.4 \\
\hline
MS14 & Final C/C++/SYCL Components and Pattern  & WP3 & M33 & D3.5 \\
\hline
MS15 & Refined Block-Diagram Modelling, SDI, Runtime System and Safety/Security Mechanisms & WP2 -- WP6 & M34 & D2.4, D3.6, D4.3, D5.3, D5.4, D6.5\\
\hline
MS16 & Final Model-Level Compliance and Diagnostics & WP2 & M35 & D2.6 \\
\hline
MS17 & Final Benchmarks, Use Cases and Roadmap & WP7 & M36 & D7.6, D7.7\\
\hline
\end{tabular*}
\end{center}
\end{minipage}

\setcounter{ms}{0}
\vspace{20pt}
\begin{center}
\begin{tabular*}{\textwidth}{|p{1.2cm}|p{13.3cm}|p{2.2cm}|}\hline
\textbf{MS No.} & \textbf{Success Criteria} & \textbf{Contributes to
  Objective(s)} \\
  \hline
MS1 & Initial Requirements Analysis Completed & \textbf{1 -- 8} \\
  \hline
MS2 & Defined C/C++/SYCL Components for Shared-Memory Systems  & \textbf{1, 2} \\
  \hline
MS3 & Developed Block-Diagram Modelling, SDIs, Runtime Systems, Optimisation and Security/Safety Mechanisms for Shared-Memory Systems & \textbf{1 -- 4} \\
  \hline
MS4 & Developed and Evaluated Initial Versions of Benchmarks and Use Cases for Shared-Memory Systems  & \textbf{6, 7} \\
  \hline
MS5 &  Completed Updated Requirements Analysis & \textbf{1 -- 8} \\
  \hline
MS6 & Developed Initial Version of Safety and Security Standards Compliance Mechanisms & \textbf{5} \\
  \hline
MS7 & Developed Initial Model-Level Compliance and Diagnostics & \textbf{5} \\
  \hline
MS8 &  Developed Dynamic Optimisation Infrastructure & \textbf{4} \\
  \hline
MS9 & Produced C/C++/SYCL Components for Small-Scale Distributed Systems  & \textbf{1, 2} \\
  \hline
MS10 & Produced Block-Diagram Modelling, SDIs, Runtime Systems and Safety/Security Mechanisms for Small-Scale Distributed Systems & \textbf{1 -- 4} \\
  \hline
MS11 & Developed and Evaluated Benchmarks and Use Cases for Small-Scale Distributed Systems  & \textbf{6, 7} \\
  \hline
MS12 & Conducted Final Requirements Analysis & \textbf{1 -- 8} \\
  \hline
MS13 & Produced Final Safety and Security Standards Compliance Mechanisms  & \textbf{5} \\
  \hline
MS14 & Developed  C/C++/SYCL Components for Large-Scale Distributed Systems  & \textbf{1, 2} \\
  \hline
MS15 & Developed Block-Diagram Modelling, SDIs, Runtime System, Optimisation and Safety/Security Infrastructure for Large-Scale Heterogeneous Systems  & \textbf{1--4} \\
  \hline
MS16 & Developed Final Model-Level Compliance and Diagnostics Mechanisms for Large-Scale Distributed Applications & \textbf{5} \\
  \hline
MS17 & Produced Benchmarks and Use Cases for Large-Scale Distributed Systems Developed and Evaluated, Future Roadmap  & \textbf{6, 7} \\ \hline
\end{tabular*}
\end{center}

%\landscape
\newpage
\fbox{\begin{minipage}{\textwidth}\begin{center}\Large\bf List of Deliverables
  \end{center}
  \end{minipage}}
\label{sect:deliverables}

\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{|p{0.8cm}|p{9.25cm}|p{0.8cm}|p{1.15cm}|p{1.6cm}|p{0.8cm}|p{0.8cm}|}  \hline
\textbf{Del. no.}              & \textbf{Deliverable name}        & \textbf{WP no.} & \textbf{Lead}
& \textbf{Type}              & \textbf{Dis. level}   & \textbf{Del. date}
\\ \hline

%% Year 1

\ref{mgt:mailinglists}           & Internal and public mailing lists
& WP1 &\coordshort{} & OTHER & CO &  1 \\
\hline \ref{mgt:swrepository} & Internal software repository & WP1 & \coordshort{} & OTHER & CO & 1 \\
\hline \ref{del:pressrelease1} & Press Release Announcing Start of \TheProject{} & \ref{wp:dissem} & \SAshort{} & DEC & PU & 3 \\
\hline \ref{del:website1} & Initial Project Website / Presentation & \ref{wp:dissem} & \SAshort{} & DEC & PU & 3 \\
\hline \ref{del:req1} & Report on Initial Requirements Analysis and Metrics & WP7 & \GOLEMshort{} & R & PU & 3 \\
\hline \ref{del:data-mgt-plan} & Data Management Plan & \ref{wp:dissem} & \SAshort{} & R & PU & 6 \\
\hline \ref{del:cppComponentsPatternsInitial} & Report on Initial Components and Patterns & WP3 & \SAshort{} & R & PU & 9 \\
\hline \ref{del:model1} & Report on Initial Modelling, Patterns and Code Synthesis & WP2 & \INRIAshort{} & R & PU & 10 \\
\hline \ref{del:SDIInitial} & Software on Initial Design and Generation of SDIs & WP3 & \AGHshort{} & OTHER & PU & 10\\
\hline \ref{del:runtime1} & Software for Initial Runtime Infrastructure  & WP4 & \CODEPLAYshort{} & OTHER & PU & 10 \\
\hline \ref{del:staticMOO} & Software for the Static Optimisation Infrastructure for Heterogeneous Hardware Architectures & WP5 & \SCCHshort{} & OTHER & PU & 10 \\
\hline \ref{del:securitySafety1} & Initial Report on Security and Safety Analysis & WP6 & \IBMshort{} & R & PU & 10 \\
\hline \ref{del:eval1} & Report on Initial Implementation of Benchmarks and Use Cases and Evaluation & WP7 & \GOLEMshort{} & R & PU & 11 \\
\hline \ref{mgt:periodic-rep-1} & Project Periodic Report (first year) & WP1 & \coordshort{} & R & CO & 12 \\
\hline \ref{del:dissemplan1} & First Interim Report on Dissemination and Exploitation & \ref{wp:dissem} & \SAshort{} & R & PU & 12 \\
\hline \ref{del:req2} & Report on Updated Requirements Analysis and Metrics & WP7 & \GOLEMshort{} & R & PU & 12 \\
\hline \ref{del:codingstan1} & Initial Report on the Mapping and Categorisation of Diagnostics  & WP6 & \PRLshort{} & R & PU & 14 \\
\hline \ref{del:initialCompliance} & Report on Initial Model-Level Compliance and Diagnostics & WP2 & \PRLshort{} & R & PU & 18 \\
\hline \ref{del:dynamicMOO} & Software for the Dynamic Optimisation for Heterogeneous Hardware Architectures & WP5 & \SCCHshort{} & OTHER & PU & 20 \\
\hline \ref{del:cppComponentsPatternsAdvanced} & Report on Refined Components and Patterns & WP3 & \SAshort{} & R & CO & 21 \\
\hline \ref{del:model2} & Report on Intermediate Modelling, Patterns and Synthesis & WP2 & \INRIAshort{} & R & PU & 22 \\
\hline \ref{del:SDIAdvanced} & Software on Refined Software-Defined Infrastructures & WP3 & \SAshort{} & OTHER & CO & 22\\
\hline \ref{del:runtime2} & Software for Intermediate Runtime Infrastructure for & WP4 & \CODEPLAYshort{} & OTHER & PU & 22 \\
\hline \ref{del:securitySafety2} & Interim Report on Security and Safety Analysis & WP6 & \IBMshort{} & R & PU & 22 \\
\hline \item \ref{del:eval2} & Report on Intermediate Implementation of Benchmarks and Use Cases and Evaluation & WP7 & \GOLEMshort{} & R & PU & 22 \\
\hline \ref{del:req3} & Report on Final Requirement Analysis and Metrics & WP7 & \GOLEMshort{} & R & PU & 23 \\
\hline \ref{mgt:periodic-rep-2} & Project Periodic Report (second year) & WP1 & \coordshort{} & R & CO & 24 \\
\hline \ref{del:dissemplan2} & Second Interim Report on Dissemination and Exploitation & \ref{wp:dissem} & \SAshort{} & R & PU & 24 \\
\hline \ref{del:codingstan2} & Final Report on the Mapping and Categorisation of Diagnostics  & WP6 & \PRLshort{} & R & PU & 30 \\
\hline \ref{del:cppComponentsPatternsFinal} & Report on Final Components and Patterns & WP3 & \SAshort{} & R & PU & 33 \\
\hline \ref{del:model3} & Software on Final Modelling, Pattern and Code Synthesis & WP2 & \INRIAshort{} & OTHER & PU & 34 \\
\hline \ref{del:SDIFinal} & Software on Final Software-Defined Infrastructures & WP3 & \SAshort{} & OTHER & PU & 34 \\
\hline \ref{del:runtime3} & Software for Final \TheProject{} Runtime Infrastructure	 & WP4 & \CODEPLAYshort{} & OTHER & PU & 34 \\
\hline \ref{del:runtime4} & Report on the \TheProject{} Runtime Infrastructure & WP4 & \CODEPLAYshort{} & R & PU & 34 \\
 \hline  \ref{del:distMOO} & Software for Decentralised Optimisation Infrastructure & WP5 & \SCCHshort{} & OTHER & PU & 34 \\
\hline \ref{del:finalMOO} & Combined report on the final optimisation infrastructure for heterogeneous hardware architectures & WP5 & \SCCHshort{} & OTHER & PU & 34 \\
\hline \ref{del:securitySafety3} & Final Report on Security and Safety Analysis & WP6 & \IBMshort{} & R & PU & 34 \\
\hline \ref{del:finalCompliance} & Report on Final Model-Level Compliance and Diagnostics & WP2 & \PRLshort{} & R & PU & 35 \\
\hline \ref{mgt:periodic-rep-3} & Project Periodic Report (third year) & WP1 & \coordshort{} & R & CO & 36 \\
\hline \ref{del:eval3} & Report on Final Implementation of Benchmarks and Use Cases and Evaluation & WP7 & \GOLEMshort{} & R & PU & 36 \\
\hline \ref{del:roadmapping} & Report on Technical Roadmap & WP7 & \SAshort{} & R & PU & 36 \\	
\hline \ref{del:pressrelease2} & Final Press Release Describing the \TheProject{} Results & \ref{wp:dissem} & \SAshort{} & DEC & PU & 36 \\
\hline \ref{del:website2} & Final Project Website / Presentation & \ref{wp:dissem} & \SAshort{} & DEC & PU & 36 \\
\hline \ref{del:dissemplan3} & Final Report on Dissemination and Exploitation & \ref{wp:dissem} & \SAshort{} & R & PU &  36 \\

\hline
\end{tabular}
\end{center}
\end{minipage}

\input{WPs/WPs}


% \TODO{Milestones need to be discussed and then described here.}

\bigskip\bigskip\bigskip
%\draftpage
\pagebreak
\fbox{\begin{minipage}{\textwidth}

\begin{center}\Large\bf
Critical Risks for Implementation
\label{sect:risks}
\end{center}
\end{minipage}}

\bigskip
Steps have already been taken to reduce the level of risk within the overall implementation plan.  The table below
identifies the main residual risks that are foreseen.  This register will be maintained
and updated as necessary during the project in order to minimise risk and so to maximise its successful completion.

\bigskip

%\begin{tabular}{| p{3.2cm} | p{1.8cm} | p{1.5cm} | p{10.3cm}  |}  \hline
%\begin{longtable}{| p{3.2cm} | p{1.8cm} | p{1.5cm} | p{10.3cm}  |}  \hline
\begin{longtable}{| p{3.5cm} | p{1.5cm} | p{11.8cm}  |}  \hline
\textbf{Description of risk} & \textbf{WPs\newline involved} & \textbf{Proposed Risk-mitigation measures} \\ \hline
\multicolumn{3}{l}{\ }
\\\hline
Failure to fully deliver initial project requirements 
by month 2 (Milestone \ref{mil:req1}). 
\par\vspace{1ex}
\textbf{Severity: Medium}
\par
\textbf{Likelihood: Medium}&
WP2--\ref{wp:eval}  &  
This is an early milestone that defines the overall project direction.  
The core requirements and features for \TheProject{} will be analysed and explored in the first iteration with focus on the concrete use cases. It will be further refined with other requirement and feature in the spiral process of agile iterations. 

\\ \hline
Building blocks and patterns are unable to capture essential non-functional
requirements, including security, safety and performance (Assumption A2, Milestones MS3, MS10, MS15).
\par\vspace{1ex}
\textbf{Severity: High}
\par
\textbf{Likelihood: Low} &
WP2, WP5, WP66  &  
\TheProject{} builds on successful previous work in
the \paraphrase{}, \rephrase{} and \paraformance{}
projects and elsewhere. We have demonstrated that pattern-based approaches can successfully capture
a variety of common parallelism structures, that they can be used to enhance safety
checking such as race condition detection, and  that they can incorporate certain types
of performance information, such as timing information. This work will be extended and
enhanced as part of \TheProject{}. In the event of unforeseen problems, we will first
attempt to define alternative patterns that may better suit the requirements,
and subsequently relax the restrictions.

\\ \hline
It is not possible to establish link between the block-diagram model and the generated intermediate C/C++/SYCL code (Assumptions A3 and A4, Milestones MS6, MS7, MS13, MS16).
\par\vspace{1ex}
\textbf{Severity: High}
\par
\textbf{Likelihood: Low} &
WP2, WP3, WP5, WP6  &
The \TheProject{} team has long-standing experience in developing high-level languages, including Erlang, and with their compilation to C/C++ and machine code.
In the \rephrase{} project, we have previously demonstrated that it is possible to establish a relationship between
a very high level language for describing patterns (RPL) and the C/C++ that is generated from it. We are therefore confident that
we will be able to establish a similar link between block-diagram models and the generated C/C++/SYCL code using the
mechanisms from WP2. In the event that this does not prove to be possible, we will insert explicit linkage annotations in the model
itself that will be carried over to the compiled code.
% , allowing us to explicitly link pieces of C/C++/SYCL code with the
% parts of the model the code was generated from. This would require more effort from the programmer in using our
% methodology, but would have no other impact on technical work in the project.

\\ \hline
Inability to capture key system properties into the high-level hardware description language (Assumption A5, Milestones MS3, MS10, MS15).
\par\vspace{1ex}
\textbf{Severity: Low}
\par
\textbf{Risk: Medium}
& WP3 &
The \TheProject{} team has expertise in developing hardware-abstraction and virtualisation
mechanisms as part of the \paraphrase{} EU project, including visualisation techniques for
representing software and hardware components. If designing the high-level visual hardware
description language proves to be infeasible, we will instead use existing lower-level mechanisms from the distributed computing
community, such as XML-based HDL that is used in computational
grids, from which the Erlang code will be generated.



\\ \hline
We cannot generate adaptive software-defined infrastructures using Erlang (Assumption A6, Milestones MS3, MS10, MS15)
\par\vspace{1ex}
\textbf{Severity: High}
\par
\textbf{Likelihood: Low} &
 WP3, WP5, WP6  &
 Our experience from the \paraphrase{} project showed that Erlang can be successfully
 used to orchestrate computations written in C/C++. One of the main the Erlang design goals  was precisely
 portability to different distributed architectures. Therefore, we are confident that the software-defined
 infrastructures that are generated using Erlang will prove to be adaptive to different target platforms. If this does
 not prove to be the case, we will consider providing hooks within the SDIs to support manual tuning to some target
 systems or fall back to some lower level model, e.g. C+MPI, at the cost of increased effort by the users of the \TheProject{} approach. 
% This would make our methodology less automated
% and, therefore, require mnore coding effort by the programmer, but would .
 
\\\hline
Unstable drivers: Existing OpenCL drivers often exhibit correctness problems with large (sets of) kernels and complex scheduling scenarios (Milestones MS3, MS10, MS15). 
\par\vspace{1ex}
\textbf{Severity: Low}
\par
\textbf{Likelihood: Medium}&
WP4 & There are several measures that can be taken to mitigate this risk, such as restricting our work to drivers that work, upgrading the existing drivers, restricting our focus onto smaller set of heterogeneous architectures with correct drivers and investing additional effort to provide workarounds in compiler or runtime system.

%\\\hline
%Information generated from model not sufficient to control compile/schedule/deploy components efficiently 
%\par\vspace{1ex}
%\textbf{Severity: Low}
%%\par
%\textbf{Likelihood: Low} & WP4 &
%Hand-tuning of the code generated by the model to achieve performance.  

\\\hline
Technical complications in implementing optimisations, including inability to target specific libraries and/or hardware platforms (Milestones MS3, MS8, MS15).
\par\vspace{1ex}
\textbf{(Severity High}
\par
\textbf{Likelihood Medium)}
&
WP4, WP5 &  
Our initial assessments indicate that it should be feasible to implement the targeted optimisation schemes on the proposed target platforms. \TheProject{} builds upon and extends successful previous work on the design of optimisations for mapping of parallel applications by \SCCHshort, \INRIAshort and \SAshort. 
In the event of unexpected technical complications in implementing optimisations, that may not be handled within the course of this project, we will target alternative libraries/platforms (while meeting all optimisation requirements). In this way, the overall objectives of the project will still be achieved, but at some potential loss in generality of the results.

\\\hline

It is not possible to produce optimisations that satisfy all stated performance requirements (Milestones MS3, MS8, MS15).
\par\vspace{1ex}
\textbf{Severity: Medium}
\par
\textbf{Likelihood: Medium}
&
WP5 &  
\TheProject{} builds on and extends successful previous work on multi-objective optimisation and machine
learning at \SCCHshort and \SAshort. In the event of unforeseen problems, we will first
consider whether the stated restrictions are reasonable, and relax them. If not,
we will investigate alternative technical approaches that will allow us to deal properly
with the requirements, and implement these instead; finally, we will document any
residual issues, and propose possible technical solutions as part of our roadmap for
adoption.

\\\hline
Vulnerability detection tools will not detect ones that are unique to a specific target platform (Milestones MS3, MS10, MS15). 
\par\vspace{1ex}
\textbf{Severity: Medium}
\par
\textbf{Likelihood: Medium}&
WP6  &  
Some vulnerabilities exist only in a specific platform. We will err on the safe side and issue a warning to all platforms when we find code that may be vulnerable at least in one platform,
since as platforms evolve, the code may subsequently become vulnerable elsewhere. We will build our solution in such a way that it can be extended to detect more patterns
of  vulnerability when they are detected in at least one of the target platforms.

\\\hline
Virtual  on-line security protection patches will add a high performance penalty (Milestones MS3, MS10, MS15). 
\par\vspace{1ex}
\textbf{Severity: Medium}
\par
\textbf{Likelihood: Medium}&
WP6  &  
In many cases, there is a trade-off between adding security mechanism and performance. We will tune our risk assessment algorithms to invoke the security on-line protection only when the risk is high. We may give the user control to decide when performance is more important to them than security and act accordingly.

\\\hline
The diversity of standards may slow down the use case implementation (Milestones MS4, MS11, MS17).  
\par\vspace{1ex}
\textbf{Severity: Medium}
\par\textbf{Likelihood: Low}
& WP7 & The consortium members and coordinator have  strong background results, knowledge and experience of previous H2020 projects (\textbf{Advance}, \paraphrase{}, \rephrase{}, \teamplay{}, etc). The key standards for implementing the use cases will be selected based on these expertise. The issue will be mitigated by Technical Steering Committee and Advisory Board.

\\\hline
The use case implementation may interfere with partners' business and production processes (Milestones MS4, MS11, MS17).
\par\vspace{1ex}
\textbf{Severity: Low}
\par\textbf{Likelihood: Low}
& WP7 &The risk is minimised by the consortium agreement formulating the terms of use case implementation and during WP7 the members will reiterate the use case scenarios and formulate implementation procedures in necessary detail to avoid interference with ongoing business processes especially in municipality. In the unlikely case of delays a contingency plan can be compiled for to address the issues promptly by the WTL, Coordinator or Steering Committee.

\\\hline

Non-performance of a partner.
\par\vspace{1ex}
\textbf{Severity: High}
\par
\textbf{Likelihood: Low)} &
ALL & 
Where skills overlap, 
effort will be redeployed to other partners;
otherwise the tasks may be scaled back, if possible; or,
if necessary, new partners with required competencies will be
incorporated into the project.
\\\hline

Failure to achieve key project objectives.
\par\vspace{1ex}
\textbf{Severity: High}
\par
\textbf{Likelihood: Low} &
ALL & 
In order to make the major progress that is required,
\TheProject{} incorporates leading-edge
research and complex software development, whose absolute success cannot be predicted. 
All partners will, however, strive to achieve project objectives
to the best of their ability.  Based on our results to date, and taking into account
the excellent technical and research track records of each of
the partners, it appears entirely feasible to achieve not only our general
technical research objectives, but also all the specific objectives
of the work packages.  In the event that it
proves impossible to achieve some specific objective within the
scope of the project, we will firstly attempt to reallocate
resources to ensure that all objectives are obtained, then
to prioritise objectives so that the most critical are achieved,
and finally, if absolutely necessary, we will scale down our technical objectives, by
relaxing or deleting some part of those objectives as required
to achieve success. 
\\\hline
\end{longtable}
%\newpage

%\vspace{-6pt}
\subsection{Management Structure and Procedures (Figure~\ref{fig:management})}
\label{sect:mgt}

\eucommentary{Describe the organisational structure and the decision-making ( including a list of milestones (table 3.2a)).\\
Explain why the organisational structure and decision-making mechanisms are appropriate to the complexity and scale of the project.\\
Describe, where relevant, how effective innovation management will be addressed in the management structure and work plan.\\
Describe any critical risks, relating to project implementation, that the stated project's objectives may not be achieved. Detail any risk mitigation measures. Please provide a table with critical risks identified and mitigating actions (table 3.2b).}

Responsibility for the overall management and technical
direction of the project will rest with the \emph{Project Coordinator}
(Prof Kevin Hammond, \SA{}) who will be the primary point of
contact with the European Commission. Responsibility for
individual work packages will rest with the \emph{Work Package Team
Leader (WTL)} identified below, who will report to the Project Coordinator.
Where a work package is split across more than one
institution, the day-to-day management of each task will be handled
locally, with the task manager reporting to the WTL.   
% Disputes
% will be resolved at the lowest possible level by an independent
% adjudicator (for disputes between WTLs this will be the Project
% Coordinator, unless he is involved in the dispute).
%
In order to ensure good integration of the project and sound
overall management, the Project Coordinator will convene
annual technical workshops containing representatives from
the entire project team.   These workshops will be open to
invited external researchers/industrialists, including members
of the \emph{Project Advisory Board}, and will usually be
accompanied by a physical meeting of the \emph{Project Steering Committee}. In
addition, the Project Coordinator will convene management
meetings involving the relevant partners and members of the
Project Advisory Board, as necessary and appropriate. These meetings will be
conducted either using video-conferencing, through a
teleconference, or in person, as appropriate and with due consideration to
cost, urgency and effectiveness.  Technical teams
working on a work package that is spread across sites will
coordinate through email, video-conferencing, telephone and
scheduled meetings.  Finally, the research teams will maintain
regular contact with the Project Coordinator and each other
through regular email reports and telephone conversations.
Progress will be carefully monitored with progress reports and
monitoring documents open to inspection by the EU project
monitoring officer. In the event of a serious and urgent matter
involving all partners, the Project Coordinator may also
convene an Extraordinary meeting of the Steering Committee.
%
All project documentation (whether managerial, legal or
technical) will be maintained through a centralised electronic
repository, accessible to all consortium members on an open
basis, and incorporating audit trails concisely recording
reasons for changes etc.  We propose to use either GIT or SVN,
which provide suitable low-cost,  low-overhead solutions that all partners are
familiar with.  Our technical reports will form the basis for
the public deliverables that appear on the project web site.

\begin{figure}[t!]
%\begin{wrapfigure}{l}{0.7\textwidth}
%\vspace{-0.75in}
%\hspace{-1in}
\begin{center}
\centerline{\hspace{1in} 
\hbox to \columnwidth{\hss\includeimage[height=5.7in,trim={0 0 0 2.5cm},clip]{Management}\hss}
}
\end{center}
\vspace{-1.8in}
\caption{Management Structure}
\label{fig:management}
\end{figure}
%\end{wrapfigure}

%\TODO{Update this to reflect the consortium.}

\subsubsection*{Technical Steering Committee}
\vspace{-6pt}

The \emph{Technical Steering Committee} comprises the WTLs, plus the
Project Coordinator (who will act as chair).  Its purpose is to ensure
the effective running of the project on a day-to-day basis, and to
coordinate work across work packages.  In particular the Technical Steering
Committee will be
responsible for the implementation of the directives of the Project Steering
Committee, for the
guidance and monitoring of the technical WPs, for coordination among
WPs, for  the timely preparation, approval, and forwarding to the Commission
of the deliverables produced by the WPs, and for the resolution of conflicts
amongst WPs.  It will meet on a regular basis, usually through a monthly
teleconference.  Meetings may also be convened on request by any member.
% but also in person where necessary.  
Each member of the Technical Steering Committee has one vote,
which may be made by proxy, or in absentia, if necessary.  
Decisions are taken by consensus, if possible, otherwise by majority vote, 
with the
Project Coordinator retaining the casting vote.

\subsubsection*{Project Steering Committee}
\vspace{-6pt}

The \emph{Project Steering Committee} comprises one representative from each partner
(usually the PI), and is chaired by the Project Coordinator.  
The purpose of this committee is to decide
the general technical direction of the project.  It will also
take major decisions on project finances, addition of partners, removal of non-performing
partners, IPR issues, reallocation of workload etc.  It will meet in person
at least once per year, supplemented by more regular teleconference
meetings as required. Extraordinary meetings may also be convened on request by any partner.
Each representative has one vote, which
may be made by proxy if necessary.  Decisions are taken by
consensus, if possible, otherwise by majority vote, with the
Project Coordinator retaining the casting vote.

\subsubsection*{Project Advisory Board}
\vspace{-6pt}

The Project Advisory Board comprises a small group of invited
academics and industrialists who will provide input to the
project on general technical trends and directions, and advise
the steering committee where required.  The initial composition
of the Advisory Board will be determined at the outset of the
project, but we expect to include academic experts from the data-intensive, high-performance and cloud computing, as well as machine learning, compilation, software-defined infrastructures and optimisation domains. We also expect to include senior representatives from the automotive, AI and IoT industry domains. The Coordinator is authorized to 
execute with each member of the EEAB a non-disclosure agreement, which 
terms shall be not less stringent than those stipulated in this 
Consortium Agreement, no later than 30 calendar days after their nomination 
or before any confidential information will be exchanged, whichever date is earlier. 
We have invited senior representatives from Aarhus University, SAP Institute for Digital Government, Ericsson, TypeSafe, British Telecom,
the oil\&gas industries, the Cloud Competency Centre (Dublin),
	and Scottish Enterprise.

\pagebreak
\subsubsection*{Work Package Team Leaders}
\vspace{-6pt}

Work package team leaders (WTLs) are responsible for tracking progress within their work package,
developing metrics for each deliverable at the outset of each
task, ensuring that results are properly reviewed against these
metrics, and consequently providing feedback to the Project Coordinator on the achievement of goals. 
%
WTLs have been chosen on the basis of managerial experience, technical expertise and
commitment to the work package programmes. % , as shown below.

%\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
\begin{tabular}{|l|l|}\hline
\textbf{WP} & \textbf{WTL}  \\ \hline
WP1 &  Kevin Hammond (\coordshort{}) \\
\hline
WP2 &  Albert Cohen (\INRIAshort{}) \\
\hline
WP3 &  Vladimir Janjic (\SAshort{}) \\
\hline
WP4 &  Andrew Richards (\CODEPLAYshort{}) \\
\hline
\end{tabular}
\quad\quad&\quad\quad
\begin{tabular}{|l|l|}\hline
\textbf{WP} & \textbf{WTL}  \\ \hline
WP5 &  Georgios Chasparis (\SCCHshort{}) \\
\hline
WP6 &  Sharon Keidar-Barner (\IBMshort{}) \\
\hline
\ref{wp:eval} &  Serguei Golovanov (\GOLEMshort{}) \\ % Michael Ro\ss{}bory (\evalshort{}) \\
\hline
\ref{wp:dissem} & Chris Brown (\dissemshort) \\
\hline
\end{tabular}
\end{tabular}
\end{center}
%\caption{Work package team leaders (WTLs).}
%\label{fig:wtls}
%\end{figure}


%\pagebreak
\subsubsection*{Principal Investigators}
\vspace{-6pt}

One principal investigator (PI) will be nominated by each partner.
The PI is responsible for properly managing the budget allocated to the partner and for performing
all the tasks that are carried out by that partner, reporting to the appropriate WTLs where necessary.  PIs
also act as line managers for the researchers/developers employed on the project by the partner.
PIs will usually also act as WTLs for the main WPs that are carried out at that site, and may be allocated their own
technical tasks. They will normally be the partner's representative on the steering committee.
They have been chosen for their technical expertise and experience of line management and budget handling.

\newcounter{partic}

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{|l|l|l|}\hline
& \textbf{Partner} & \textbf{PI} \\ \hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Kevin Hammond \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Sharon Keidar-Barner \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Evgueni Kolossov \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Albert Cohen \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Georgios Chasparis \\\hline
\end{tabular}
\quad\quad&\quad\quad
\begin{tabular}{|l|l|l|}\hline
& \textbf{Partner} & \textbf{PI} \\ \hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Serguei Golovanov \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Andrew Richards \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Alexsander Byrski \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Gints Reinsons \\\hline
\end{tabular}
\end{tabular}
\end{center}

\vspace{12pt}
\subsubsection*{Project Coordinator}
\vspace{-6pt}

The Project Coordinator is \emph{Prof Kevin Hammond}.  His role
is to act as the primary point of contact with the European
Commission, to receive feedback on research results from each
work package, to ensure the project maintains effective
progress towards the project objectives based on these results,
to produce any required  project management reports, to ensure
that deliverables are produced according to the planned
schedule and delivered to the Commission and project reviewers
as required, and to resolve disputes between project partners
as and when these arise.  He will convene regular management
and technical meetings, monitor progress on each work package,
collate deliverables, and maintain good contact with each site,
in addition to producing the annual management reports, and
ensuring that each site produces the required financial (audit)
certificates.  He will also be responsible for ensuring that
the Consortium Agreement (including IPR issues, voting rules and the conflict resolution procedures)
and any other legal documents are properly prepared and managed. This will be
done through \SAshort{} \emph{Research and Enterprise
Services}, who have significant expertise in preparing such
agreements.

\vspace{12pt}
\subsubsection*{Project Administrator}
\vspace{-6pt}

The Project Coordinator will be supported in his management
duties by a part-time \emph{Project Administrator} (to be appointed
from staff already employed by \SA{}) and located at \SA{}.  The Administrator
must possess both strong organisational skills and a
sufficient level of technical expertise in order to communicate
management requirements to the Partners, but will not be
involved in the management of day-to-day RTD activities.

\vspace{12pt}
\subsubsection*{Consortium agreement}
\vspace{-6pt}

% The relationship between all partners will be fixed in a Consortium Agreement based on the following principles:

% In order to have a management system applicable through all phases of the
% project, a reasonable approach is to have straight, clear and direct
% management and organization protocol at all levels. This is particularly
% relevant given the challenging financial and industrial policy
% constraints. Therefore, in order to have clearly assigned
% responsibilities, to avoid any friction and to progress as per the
% project plan, the responsibilities and authorities of the project manager
% and the team members will be unambiguous.

The partners will be bound by a formal consortium agreement that is
planned to be signed prior to the beginning of the project of the project,
and in which their roles, responsibilities and mutual obligations will be
defined both for the project life and, where relevant, beyond.  This will
formalise key issues including conflict resolution, IPR procedures, governance structure
etc.  %It will be based on the model consortium agreement issued by the European Commission.
The Digital Europe version of the DESCA, including the European Commission's
inputs will constitute the basis for such consortium agreement.

%\pagebreak
% \vspace{-6pt}
\subsubsection*{Conflict Resolution}
\label{conflict-resolution}
If conflicts arise during the execution of the project, they will be resolved according to the following principles:
% 
% \begin{itemize}
% \item
They will first be addressed within the relevant WP through discussion chaired by the WTL;
% \item
If this fails, the issue will be presented by the WTL either to the Technical Steering Committee
or to the Project Steering Committee, depending on the nature of the problem (technical or business/strategic).
% \item
The relevant board will attempt to resolve the issue through the usual voting procedure.
% \end{itemize}
%
Technical issues between WPs will also be addressed by the Technical Steering Committee.
%  As noted above, the TSC of the project consists of the WP leaders (chaired by the technical coordinator), and the GA consists of representatives of each partner (chaired by the management coordinator).
Any conflicts that cannot be resolved through the principles above will
be handled according to the dispute resolution provision set forth in the
Consortium Agreement.

% \subsubsection*{Management Costs}

% We have budgeted for the Project Administrator at 25\% effort
% over the lifetime of the project (i.e. 9 person-months) at
% \SAshort{}, plus small-scale management effort as required by each support to support the
% project through the preparation of reports for review meetings and other project-level
% management tasks.
% We have also budgeted for the cost of running annual Advisory Committee meetings, including travel
% support for unfunded Advisory Committee members, at \euros{} XX p.a. for a total cost of \euros{} XX.
% We have budgeted an additional
% \euros{}~4,800 for travel by the Project Coordinator (estimated
% as an additional two trips per annum).
% In order to minimise costs and time expenditure, as far as
% possible, all project management activities will be carried out using
% low-cost means such as email, Skype, telephone or
% video-conferencing, and any managerial travel will normally be
% combined with technical or research meetings.  Each site with expenditure
% in excess of \euros{}~325,000 also requires specific costs
% to cover the preparation of the required financial certificates, which will
% generally involve subcontracting an external financial auditor.

\draftpage
\subsection{Consortium as a Whole}
\eucommentary{\begin{itemize}
\item
Describe the consortium. How will it match the project's objectives? How do the members complement one another (and cover the value chain, where appropriate)? In what way does each of them contribute to the project? How will they be able to work effectively together?
\item
If applicable, describe the industrial/commercial involvement in the project to ensure exploitation of the results and explain why this is consistent with and will help to achieve the specific measures which are proposed for exploitation of the results of the project (see section 2.3).
\item
Other countries: If one or more of the participants requesting EU funding is based in a country that is not automatically eligible for such funding (entities from Member States of the EU, from Associated Countries and from one of the countries in the exhaustive list included in General Annex A of the work programme are automatically eligible for EU funding), explain why the participation of the entity in question is essential to carrying out the project
\end{itemize}
}

\begin{figure}[t]
\begin{center}
\includeimage[scale=0.6,angle=0]{BlessConsortium.pdf}
\end{center}
\vspace{-0.3in}
\caption{Areas of Partner Expertise}
\label{fig:consortium}
\end{figure}

\textbf{Figure~\ref{fig:consortium}} shows the areas of expertise that
are relevant to this project and the consortium partners that
possess that expertise.  All partners span multiple areas,
providing technical depth within the consortium, and avoiding
knowledge gaps.  Within the areas, each partner possesses
complementary expertise, but with enough knowledge overlap to
ensure tight cohesion of the consortium. The consortium
comprises both academic and industrial expertise.
%  from XX
% highly-respected partners.
The consortium links the world-leading technical expertise of
the participating groups on parallel programming,
refactoring,
compilation,
multi-objective optimisation,
resource modelling,
and security with exciting applications in telecommunications, 3D modelling, and the automotive sector.
Moreover, the academic groups at \SAshort{}, \INRIAshort{},  all have
excellent connections with industrial, scientific, commercial
and academic software developers.  These connections will be exploited
to disseminate the results of the project as widely as possible,
and to ensure the best possible uptake of the project results.

\paragraph*{Integration of the Consortium.}
%\vspace{-6pt}

Several of the partners already have close working relationships through
recent and ongoing research projects (e.g. \paraphrase, \rephrase and \teamplay).
The teams share common technical interests and several are active members in e.g. the
HiPEAC network of excellence (\SAshort{}; \INRIAshort{}; \PRLshort{}; \SCCHshort{}).
Work Packages have been designed to foster close collaboration
between teams at different organisations, with multiple groups involved in all of
the technical and evaluation work packages. The tasks in 
each work package have been allocated on the basis of
technical expertise and ability. All tasks have been designed to involve multi-site
collaboration and/or the exchange of information, which is
intended to promote healthy interaction between the partners.
Finally, in order to ensure good integration between the partners,
we propose to run at least one technical workshop each year, and
have also requested funds to allow researchers from each group
to visit other groups on a regular basis.  We anticipate that
all of the \TheProject{} researchers will participate in these
technical workshops and exchanges.  We also intend to publish a significant
number of research papers and technical reports deriving from our joint research,
and to collaborate on joint tool production. 
We thus foresee a necessary and close level of integration between
the \TheProject{} partners.

\paragraph{Industrial/Commercial Involvement.}

\TheProject{} directly involves one large company (\IBMshort{}), 
%% sloppy -- KH
four SMEs (\PRLshort{}, \CODEPLAYshort{}, \GOLEMshort{},  \SCCHshort{}) and one city municipality (\JMOICshort{}).
These organisations have included draft exploitation plans that will directly use the results of the project
as part of their ongoing business strategies and commercial development.
% follows an industrially-inspired agenda and addresses a key and topical challenge in
The \TheProject{} project further engages directly with industry through its
dissemination, user community and outreach activities 
% a dedicated  workpackage (WP7) 
% that are aimed at promoting
will promote the
\TheProject{} tools, technologies and above all \emph{mindset} and \emph{methodology} to a wider user base,
especially through industry-focused events. % that should attract C/C++ programmers and Simulink/SCADE users.
The objective is to ensure widespread uptake of the project results in a broad base of potential industrial
software developers targeting a variety of commercially important domains. % including telecommunications, 3D modelling and the automotive sector.
This will be assisted by including major industry participants on its Advisory Committee,
by actively engaging with the C, C++, Erlang and Simulink communities, and by engaging with the ISO C++ Standard Committee and ITU FG-DPM.
% and with the upcoming C++17 design.
% and by actively engaging with new coding standards for parallel and data-intensive applications, as well as C++.
% Finally, members of the consortium are also active members of the ISO C++ Standards Committee. This will
% ensure that the techniques, approach and methodology developed in the \TheProject{} will
% have a broad exposure through the C++ community, and through the evolution of the C++ language
% design itself.

% \khcomment{More needed here.}

\paragraph{Project Management Expertise.}

Members of the team have been heavily involved in running
various national and international research projects.  The
Project Coordinator, Prof. Kevin Hammond, has obtained numerous
research grants and awards from national and international
bodies, mainly the UK's Engineering and Physical Sciences
Research Council (EPSRC). He coordinated the FP7 \paraphrase{} project (IST-288570),
coordinates the Horizon 2020 RePhrase project (IST-644235),
is  Workpackage Team Leader and Principal Investigator for the Horizon 2020 \teamplay{} project, % (IST-248828),
was Workpackage Team Leader and Principal Investigator for the FP7 \advanceproject{} project (IST-248828),
coordinated the Joint
Research Activity of \science{} (RII3-CT-026133) -- a six-year
infrastructure project aimed at developing symbolic computing
infrastructure in Europe, coordinated the EU FP6
\embounded{} project (IST-510255),
and is investigator/co-investigator on various EPSRC, UK government and industry-funded projects.
Final project evaluations have consistently rated these projects as
delivering excellent or outstanding research outcomes and
representing excellent value for money.
%
Serguei Golovanov has participated in numerous RDI and industrial
projects from international bodies, mainly at IIASA and UNIDO (various
industrial modernisation projects and workshops in 30 countries), lead
the PharosN platform implementation as its chief architect, and provided
major contributions into the H2020 {Smart Urbana} project (CPS Labs, ICT-644400). He previously supervised the \GOLEMshort{} collaboration with several
major technology providers (Siemens, Texas Instruments, etc).
%
As described in the partner descriptions below,
%% Should add activities for INRIA, CodePlay etc.
most of the other partners have been extensively involved in previous and
ongoing EU projects at both technical and managerial
levels, and have dedicated experienced senior staff on the \TheProject{} project.
%XX
%
This experience will be called on as necessary to resolve any
managerial problems that may arise during the course of the
project.


%\bigskip
%\bigskip
\subsection{Resources to be Committed}

\eucommentary{Please provide the following:
\begin{itemize}
\item
a table showing number of person/months required (table 3.4a)
\item
a table showing 'other direct costs' (table 3.4b) for participants where those costs exceed 15\% of the personnel costs (according to the budget table in section 3 of the administrative proposal forms)
\end{itemize}}

\input{resources}

%\pagebreak
\subsubsection{Management Level Description of Resources and Budget}
\vspace{-6pt}

\TODO{This needs to be updated in line with the rest of the
project.}

The project will employ 456.5 person-months of effort over three
years, comprising one or more full-time or part-time researchers at each site
plus one part-time project administrator at \SAshort{}, % and one part-time web designer at \INRIAshort,  Not in PMs?
20\% of the Project Coordinator and 10\% of the WTLs. The researchers will be supported by
the necessary dedicated computing equipment,
% the
% usual basic research equipment (workstations and/or laptop
% computers, network facilities, printers, dedicated file
% servers, etc.) funded from the project overheads, by
% special-purpose heterogeneous hardware necessary to carry out
% the research, 
by funding to enable the necessary travel to scientific and technical conferences, trade shows, 
project meetings and other project-related events, and by the funding that is needed to establish/enhance existing
industrial and academic contacts and to establish a user
community for the \TheProject{} tools and technologies.
%
The quoted budget includes all relevant national social and
other legitimate employment costs as permitted under the rules
governing EU Horizon 2020 ICT projects, including costs of
healthcare, social security and pensions provision, in line with national norms for each site.
%
Sufficient travel funding is also needed to support good
collaboration between the groups, including attendance at the
annual technical project meetings, plus individual visits between
sites. We have budgeted approximately \euros{}~8,000 per
site per year (varied in line with previous
costs at each site)  to cover, for example:
% \begin{itemize}
% \item 
attending two project workshops at \euros{}~600 each;
% \item
attending the annual Project Review Meeting at \euros{}~600;
% \item
one 1-week inter-site visit at \euros{}~750 each;
% \item
attending two conferences per year within the EU at \euros{}~1,000 each;
% \item
attending one conference per year outside the EU at \euros{}~1,500;
%\item
three conference fees per year at \euros{}~650 each.
% \end{itemize}
%
\noindent
In addition, \SAshort{} has budgeted \euros{}~1,500 per year to cover attendance at
IFIP Working Group meetings, visits to industrial concerns for dissemination purposes,
attendance at developer conferences, demonstrations etc. to promote the project,
and \euros~1,000 per year to support travel that is related to the management of the
project.
Wherever possible, travel for different purposes will be
combined into a single trip. We have also budgeted \euros~1,000
per year at \SAshort{} to support attendance by the Project Advisory Board members at
the annual Advisory Board Meetings, where this cannot be met from other sources.

\TODO{Add any specialist equipment.  We might add a serious
  multi-core/multi-GPU machine.}

%\pagebreak
\subsubsection{Additional Partner Costs}
\vspace{-6pt}

\paragraph{Testbed systems and development servers.}
%% \SAshort{} has budgeted \euros{} 21,962 for a dedicated parallel machine to
%% support specialist medium-large scale development and research on modelling, refactoring, implementation,
%% scalability,
%% analysis,
%% % adaptivity, 
%% scheduling and machine learning etc, and to act as a testbed for the project as a whole.
%% The machine configuration would comprise e.g.  a 64-core AMD CPU, 4x4TB
%% hard drives for data storage, 256GB of RAM plus 4 Nvidia GPUs.  
% This will
% allow experimentation with CPU/GPU heterogeneity as well as demonstrating
% elastic scalability.  
\SAshort{} has budgeted \euros{} 4,216 for a central
server to support the various software and document repositories that are needed by the project, to run the project website and to provide access
to the shared research data that will be generated by the project.
% , and to
% provide archival support for this data.
%
% and many core accelerator cards.
%
% In order to obtain maximum benefit and use from these machines across the
% project, they will be defined with different specifications but similar
% development systems, allowing different tests to be carried out by other
% project partners, but minimising unnecessary development issues.  The
% machines will be accessible by other project partners for development,
% testing and experimentation.
%
% \paragraph{Workshop Organisation Costs.}
% XX{} has budgeted \euros{4,000} to support the organisation of
% technical workshops.
\GOLEMshort{} has budgeted \euros{} 7,000 for four IoT processing and
four 5G communication modules (Compute Card Intel 4 cores/8GB/128GBSSD,
Qualcomm 5G) and one multicore development server for prototyping high
speed distributed execution environment and network topologies in
software-defined infrastructures and scalable data intensive applications
in WP7. The purchase of the equipment is planned for year 2 of the
\TheProject{}, taking into consideration the intensive progress that is
being made on the relevant hardware.  This equipment will be available for use by other project partners.
%
\PRshort{} has budgeted \euros{} 10,000 to purchase the Matlab/Simulink software that is necessary for the implementation and testing of model amendments and
compliance. The cost, based on a quote from MathWorks, is:
\begin{itemize}
\item 2 Matlab licenses \euros{4,000} 
\item 2 Simulink licenses \euros{6,000}
\end{itemize} 
%% Covered above.  KH
% \PRshort{} also budgeted approximately 2,500 euro for an additional powerfull laptop to be able to demonstrate and deliver prototype to the Use Case supplier in USA. \\
\paragraph{Open Access Publication Fees.}
Each academic partner has budgeted approximately \euros{5,000} to
support gold open access publication for key project publications
(representing about 10-20\% of the expected project output).  The
budget will be pooled if not used by a specific partner, and used to
support further gold open access publication by other partners or
other dissemination activities, as necessary to maximise the overall
success of the project.  There will be no charge for green open access
publication, which will be used for the remainder of the project
publications.

%\vspace{-12pt}
\label{bibliography}
\addcontentsline{toc}{section}{References}

\bibliographystyle{abbrv}
\bibliography{bibliography,bibliography_scch}
%\bibliography{bibliography_ustan}


%% Write macro to split Sections 1-3
\Split{1-3}

% ---------------------------------------------------------------------------
%  Section 4: Members of the Consortium
% ---------------------------------------------------------------------------

\newpage

\eucommentary{Page limits do not apply.}

\section{Members of the Consortium}

\eucommentary{Please provide, for each participant, the following (if available):\\
\begin{itemize}
\item
a description of the legal entity and its main tasks, with an explanation of how its profile matches the tasks in the proposal;
\item
a curriculum vitae or description of the profile of the persons, including their gender, who will be primarily responsible for carrying out the proposed research and/or innovation activities;
\item
a list of up to 5 relevant publications, and/or products, services (including widely-used datasets or software), or other achievements relevant to the call content;
\item
a list of up to 5 relevant previous projects or activities, connected to the subject of this proposal;
\item
a description of any significant infrastructure and/or any major items of technical equipment, relevant to the proposed work;
\item
[any other supporting documents specified in the work programme for this call.]
\end{itemize}}

\subsection{Participants}
\Participant{SA}{(\url{http://www.st-andrews.ac.uk})}

\begin{wrapfigure}{R}{2cm}
\vspace{-3.95cm}
\hfill \includeimage{logos/st-andrews-logo.jpg}
\vspace{-1cm}
\end{wrapfigure}

\label{sec:participantUSTAN}

%===============================================================================
The \SAlong{} is the third-oldest in the English-speaking world (founded 1413).
The School of Computer Science was likewise one of the earliest Computer Science departments in the world (founded 1972).
It has established an excellent reputation for its pioneering research in e.g.,
parallel computing, software engineering, programming language design,
software architectures, theoretical computer science and
distributed/mobile systems.  This research expertise has been
recognised through the award of numerous research grants and
awards from the UK and the European Commission.

\vspace{10pt}
\textbf{The \SAlong{} coordinates the \TheProject{} project and
leads work packages WP1 on Management, WP3 on Parallel Pattern Implementation and Manipulation and WP8 on Dissemination
and Exploitation.
It contributes to all the other work packages with expertise in software engineering for complex parallel systems,
parallel patterns, mapping to heterogeneous parallel systems
cloud and distributed computing, static and dynamic analysis for pattern discovery and thread safety, machine-learning,
adaptive runtime systems and programming language implementation.}
\vspace{10pt}

\paragraph{Prof Kevin Hammond} \url{http://www.kevinhammond.net}

Kevin Hammond will be the Principal Investigator at \SAshort{}. He leads
the programming languages group at \SAshort{}, and has worked
extensively in the field of advanced programming language
design and implementation, with a focus on cost and performance
issues.  His work concentrates on declarative language designs,
especially functional programming languages.
He served on the international design committee for the standard
non-strict functional language Haskell,
and worked on the dominant compiler, GHC. Since receiving his
PhD in 1989 on parallel language implementation, he has published over 100 books, book chapters,
journal papers and other refereed publications,
focusing on security and other software properties,
real-time systems, time- and space-cost analysis,  adaptive run-time environments,
domain-specific programming languages,  lightweight
concurrency, high-level programming language design,  parallel computing, and
performance monitoring/visualisation.  
He has run over 20
successful national and international research projects,
is a senior member of the ACM,
is a founder member of IFIP WG 2.11 (Generative Programming),
and a full member of the HiPeaC network of excellence (High Performance and Embedded Computers).
He was the UK representative on the TACLe Cost Action (Code-Level Timing Analysis).
His H-Index is 27 and Google Scholar reports over 5000 citations of his work.
%
Kevin Hammond's team has worked extensively on the design and implementation
of parallel systems, where he has developed new pattern-based parallelisation techniques,
novel adaptive scheduling technologies for multi-core, 
cluster, cloud and Grid systems
applied advanced refactoring techniques to parallel software,
developed new debugging technology for parallel programs
and developed rigorous models for determining execution costs of structured parallel programs.
In addition to his expertise in parallelism and distribution, Kevin Hammond also has
significant expertise in the area of cost estimation, having worked on
cost models and analyses for both
parallel
and real-time languages. 
This work has been supported by numerous national and international
research projects.
Kevin Hammond is workpackage team leader on the Horizon 2020 \teamplay project (ICT-779882),
coordinated the Horizon 2020 Project \rephrase
(ICT-644235), the FP7 \paraphrase project (ICT-288570), and
the FP6 EmBounded project (IST-510255), led the Joint Research Activity
for the FP6 SCIEnce project (RII3-CT-026133), was workpackage team
leader for the FP7 ADVANCE project (IST-248828), and holds several national
and industrially-funded grants that are directly relevant to this
proposal, including the EPSRC \discovery project.  He has strong connections with both academia and industry,
having collaborated with over 100 institutions and companies on
various research projects and technical research papers.

\paragraph{Dr John Thomson}   is a Lecturer in the School of Computer Science, who has been working in the field of compiler optimisation
and machine-learning for eight years. He earned his PhD from the University of Edinburgh
in 2009. % , before taking a position as a lecturer at the University of Innsbruck, and subsequently the University of St Andrews.
Dr Thomson pioneered the use of machine-learning for compiler optimisation, writing numerous papers,
including the foundational paper in the area. %~\cite{FKM+11}.
He has authored and co-authored successful research proposals for EPSRC (EP/H051988/1), Fonds zur F\"orderung der wissenschaftlichen Forschung (FWF), and EU funding agencies, and has served as work package leader on the EU FP6 \milepost project.

\paragraph{Dr Susmit Sarkar}  
is a Lecturer in the
School of Computer Science. He was previously awarded an EPSRC
Postdoctoral Fellowship (EP/H027351/1) on Multiprocessor Semantics,  held at the University of
Cambridge (2010-13). He earned his PhD from
Carnegie Mellon University in 2009 on formal semantics for low-level
software, and has since worked extensively on shared-memory
concurrency and relaxed memory consistency, with a strong
collaboration with industry (IBM, ARM) and standards bodies (ISO
C/C++). He has published $20$ papers in top-tier programming language
and verification conferences (POPLx3, PLDIx3, CAVx2, ICFP), and has
presented invited tutorials at POPL. He was a named SRA and co-author
on the EPSRC REMS Programme Grant (EP/K008528/1) while at Cambridge,
and currently holds an Industry-partnered Fellowship funded by the Scottish
Funding Council.

\paragraph{Dr Christopher Brown}  
has significant expertise in refactoring and program
transformation. He has undertaken pioneering work on automated
refactoring for parallelism in Haskell, Erlang and C++,
including developing new ideas of program shaping and pattern discovery.
 He received his PhD from the University of Kent in 2008,
 with a thesis on Tool Support for Refactoring Haskell Programs,
 and subsequently worked with Prof. Simon Thompson % as a primary researcher
 on the Refactoring Functional Programs project.
 He played a key role in implementing the Haskell Refactorer, HaRe %~\cite{HaRe},
 which he has released as an Open Source Project. % Dr Brown still maintains
%  and develops HaRe as a side-project to his main research.
%  Since
%  finishing his PhD, Dr Brown has published papers in the area of
%  refactoring, general program transformation and parallelisation. He now
%  works in the field of parallel functional programming.
 He worked on the \euros{3.2}M
 EU \science{} project, where he built domain-specific parallel
 skeletons for use in both Computer Algebra and Functional Programming.
 He was workpackage team leader on the \euros{4.2}M EU FP7 \paraphrase{}
 project, which investigated the application of pattern-based
 refactoring in Erlang and C++.
 He is currently workpackage team leader on the \euros{3.6}M EU Horizon 2020 \rephrase{} project,
 which is applying these refactoring techniques as part of a new software
 engineering approach to parallel programming using C++.
 He leads a technical team of six developers on the \pounds{537K} Scottish Enterprise
 \paraformance{} project, which is investigating commercial applications of refactoring
 for parallel programming, and which forms a key part of our longer-term exploitation plan.
 This approach is both effective and practical: using the \paraformance{} tooling, we have successfully analysed million-line C++ programs,
 narrowing over 1,000 potential sites of potential parallelism to 29 effective sites.
 This task would take weeks or months of human effort, but our tool achieves it in less than 60 seconds.

\paragraph{Dr Vladimir Janjic}
received his PhD from the University of St Andrews in 2012.
% He has expertise in parallel patterns, adaptivity and runtime
% systems for both shared-memory and distributed\-/memory systems.
He has published 15 research papers, most of them in large international
conferences and journals. His PhD focused on load balancing in heterogeneous distributed computing systems, similar to the ones that will be considered in \TheProject{}, which was incorporated into a Grid computing middleware for executing large-scale symbolic computations as a part of the \science EU FP6 project. Since then, he has worked mostly on pattern-based
programming in the context of \paraphrase{} and \rephrase{} EU projects, 
developing novel scheduling and mapping-based techniques for adapting the high-level patterned code to the heterogeneous architectures. His most recent work is on dynamic scheduling of threads of parallel applications onto non-uniform memory architecture machines. He is currently a Post\-/Doctoral fellow on the \textbf{Discovery} UK project which deals with discovery of parallel patterns in the legacy C/C++ code. He has authored the \euro 3.6M \rephrase EU project (together with Kevin Hammond and Christopher Brown) and the 720k EPSRC \textbf{Discovery} project (together with Kevin Hammond and Murray Cole from University of Edinburgh).
%where he investigates mechanisms for mapping
%patterned applications to integrated heterogeneous parallel systems.
%\pagebreak
%\cite{europar2013,parco2015,pdp2016}

%\nocite{GMB+13,BLT10,BDH+13,JHY08,JaHa13}
% \nocite{DBLP:conf/popl/JostHLH10}

%\pagebreak
\khcomment{Update the publications and refer to recent ones, e.g. RPL}
\subsubsection*{Relevant publications}
\begin{itemize}
\item Georgios Chasparis, Michael Rossbory, Vladimir Janjic, \emph{Efficient Dynamic Pinning of Parallelized Applications by Reinforcement Learning With Applications}. Proc. 2017 International European Conference on Parallel and Distributed Computing (EURO-PAR 2017), pp.164--176, Springer, 2017. 
%\item Vladimir Janjic, Kevin Hammond, \textit{How to be a Successful
%    Thief: Feudal Work-Stealing for Irregular Divide-and-Conquer
%    Applications on Heterogeneous Distributed Systems}, Proc. 2013
%  International European Conference on Parallel and Distributed
%  Computing (Euro-Par 2013), pp 114--125, Springer, 2013.
\item Vladimir Janjic, Christopher Brown, Kenneth MacKenzie, Kevin Hammond, Marco Danelutto, Marco Aldinucci, Jose Daniel Garcia. \emph{RPL: A Domain-Specific Language for Designing and Implementing Parallel C++ Applications}, 
Proc. of the 24th Euromicro International Conference on Parallel, Distributed and Network-based Processing, PDP 2016, pp. 288-295, IEEE, 2016.
\item Mehdi Goli, John McCall, Christopher Brown, Vladimir Janjic and
  Kevin Hammond, \textit{Mapping Parallel Programs to Heterogeneous
    CPU/GPU Architectures Using a Monte Carlo Tree Search}, Proc. 2013
  IEEE Congress on Evolutionary Computing (CEC 2013), 2013.
\item Christopher Brown, Huiqing Li, Simon Thompson, \textit{An
    Expression Processor: A Case Study in Refactoring Haskell Programs},
  Proc. 2010 Symposium on Trends in Functional Programming (TFP 2010),
  pp. 31--49, Springer, 2010.
\item Christopher Brown, Marco Danelutto, Kevin Hammond, Peter
  Kilpatrick and Archibald Elliott, \textit{Cost-Directed Refactoring
    for Parallel Erlang Programs}, International Journal of Parallel
  Processing (IJPP), pp 1--19, Springer, 2013.
\end{itemize}

\pagebreak
\subsubsection*{Relevant Research Projects}

\begin{itemize}
% \item
% Automatic Prediction of Resource Bounds for Embedded Systems (EmBounded, IST-2004-510255, 2005-2008, \url{http://www.embounded.org});
\item
Asynchronous and Dynamic Virtualisation through performance ANalysis to support Concurrency Engineering (ADVANCE,
ICT-248828, 2010-2013, \url{http://www.project-advance.eu});
\item
Parallel Patterns for Adaptive Heterogeneous Multicore Systems (ParaPhrase, ICT-288570, 2011-2014, \url{http://paraphrase-ict.eu});
\item
Refactoring Parallel Heterogeneous Resource-Aware Applications --- a Software Engineering Approach (RePhrase, ICT-644235, 2015-2018);
\item
Parallel Pattern Discovery and Program Shaping for Heterogeneous Manycores (\discovery, UK EPSRC EP/P020631, 2017-2020).
\item
Time,   Energy   and   security   Analysis   for   Multi/Many-core   heterogeneous PLAtforms,   (\teamplay, ICT-779882,  2018-2020).
\end{itemize}

\Participant{IBM}{(\url{http://www.research.ibm.com/labs/haifa/)}}


\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/ibm.jpg}
\vspace{-1cm}
\end{wrapfigure}

\ 

%\vspace{24pt}
For more than sixty years, IBM Research, as the world's largest IT research organisation has been the innovation engine of the IBM corporation. Since the beginning of 2000, IBM has spent \$75 billion in R\&D, enabling IBM to deliver key innovations and maintain U.S. patent leadership for the 21st consecutive
year in 2013.
IBM also participates in and contributes to the work of standards consortia, alliances, and formal national and international standards organisations. 
The IBM Haifa Research Lab is IBM's largest research laboratory outside of the United States, 
employing almost 500 researchers, the majority of whom hold doctorate and master degrees in computer science, electrical engineering, mathematics, and related fields. Since its founding in 1972, HRL has conducted world class research vital to IBM's success. R\&D projects are being executed today in areas such as Cognitive computing, Healthcare and Life Sciences, Verification Technologies, Telco, Machine Learning, Cloud Computing, Multimedia, Active Management, Information Retrieval, Programming Environments and Information and Cyber Security. The Quality and Security Department of the IBM Research Haifa includes researchers in the fields of cyber security and privacy. As a multi-disciplinary research area, researchers come from different research domains including verification, data analytic, operating systems and runtime systems, languages and compilers, network systems, and protocols and cloud technologies.
HRL has a long history of successful participation in EU projects. A partial list includes the following: SMESEC (H2020), REPHRASE (H2020), SHARCS (H2020), PINCETTE (FP7, coordinator), RESERVOIR (FP7, coordinator), CloudWave (FP7, coordinator), SHADOWS (FP6, coordinator), CASPAR (FP6), HiPEAC (FP6 + FP7), SARC (FP6), ACOTES (FP7), MilePost (FP7), HYPERGENES (FP7), HERMES (FP7), SAPIR(FP6, coordinator), PROSYD (FP6, coordinator), Modelplex (FP6, coordinator).
The Quality and Security Department develops advanced tools and technologies spanning the entire spectrum of Functional Verification, Code Analysis and Cyber Security.

As a global leader in IT security, IBM offers the strategies, capabilities, and technologies necessary to help organizations in the private and public sectors preemptively protect the organisation from threats and address the complexities and growing costs of security risk management and compliance. IBM is helping to solve essential security challenges including:
\begin{itemize}
\item
  Better secure data and protect privacy
\item
	Control network access and help assure resilience
\item
	Defend mobile and social workplace
\item
	Manage third-party security compliance
\item
	Address new complexity of cloud and virtualization
\item
	Build a risk-aware culture
\end{itemize} 	
To facilitate a comprehensive offering IBM is continuously investing in emerging technologies in the area of security intelligence and has a wide variety of security products and services. IBM is recognized in the industry as a leader in IT cyber security.
In recent years, IBM has acquired several cyber security start-ups in Israel increasing its R\&D presence in the region. IBM has recently also announced the establishment of a Cyber Center of Excellence (CCoE) in Beer-Sheva, Israel. The IBM Haifa Research Lab, as a well recognized IBM research facility and the largest one outside of the US, is collaborating with European research facilities to support the buildup of the CCoE as well as the acquired cyber security start-ups.

\vspace{10pt}
\textbf{IBM leads work package WP6 on Safety and Security, and contributes to focusing on secure code development. It contributes to WP2,  WP7 and WP8 with its expertise on security and implementations of security policies and mechanisms}
\vspace{10pt}

\paragraph{Sharon Keidar- Barner}  is the manager of the Emerging Security and Quality Technologies group at \IBM. She holds B.Sc. and M.Sc. degrees (both Summa Com Laude) both in 
Computer Science from Technion - the Israel Inst. of Technology. Since 2005, Sharon has been the manager and lead architect of an IBM research group which was at the forefront of the company's technology development in the area of reliability and assurance of complex systems. Sharon's technical contributions have been recognized by the receipt of an IBM outstanding technical achievement award.

\paragraph{Fady Copty} is a Research Staff Member at \IBM, where he leads artificial intelligence and machine learning research in advanced security defence technologies. Before focusing on security research, Fady worked on the formal verification of IBM POWER processors, for which he received an IBM Outstanding Technical Achievement Award in 2013. Before joining IBM in 2008, he worked at Intel on the development of formal verification algorithms and tools; he received several awards for the successful pilot of BMC (Bounded Model Checking). Fady received his BSc in computer science from the Technion - Israel Institution of Technology, where he graduated cum laude. He also has a BFA in cinema and television from Tel-Aviv University.

\paragraph{Dmitry Pidan} holds M.Sc (2013) and B.Sc (2003) degrees in Computer Science
from Technion - the Israel Institute of Technology. Since joining IBM HRL at
2000, Dmitry participated in the development of IBM hardware model checker
RuleBase PE, property specification language PSL, and variety of tools for
both static and dynamic verification. Since 2012, Dmitry leads the development
of ExpliSAT - an IBM model checker for C/C++ software.     

\subsubsection*{List of Publications}


%Formal verification:
\begin{itemize}
 \item Sharon Barner, Cindy Eisner, Ziv Glazberg, Daniel Kroening, and Ishai Rabinovitz. 
 \newblock \textit{ExpliSat: Guiding SAT-based software verification with explicit states.}
 \newblock In Hardware and Software, Verification and Testing, pp. 138-154. Springer Berlin Heidelberg, 2007. 

% \item Hana Chockler and Sitvanit Ruah. 
% \newblock Verification of software changes with explisat.
% \newblock In Hot Topics in Software Upgrades (HotSWUp), 2012 Fourth Workshop on, pp. 31-35. IEEE, 2012. 

\item Hana Chockler, Dmitry Pidan, and Sitvanit Ruah. 
\newblock \textit{Improving Representative * Computation in ExpliSAT.}
\newblock In Hardware and Software: Verification and Testing, pp. 359-364. Springer International Publishing, 2013.

\item Michael Factor, David Hadas, Aner Harnama, Nadav Har'el, Elliot K. Kolodner, Anil Kurmus, Alexandra Shulman-Peleg, and Alessandro Sorniotti.
\newblock \textit{Secure Logical Isolation for Multi-tenancy in cloud storage.} 
\newblock In Mass Storage Systems and Technologies, IEEE / NASA Goddard Conference on, pp. 1-5, 2013 IEEE 29th Symposium on Mass Storage Systems and Technologies (MSST), 2013.

\item Ein-Dor, L.; Goldschmidt, Y.; Lavi, O.; Miller, G.E.; Ninio, M.; Dillenberger, D.
\newblock \textit{Analytics for resiliency in the mainframe.} 
\newblock In IBM Journal of Research and Development , vol.57, no.5, pp.8:1,8:5, Sept.-Oct. 2013.
 
\item Torsten Bandyszak, Micha Moffie, Abigail Goldsteen, Panos Melas, Bassem I. Nasser, Costas Kalogiros, Gabriele Barni, Sandro Hartenstein, Giorgos Giotis, Thorsten Weyer
\newblock \textit{Supporting Coordinated Maintenance of System Trustworthiness and User Trust at Runtime.}
\newblock In IFIPTM 2016: 96-112. 
\end{itemize}

\subsubsection*{Relevant Research Projects}
\begin{itemize}
\item
Validating Changes and Upgrades in Embedded Software (PINCETTE, ICT-257647);
\item
CloudWave: Agile Service Engineering for the Future Internet (CloudWave, ICT-610802);
\item
Refactoring Parallel Heterogeneous Resource-Aware Applications --- a Software Engineering Approach (RePhrase, ICT-644235);
\item
Secure Hardware-Software Architectures for Robust Computing Systems (SHARCS, ICT-322014).

\end{itemize}

\Participant{PRL}{(http://www.programmingresearch.com)}
\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/PRL.png}
\vspace{-3cm}
\end{wrapfigure}


Established in 1985, Programming Research Ltd. (\PRshort{}), ISO 9001 and TickIT plus certified, is recognised throughout the industry as a pioneer in static analysis, championing automated coding standard inspection and defect detection, delivering its expertise through industry-leading software inspection and standards enforcement technology used by over 3,000 companies globally.

\PRshort{}'s industry-leading tools, QA-C, QA-C++ and QA-Verify, offer the closest possible examination of C and C++ code. All contain powerful, proprietary parsing engines combined with deep accurate dataflow which deliver high fidelity language analysis and comprehension. They identify problems caused by language usage that is dangerous, overly complex, non-portable or difficult to maintain. Plus, they provide a mechanism for coding standard enforcement. 
\PRshort{} has developed High Integrity C++ standard which is very popular in scientific community (over 30,000 downloads).

\PRshort{} has corporate offices in UK, USA, India, Ireland and Netherlands, complemented by a worldwide distribution network.

\vspace{10pt}
\textbf{PRL contributes to work packages WP2, WP6, WP7 and WP8 with expertise in static analysis for security and compliance of code to standards.}
\vspace{10pt}



\paragraph{Dr. Evgueni Kolossov} 
(R\& D Director at Programming Research Ltd.)
Evgueni spent his whole career in science and software development. He
has managed software projects and people in scientific areas for more
than 30 years. He is a internationally recognised specialist with over
60 publications in different areas, regular presenter at scientific
and industrial conferences. During the last 10 years he has been
developing new products for ID Business Solutions (IDBS) for the
pharmaceutical industry, has been managing software development at
Scientific Software Solutions, and at Cerulean (Molins PLC). Evgueni
has a PhD in Organic Chemistry and Computer Science from
St.\ Petersburg Institute of Technology (1981), and a B.Sc. from St.\ Petersburg University (1978). He is a member of the New York Academy of Science.

% \paragraph{Selected publications}
% \begin{itemize}
% \item Evgueni Kolossov, Andrew Lemon,  Medicinal chemistry tools: making sense of HTS data. European Journal of Medicinal Chemistry, Volume 41, Issue 2, February 2006, Pages 166--175
% \item Robert Stanforth, Evgueni Kolossov, Boris Mirkin, Hybrid k\-Means: Combining Regression\-Wise and Centroid\-Based Criteria for QSAR. Selected Contributions in Data Analysis
%  and Classification Studies in Classification, Data Analysis, and Knowledge Organization 2007, pp 225-233
% \item Robert W. Stanforth, Evgueni Kolossov, and Boris Mirkin, A Measure of Domain of Applicability for QSAR Modelling Based on Intelligent K-Means Clustering. 
% QSAR \& Combinatorial Science, Volume 26, Issue 7, pages 837--844, July 2007
% \item E. Kolossov \& R. Stanforth, The quality of QSAR models: problems and solutions. SAR and QSAR in Environmental Research, Volume 18, Issue 1-2, 2007 
% \end{itemize}

\paragraph{Dr. Wojciech Basalaj}
Wojciech graduated from King's College, London with a B.Sc. in Computer Science in 1997. As part of the course, he undertook a one-year industrial placement at Lucent Technologies Wireless in Winchester, UK. Wojciech obtained his Ph.D. in the field of Information Visualisation at Trinity College, Cambridge in 2000. Since then he works for \PRshort{}, initially in the Consulting Services Group, and for the last 4 years in the R\&D Group as a Senior Developer.

% \paragraph{Relevant publications}
% \begin{itemize}
% \item W. Basalaj, Incremental multidimensional scaling method for database visualization, Proceedings of Visual Data Exploration and Analysis VI, SPIE volume 3643, pp. 149-158, San Jose, California, USA, January 1999
% \item W. Basalaj and K. Eilbeck, Straight-Line Drawings of Protein Interactions, Proceedings of Graph Drawing '99, LNCS volume 1731, pp. 259-266, Stirin, Czech Republic, September 1999
% \item W. Basalaj, Proximity Visualization of Abstract Data, Technical Report 509, University of Cambridge Computer Laboratory, January 2001.
% \item W. Basalaj, Correlation Between Coding Standards Compliance and Software Quality, Embedded Systems Conference Silicon Valley, San Jose, California, USA, April 2006
% \item W. Basalaj, How Good is Your Compiler (as Finding Coding Defects), Embedded Systems Conference UK, Farnborough, UK, October 2009
% \end{itemize}

\paragraph{Richard Corden}
Richard has B.Sc. in Computer Applications from Dublin City University, 1998.
He has 18 Years of Professional Experience.
Richard has primarily responsibility as lead developer for QA C++, a Programming Research Ltd. (\PRshort{}) static analysis tool for the C++ language.  Through his work on QA C++ he has gained a deep knowledge of the C++ language and represents \PRshort{} as a voting member of the ISO C++ Committee.  He has extensive experience in the development and implementation of C++ coding standards and is a co-author of MISRA C++ and \PRshort{}'s High Integrity C++.  He has also spoken on the benefits of using coding standards at ``using cpp::std 2013" conference in Madrid.

% \paragraph{Relevant publications}
% \begin{itemize}
% \item MISRA C++ 2008: Guidelines for the use of the C++ Language in Critical Systems, June, 2008, \PRshort{}
% \item Richar Corden, High Integrity C++, Coding Standard Version 4.0, 3 October 2013, \PRshort{}
% \end{itemize}

\paragraph{Christof Meerwald}
obtained a M.Sc. in Computer Science, specialising in distributed simulation, from the University of Salzburg, Austria in 2001. He then worked for various companies in Austria and the United Kingdom as a software developer on a diverse range of systems from self-service banking terminals and cash machines to network management systems for optical networks, but also started contributing to the Open Watcom C++ compiler front-end.
He is currently working on static code analysis for C++, focusing on the aspects of parsing the C++ language as well as source code analysis checks for coding standard compliance. He is also a member of the ISO C++ Standards Committee and has more than 15 years software development experience with around 10 years of experience working on C++ parsers.

\paragraph{Alexander Gilding}
obtained M.Sc. in Computer Science from Bristol in 2011. Previous experience is in game development, 3D graphics optimization (made a "million-node" instancing patch to the open-source miniB3D engine to allow display of very large numbers of on-screen objects on non-gaming hardware; also did some raytracing work), and medical training software (Vycaria VCL).
Relevant interests are in JIT (and its abuses to allow for language extensions), realtime GC, type-based memory management (e.g. linear types, safe regions), and generating branchless, non-allocating code for SIMD execution, as well as how such code can be heuristically derived. I have no current publications, but have been cited by WG14 member Jens Gustedt in his book "Modern C".
he is current member of the QAC/QAC++ parser development team since Nov 2015.

\pagebreak
\paragraph{Franck Claudel}
is a senior C++ analyst programmer with extensive experience in the development of large scale and distributed software. He holds a Masters degree in Computer Information Systems from Ecole Internationale des Sciences du Traitement de l'Information. With previous roles in the electronic trade processing and Business Intelligence industries, Franck has acquired in-depth knowledge of design, development and quality processes.

\paragraph{List of Publications}
\begin{itemize}

\item Robert W. Stanforth, Evgueni Kolossov, and Boris Mirkin, \textit{A Measure of Domain of Applicability for QSAR Modelling Based on Intelligent K-Means Clustering.} 
QSAR \& Combinatorial Science, Volume 26, Issue 7, pages 837--844, July 2007

\item Richard Corden, \textit{High Integrity C++, Coding Standard Version 4.0}, 3 October 2013, \PRshort{}
\item W. Basalaj, \textit{How Good is Your Compiler (as Finding Coding Defects)}, Embedded Systems Conference UK, Farnborough, UK, October 2009
\item E. Kolossov \& R. Stanforth, \textit{The quality of QSAR models: problems and solutions}. SAR and QSAR in Environmental Research, Volume 18, Issue 1-2, 2007 
\item W. Basalaj, Proximity Visualization of Abstract Data, Technical Report 509, University of Cambridge Computer Laboratory, January 2001.
\item W. Basalaj, Correlation Between Coding Standards Compliance and Software Quality, Embedded Systems Conference Silicon Valley, San Jose, California, USA, April 2006

\end{itemize}

\subsubsection*{Relevant Research Projects}

\begin{itemize}
\item
Refactoring Parallel Heterogeneous Resource-Aware Applications --- a Software Engineering Approach (RePhrase, ICT-644235, 2015-2018);
\end{itemize}



\Participant{INRIA}{(http://www.inria.fr)}

\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/logo-inria.jpg}
\vspace{-1cm}
\end{wrapfigure}

\ 


\INRIA{}, the French National Institute for computer science and applied
mathematics, promotes ``scientific excellence for technology transfer
and society''. Graduates from the world's top universities, Inria's
2,600 employees rise to the challenges of digital sciences. Research
at Inria is organised in ``project teams'' which bring together
researchers with complementary skills to focus on specific scientific
projects. With this open, agile model, Inria is able to explore
original approaches with its partners in industry and academia and
provide an efficient response to the multidisciplinary and application
challenges of the digital transformation. The source of many
innovations that add value and create jobs, Inria transfers expertise
and research results to companies (startups, SMEs and major groups) in
fields as diverse as healthcare, transport, energy, communications,
security and privacy protection, smart cities and the factory of the
future.

Two of Inria's teams are involved in this project. The first one is
PARKAS, which focuses on the design, semantics and compilation of
high-level languages which allow to go, all the way down, from a
parallel deterministic specification to a target embedded code that
may execute on parallel (multi-core) architectures. The team bases its
research on the theory and practice of synchronous languages, typed
functional languages and modern compilation techniques (polyhedral
compilation) to obtain provably safe and efficient code. The team
develops languages and compilers (e.g., Lucid Synchrone, ReactiveML,
contributions to GCC) used as vehicules for validating, communicating
and transfering research results. Lucid Synchrone, for example, is the
basis of the new SCADE 6 language commercialised since 2008 by
Esterel-Technologies.

The second team is AOSTE, which conducts research on design of
real-time embedded systems. Here ``design'' means altogether:
high-level modelling, transformation and analysis, and implementation
onto embedded platforms. To cover this vast spectrum we specialise the
type of formalisms considered. We focus on synchronous formalisms,
and on the AAA/SynDEx codesign methodology. We insist on full-fledge
sound semantic definitions of our constructs, using the so-called
``synchrony'' hypothesis, to allow and justify powerful techniques
for analysis, optimization, verification, and synthesis/compilation
onto distributed embedded architectures under real-time constraints.


\vspace{10pt}
\textbf{\INRIAshort leads work package WP2 on Parallelism Modelling and Synthesis
and contributes to WP3, WP4, WP5, WP7 and WP8 with expertise in dataflow 
modelling languages and parallelism.}
\vspace{10pt}

\paragraph{Albert Cohen} is a senior research scientist at Inria. He graduated from Ecole Normale Suprieure de Lyon, and
received his Ph.D. from the University of Versailles in 1999 (awarded two national prizes). He has been a part-time
associate professor at Ecole Polytechnique for 12 years, and has been a visiting scholar at the University of Illinois
and an invited professor at Philips Research, both for six months. Albert Cohen works on paralleling and optimising
compilers, parallel programming, and synchronous programming for embedded systems. He has been the general
or program chair of major conferences, including PLDI, PPoPP, HiPEAC, CC, and the DAC embedded software track.
He has been the advisor for 24 PhD theses and co-authored more than 160 peer-reviewed publications. Several
research projects initiated by Albert Cohen resulted in selective transfer to production compilers. In particular, Albert
Cohen pioneered the transfer of polyhedral compilation technology into industrial products, including IBM XL, GCC,
and LLVM.

\paragraph{Dumitru Potop-Butucaru} received his Ph.D. degree from the Ecole des Mines de Paris in 2002, and his Habilitation
to conduct research (HDR) from University Pierre et Marie Curie (Paris 6) in 2015. Since 2005, he has been
a tenured researcher in Inria Paris, in the AOSTE team. Continuing a successful history of academic and industrial
collaborations, his current research introduces and promotes the concept of Real-Time Systems Compilation
(\url{https://hal.inria.fr/tel-01264021}). By analogy with classical compilation, real-time systems compilation
consists in the fully automatic construction of running, correct-by-construction implementations from functional and
non-functional specifications of embedded control systems. As in a classical compiler, the whole process must be
fast (thus enabling a trial-and-error design style) and produce efficient code. This requires the use of fast heuristics,
and the use of fine-grain platform and application models. Unlike a classical compiler, a real-time systems compiler
must take into account non-functional properties of a system and ensure that non-functional requirements are respected
(in addition to functional correctness).

\pagebreak
\subsubsection*{Relevant publications}
\begin{itemize}
\item I. Llopard, C. Fabre, and A. Cohen. \textit{From a formalized parallel action language to its efficient code generation.}
ACM Transactions on Embedded Computing Systems (TECS), 16(2), January 2017.
\item C. Hong, A. Cohen, S. Krishnamoorthy, L.-N. Pouchet, J. Ramanujam, F. Rastello, and P. Sadayappan. \textit{Effective
padding of multi-dimensional arrays to avoid cache conflict misses.} In ACM Conference on Programming
Language Design and Implementation (PLDI), Santa Barbara, CA, June 2016.
\item A. Cohen, V. Perrelle, D. Potop-Butucaru, M. Pouzet, E. Soubiran, Zhen Zhang. \textit{Hard Real Time and Mixed
Time Criticality on O-The-Shelf Embedded Multi-Cores.} in Proceedings ERTS2, Toulouse, France, 2016.
\url{https://hal.inria.fr/hal-01425887}
\item S. G. Bhaskaracharya, U. Bondhugula, and A. Cohen. \textit{Automatic storage optimization for arrays.} ACM Transactions
on Programming Languages and Systems (TOPLAS), 38(3), May 2016. Selected for presentation at PLDI 2016.
\item T. Carle, D. Potop-Butucaru, Y. Sorel, D. Lesens. \textit{From Dataflow Specification to Multiprocessor Partitioned
Time-triggered Real-time Implementation.} in Leibniz Transactions on Embedded Systems vol 2(2), 2015.
\url{http://ojs.dagstuhl.de/index.php/lites/article/view/LITES-v002-i002-a001}
\end{itemize}

\subsubsection*{Relevant Research Projects}
\begin{itemize}
\item ASSUME (ITEA3, 2015-2018). \textit{Affordable Safe and SecUre Mobility Evolution.} INRIA contributed on verified compilation and on the parallelization of hard real-time applications onto multi-cores. \url{http://assume-project.eu/}
\item EMC$^2$ (ARTEMIS	2014-2017). \textit{Embedded Multi-Core systems for Mixed Criticality applications in dynamic and changeable real-time environments.} EMC$^2$ finds solutions for dynamic adaptability in open systems, provides handling of mixed criticality applications under real-time conditions, scalability and utmost flexibility, full scale deployment and management of integrated tool chains, through the entire lifecycle.	\url{https://www.artemis-emc2.eu/} \end{itemize}

\Participant{SCCH}{(http://www.scch.at)}

\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/SCCH.jpg}
\vspace{-1cm}
\end{wrapfigure}

\SCCH{} (\url{www.scch.at}) is an Austrian research and technology organisation, funded in 1999 by several institutes of Johannes Kepler University, Linz. Its primary focus is on applied research in the fields of software and data science. 
In projects with partner companies state-of-the-art research results are applied to practical industrial projects to increase and maintain their competitiveness. One of the focus areas is Data Analysis Systems (DAS), specialising on the advancement and application of methods for the analysis and modelling of complex and massive sensor data in its (industrial) application context. Particular application domains include
\begin{inparaenum}[a)]
\item modelling, prognosis, forecast and control of systems
\item industrial fault detection, diagnosis and prognosis
\item the discovery of knowledge and structure in industrial processes.
\end{inparaenum}
\SCCHshort{} is or was involved in the EU-funded projects H2020 ALOHA (Grant Agreement 780788), TRESSPASS (SEC-2016-2017-2, Proposal Nr. 787120) RePhrase (ICT-644235), ParaPhrase (IST-2011-288570), ADVANCE (IST-2010-248828), SECO, and FACETS, where in ALOHA and TRESSPASS SCCH contributes with its expertise in deep learning and the latter are concerned with parallel computation.  The participating investigators are project managers or researchers for relevant applied research projects in the fields of machine learning, parallelisation, and scheduling, and are active in the research community by publishing and reviewing for journals and conferences.

\vspace{10pt}
\textbf{\SCCHshort{} leads WP5 on Optimisation of Extra-Functional Properties and contributes to WP2, WP6, WP7 and WP8 with their expertise in machine learning, multiobjective optimisations, scheduling, parallel programming and software analytics.}
\vspace{10pt}

\paragraph{Dr Georgios Chasparis}
received the M.Sc. and Ph.D. degrees from the University of California, Los Angeles, CA, in 2004 and 2008, respectively. From 2008 to 2010, he was a Postdoctoral Fellow with the Department of Electrical and Computer Engineering at the Georgia Institute of Technology, Atlanta, GA, and from 2010 to 2012, he was a Postdoctoral Fellow with the Department of Automatic Control, Lund University, Lund, Sweden. Since 2012, he has been with the \SCCH{}, Austria. His current research interests include distributed optimisation and evolutionary learning. He has participated in several research projects both in the United States (Air Force Office of Scientific Research, \#FA9550-09-1-0420, \#FA9550-09-1-0538 and \#FA9550-10-1-0573) and in Europe (Linnaues Center of the Swedish Research Council, mpcEnergy and modDiscovery (Regionale Wettbewerbsf\"{a}higkeit O\"{O} 2007-2013), and H2020 RePhrase (No. 644235)), concerned with the design of distributed learning algorithms for efficient behaviour of multi-component systems. He has been author of several scientific articles in this topic published in high-impact journals including SIAM Journal on Control and Optimization, (Elsevier) Automatica, and International Journal of Game Theory. Part of his recent work is specialised in the problem of allocating computing resources to patterned parallel applications through learning-based optimisation techniques, which has led to the development of novel resource managers built over Linux. 

\paragraph{Dr Thomas Natschl\"ager}
(Scientific Head of Data Analysis Systems Group) published more than 45 papers in the area of computational neuroscience,
computational intelligence and machine learning and serves as reviewer for various
journals (e.g., Neural Computation, IEEE TNN, Neurocomputing) and conferences.
During his research he developed a simulation environment for large
scale neural circuits, %\cite{pecevski2009}, 
where he gained experience in developing parallel and distributed software in C++.
He is project manager for several relevant applied research projects like imPACts (online monitoring of chemical processes), proKNOW (knowledge discovery of industrial process analysis), inFADIA (fault detection and diagnosis for industrial systems), EStore-M and Flex+ (model predictive control of energy management systems).
He also contributed significantly to the development of the machine learning tools \emph{MLF} (machine learning framework for Mathematica; \url{www.unisoftwareplus.com/products/mlf/}) and \emph{MLPP} (machine learning with parallel patterns; \url{sourceforge.net/projects/ml-pp/}).

\pagebreak
\subsubsection*{List of Publications}

\begin{itemize}

\item G. Chasparis, M.~Rossbory, \textit{Efficient Dynamic Pinning of Parallelized Applications by Distributed Reinforcement Learning}, International Journal of Parallel Programming, 2017.

\item G. Chasparis, M. Rossbory, and V. Janjic, \textit{Efficient Dynamic Pinning of Parallelized Applications by Reinforcement Learning with Applications}, Euro-Par 2017, LNCS 10417, pp. 1-13, 2017.

\item G. Chasparis, M. Maggio, E. Bini, and K. \AA{}rz\'{e}n. \textit{Design and implementation of distributed resource management for time-sensitive applications}, Automatica, Vol.~64, pp.~44--53, 2016.

\item G.~Chasparis, \textit{Reinforcement-Learning-Based Efficient Resource Allocation with Demand-Side Adjustments}, European Control Conference, Linz, Austria, pp.~3071--3077, 2015.

% \item C. Brown, V. Janjic, K. Hammond, H. Sch\"oner, K. Idrees, J. Gracia, and C. Glass, \textit{ Agricultural Reform: More Efficient Farming Using Advanced Parallel Refactoring Tools}, in Proceedings of the 22nd Euromicro International Conference on Parallel, distributed and network-based Processing PDP 2014, Torino, Italy, 2014.

% \item G. Chasparis, M. Maggio, K. \AA{}rz\'{e}n, and E. Bini. \textit{Distributed management of CPU resources for time-sensitive applications}. In American Control Conference, Washington, DC, June 2013.

\item M. Maggio, E. Bini, G. Chasparis, and K. \AA{}rz\'{e}n. \textit{A game-theoretic resource manager for real-time applications.} In 25th Euromicro Conference on Real-Time Systems (ECRTS), 2013.

 %% \item A. Kosorus, M. Zhariy, T. Natschl\"{a}ger, B. Freudenthaler, J. K\"{u}ng, \textit{On the Relevance of Graphical Causal Models for Failure Detection for Industrial Machinery}. In: Moreno-D\'{i}az R., Pichler F., Quesada-Arencibia A. (eds) Computer Aided Systems Theory - EUROCAST 2013, LNCS 8111, 2013.

%% \item R. Ramler, T. Natschl\"{a}ger. \textit{Applying heuristic approaches for predicting defect-prone software components}. In R. Moreno-Diaz, F. Pichler, A. Quesad (editor/editors) , Computer Aided Systems Theory - Proc. EUROCAST 2011, Part I, Lecture Notes in Computer Science, volume 6927, pages 384-391, Springer, February, 2012.

% \item V. Wieser, C. Grelck, H. Sch\"oner, P. Haslinger, K. Bosa, and B. Moser, \textit{GPU-based Image Processing Use Cases: A High-Level Approach}, presented at the International Conference on Parallel Computing (ParCo 2011), 2011, vol.~22, pp. 199???-206.

%\item D. Pecevski, T. Natschl\"ager, and K. Schuch. PCSIM: a parallel simulation environment for neural circuits fully integrated with Python. Frontiers in Neuroinformatics, 3(11), 2009.

\end{itemize}

\subsubsection*{Relevant Research Projects}

\begin{itemize}

\item RePhrase (H2020, ICT-644235) - Refactoring Parallel Heterogeneous Resource-Aware Applications. SCCH's role: reinforcement learning based dynamic scheduling and industrial use cases

\item ParaPhrase (IST-2011-288570) - Parallel Patterns for Adaptive Heterogeneous Multicore Systems. SCCH's role: use cases in the area of machine learning

\item ADVANCE (IST-2010-248828) - Asynchronous and Dynamic Virtualisation through performance ANalysis to support Concurrency Engineering, \url{http://www.project-advance.eu});

\item ALOHA (H2020 Grant Agreement 780788) - Software framework for runtime-Adaptive and secure deep Learning On Heterogeneous Architectures. SCCH's role: develop deep transfer learning based methods for surveillance applications

\item TRESSPASS (H2020 SEC-2016-2017-2, Proposal Nr. 787120) - robusT Risk basEd Screening and alert System for PASSengers and luggage. SCCH's role: develop deep learning based methods for security applications

\end{itemize}


\Participant{CODEPLAY}{(http://www.codeplay.com)}

\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/codeplay.png}
\vspace{-1cm}
\end{wrapfigure}

\ 

Codeplay Software Ltd. is an SME founded in 2002 to commercialise compiler technology, developed by the founders Andrew Richards and Jens-Uwe Dolinsky, with funding from Jez San (founder of Argonaut and Arc).  
Codeplay has produced software solutions for a series of innovative heterogeneous processors for games systems (Ageia PhysX, PlayStation 2, PlayStation 3, Movidius and Qualcomm). The company works with semiconductor companies to develop compiler tools to enable software to be ported to their processors. 

% \subsubsection*{Role in \TheProject{}}
\vspace{10pt}
\textbf{\CODEPLAYshort{} leads WP4 on Distributed Runtime Infrastructure and contributes to WP2, WP3, WP5, WP6, WP7 and WP8 with their expertise in compilation, heterogeneous systems, AI systems and deployment.}
\vspace{10pt}

\subsubsection*{Key Personnel Involved in the Project}
\paragraph {Andrew Richards} is CEO and co-founder of the company. He is also chair of the HSA Foundations Software working group. Andrew was previously a game programmer at Eutechnyx, where he wrote best-selling titles such as Pete Sampras Tennis and Total Drivin. He graduated with a degree in Physics and Computer Science from the University of Cambridge and used that combined knowledge to develop innovative physics simulation technologies for those games.

\paragraph{Dr Jens-Uwe Dolinsky} Codeplay`s Chief Scientist in charge of research to improve the company`s in-house core technology. Uwe graduated in computing from the University of Wismar in Germany and holds a PhD in robotics/machine learning from John Moores University in Liverpool, UK. Uwe has over 15 years of extensive experience delivering commercial compilers. His main expertise is in compiler implementation and testing, including both high and low-level performance optimisations; front-ends and language extensions, compiler support enabling high-level heterogeneous programming models; and compiler back-ends for numerous heterogeneous architectures. Uwe leads various commercial and research projects, including FP7 PEPPHER; and is author/co-author of Codeplays core patents and publications. 

\paragraph{Michael Wong}  is VP of Research \& Development of the company. He is the chair of Khronos SYCL Heterogeneous C++ programming language and a senior leader of the ISO C++ Standard committee, holding positions of director, and VP, as well as chairing several work groups. Previously, he was the Senior Technical Strategy Architect for IBMs XL Compilers. He is the Canadian Head of Delegation to the ISO C++ Standard and a past CEO of OpenMP. He is also a Director and VP of ISOCPP.org, and Chair of Programming Languages for Canadas Standards Council. He chairs WG21 SG14 Games Development/Low Latency/Financial/Embedded Devices and WG21 SG5 Transactional Memory, and is the co-author of a book on C++ and a number of C++/OpenMP/Transactional Memory features. He holds a B.Sc from University of Toronto, and a Masters in Mathematics from University of Waterloo.
\paragraph{Dr Alastair Murray} received his BSc and PhD degrees from the
University of Edinburgh and has 9 years of research and industry experience in compilation and programming for parallel and heterogeneous systems. He is now team lead of CODE's internal OpenCL implementation and parallel hardware interface.

\paragraph{Dr Mehdi Goli} Recently joint Codeplay in September 2017 as a Senior Software Engineer- AI Parallelisation,  Mehdi Goli is a Team Lead of Eigen and SYCL-BLAS projects. He got his PhD in Parallel Computing at Robert Gordon University, Aberdeen (2015). As part of his PhD work, he has created the OpenCL backend for FastFlow framework. His research interest are Parallel Computing,  GPGPU (CUDA/OpenCL/SYCL) programming model, AI parallelisation and  High Performance Computing.  Before joining codeplay as a permanent staff, he was a research associate at the University of West of Scotland for 2 years working with the Codeplay company through Knowledge Transfer Program (KTP) to deliver the VisionCPP  framework.

\paragraph {Ralph Potter} Is a research engineer and doctoral candidate registered at the University of Bath, but located at Codeplay's Edinburgh office. Ralph holds a master's degree in Computer Science from the University of Reading (2003). Before joining Codeplay he spent 9 years as lead programmer at a private software company developing healthcare software. Ralph's research interests vary from in real-time graphics, programming models for computer graphics and the application of heterogeneous processors to graphics algorithms. He is currently working on raytracing optimisation on Heterogeneous System Architecture.

\paragraph{Illya Rudkin} Generalist principle engineer with 20 years programming experience ranging from games development tools, anti-hacking measures and now CODEs lead on safety-critical software and lead on LPGPU2 project. Currently editor for Khronos Safety Critical Advisory Panel working group. He holds a Bsc (Hons) from the University of Surrey in Electrical and Electronic Engineering.
\paragraph{Meenakshi Ravindran} (female) Compiler developer working on the parallelization technology for CODEs OpenCL implementation of ComputeAorta. 

\subsubsection*{List of Publications}

\begin{itemize}

\item Mehdi Goli, Luke Iwanski, Andrew Richards:
\textit{Accelerated Machine Learning Using TensorFlow and SYCL on OpenCL Devices.} IWOCL 2017: 8:1-8:4
\item Jos Ignacio Aliaga, Ruymn Reyes, Mehdi Goli:
\textit{SYCL-BLAS: Combining Expression Trees and Kernel Fusion on Heterogeneous Systems.} PARCO 2017: 349-358
\item Mehdi Goli, Horacio Gonzlez-Vlez:
\textit{Formalised Composition and Interaction for Heterogeneous Structured Parallelism.} International Journal of Parallel Programming 46(1): 120-151 (2018)

\item Mehdi Goli, Horacio Gonzlez-Vlez:
\textit{Autonomic Coordination of Skeleton-Based Applications Over CPU/GPU Multi-Core Architectures.} International Journal of Parallel Programming 45(2): 203-224 (2017)

% \item Jos Ignacio Aliaga, Ruymn Reyes, Mehdi Goli:
% \textit{SYCL-BLAS: Combining Expression Trees and Kernel Fusion on Heterogeneous Systems.} PARCO 2017: 349-358

\item Sonia Campa, Marco Danelutto, Mehdi Goli, Horacio Gonzlez-Vlez, Alina Madalina Popescu, Massimo Torquati:
\textit{ Parallel patterns for heterogeneous CPU/GPU architectures: Structured parallelism from cluster to cloud.} Future Generation Comp. Syst. 37: 354-366 (2014)

\end{itemize}

\subsubsection*{Relevant Research Projects}

\begin{itemize}
\item
CARP: Correct and Efficient Accelerator Programming (ICT-2011.3.4-287767, 2011-2014, http://carp.doc.ic.ac.uk/external/);
\item
LPGPU: Low-power GPU (ICT-2011.3.4-288653,  2011-2014, http://lpgpu.org/wp/, LPGPU2 (ICT-04-2015-688759  2016-2018) );
\item
PEPPHER: Performance Portability and Programmability for Heterogeneous Many-core Architectures (ICT-2009.3.6-248481, 2010-2013, http://www.peppher.eu/);
\end{itemize}


\subsubsection*{Products and services}

\begin{itemize}
\item
\textbf{ComputeAorta}: enables delivery of OpenCL, SPIR, HSA, and Vulkan
\item
\textbf{ComputeCpp}: enables easy integration of C++ applications into complex heterogeneous compute systems with SYCL
\item
\textbf{ComputeSuitefor Application Developers}: enables high-performance development with open standards
\item
\textbf{ComputeSuite for Automotive}: open standards for safety-critical solutions
\item
\textbf{ComputeSuite for Hardware Vendors}: integrate the power of a System-on-Chip
\end{itemize}


\subsubsection*{Infrastructure}
The \CODEPLAYlong{} business is profitable and has about 60 full-time staff
based in Edinburgh, UK. \CODEPLAYshort{} is a member of The Khronos Group,
participating in the specification of both OpenCL and SYCL from the start
and contributing to the OpenVX and Safety Critical working groups; a
member of the Multicore Association, being involved in the MCAPI and
MRAPI standards process; a member of the HSA Foundation, chairing the
System Run-time and Tools working groups; and a member of ISO C/C++
Standard, chairing SG14 Low Latency, and participating in SG12 Undefined
Behavior in C++ , and starting a new SG on Safety Security for C and
C++. \CODEPLAYlong{} also participates in WG23 Programming Language
Vulerabilities and MISRA.  The company also has a series of academic
collaborations, including sponsoring PhD students at the University of
Glasgow and Imperial College London, an EngD student at the University of
Bath, and being involved in three FP7 and one H2020 EU
collaborations. \CODEPLAYshort{} has now developed its own licensable implementations
of the OpenCL and SYCL for OpenCL standards, and has expanded into other
areas of heterogeneous systems such as debuggers and test suites.


\Participant{GOLEM}{(http://golem.at)}

\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/golem-logo.jpg}
\vspace{-1cm}
\end{wrapfigure}

\ 

\GOLEMlong, Vienna, Austria, is SME actively researching, developing and
innovating (RDI) the novel generation of IoT platform PharosN capable to
identify and control sustainable operation of complex cyber-physical
systems (CPS). The company implemented Smart City Monitor as PharosN
application running the urban CPS model linking physical and digital
worlds and enabling advanced digital transformation of multiple
heterogeneous data streams into rich set of smart custom applications and
controls supporting management of a city as big CPS. The RDI addresses
growing social and business demand for smart management and governance
assisted by AI-driven ICT / IoT tools providing holistic, user friendly
vision of the increasingly complex processes in enterprises and urban
areas, its results, performance, quality and sustainability. The company
provides the development platform for wide range of customised solutions
and customer groups such as Smart Future Cities and Communities by Smart
City Monitor, Smart factories of future and Industry 4.0 by Smart
Enterprise Monitor, Smart connected assets, Sustainable environment,
energy, transportation, water, waste, health, other industries as well as
their nexus and relevant education in smart everything and IoT.  The
company is active member of existing EU RDI initiatives such EIP for
Smart Cities and Communities, Focus Group for new standard for Data
Processing Management for Smart Cities and IoT by ITU (Geneva) and new
eLearning platforms for IoT. As partner in consortium, the company
supports RDI, experimental studies, practical TRL prototyping and novel
digital services for industrial and city stakeholders enabling digital
transformation of big data streams in customised information services and
controls assisting in smart management and governance, monitoring,
analytics, benchmarking and simulation of change processes for optional
responses.  As the official partner of United Nations Industrial
Development Organization ({UNIDO}), \GOLEMshort{} supports PharosN platform
applications for sustainable inclusive development in green metropolitan
areas, industries and circular economy in UN projects.

\vspace{10pt}
\textbf{\GOLEMshort{} leads WP7 on Requirements, Use Cases and Evaluation, and contributes their expertise on smart cities and the Internet-of-Things
to WP2-WP6 and WP8.}
\vspace{10pt}

\subsubsection*{Key Personnel Involved in the Project}

\paragraph{Serguei Golovanov} is General Manager of \GOLEMlong, is internationally recognized expert and supervisor of all company RDI ICT projects, Chief architect of the PharosN IoT platform for modelling of complex cyber-physical systems. He has in depth knowledge in advanced systems sciences, design and modelling of complex cyber-physical systems, information technology and computer architectures, measurements and metrology, environmental studies, engineering and business development. Much of this international collaboration experience had been accumulated during his former work as senior scientist of International Institute for Applied Systems Analysis (IIASA) and UNIDO international consultant. He is active presenter of PharosN applications and and keynote speaker in many international conferences, workshops and training courses worldwide. He has engineering diploma from Moscow Aviation Institute and Ph.D. in Systems and Computer Sciences by Institute for System Analysis by Russian Academy of Sciences (ISA RAS, 1983).

\paragraph{Alexander Ostrovsky} Dipl. Engineer, PhD in Computer-aided design in Industrial applications, is Team Leader for research and developments. He has deep professional knowledge of ICT architectures, databases and experience of design, development and implementation of the complex software projects since 2002. He coordinated and actively participated in the successful implementation of the five distributed enterprise information systems and had managed software developer teams since 2009. He was engaged in research in the field of data mining, clustering and fuzzy logic has 14 scientific publications and PhD thesis on "Fuzzy clustering of information resources in the computer-aided software repository (2010).

\paragraph{Sergey Pikuz} PhD and MSc in Physics, is the lead researcher for the advanced smart measurement methods and sensoring instrumentation coupled with a novel IT-platforms to study and control of new energy and environmental systems. He is experienced in the development, testing and implementation of precise diagnostics equipment for basic science and technology. He participates in a wide international collaboration of most advanced research institutions from EU, USA, Japan and Russia aimed to study and develop novel concepts in energy generation, distribution and laser technology applications and measurement methods. He is a representative at several European-based research associations and users' consortiums at large-scale EU research infrastructure facilities such as COST initiative for Inertial Confinement Fusion, European XFEL HIBEF consortium, European Task Force for Laboratory Astrophysics. He has been involved in the company platform RDI since 2011 and took active part in the concept and methodology development.

\pagebreak
\subsubsection*{Relevant Publications}

\begin{itemize}
\item Golovanov S. and Petrova E., \emph{Final report for public dissemination: Smart Urbana project Enabling municipalities with CPS instruments and business models for digital transformation of real time data about urban processes into digital services for the community, grant agreement within the EU-funded project Cyber-Physical Systems Engineering Labs under Grant Agreement No. 644 400, (2017)}.  
\item Golovanov, S. Ostorovsky A., \emph{The PharosN concepts, methods, algorithms and interactive ICT solutions for modelling complex cyber-physical systems consisting of multiple interlinked smart agents/components for digital transformation of big data streams, 2014-2017 (Set of research documents, studies, technical specifications and algorithms for experimental prototypes)}.
\item Golovanov. S. and Petrova. E., \emph{Experimental model and online demo prototype of urban and metropolitan areas as complex cyber-physical systems for Smart City Monitor applications in compliance with ISO 37120:2014 specifications} (2015), published at \url{http://smartcity.pharosnavigator.com}.
\item Golovanov. S. and Petrova. E., \emph{Experimental model and online demo of Smart Manufacturing Enterprise as complex cyber-physical system for Factory of Future and Industry 4.0 experiments and prototyping in Smart Enterprise Monitor applications} (2014), published at \url{http://enterprise.pharosnavigator.com}
\item Golovanov S. and Petrova. E., \emph{Pharos Navigator: The novel digital transformation and knowledge management platform for modelling and managing complex cyber-physical systems - The architecture, design, operational principles, technical specifications, user reference and training materials"}, large set of publications and online demo models at http://pharosnavigator.com (2014-2018)   
\end{itemize}

\subsubsection*{Relevant Research Projects}

\begin{itemize}
\item
{Smart Urbana} project ``Enabling municipalities with CPS instruments and business models for digital transformation of real time data about urban processes into digital services for the community'', grant agreement within the EU-funded project Cyber-Physical Systems Engineering Labs under Grant Agreement No. 644 400, (2017) 

\item
Pharos Navigator\textregistered, PharosN: The novel digital transformation and knowledge management platform for modelling and managing complex cyber-physical systems: The research and development of the concepts, methodology, system architecture, design, algorithms, technical specifications, and applications ``Smart Enterprise Monitor'' and ``Smart City Monitor'' (implemented by GOLEM IMS GMBH without public funding during 2010-2018).  
\end{itemize}

\Participant{AGH}{(http://www.agh.edu.pl)}

\begin{wrapfigure}{R}{2.5cm}
\vspace{-2cm}
\hfill \includeimage[width=2cm]{logos/agh.jpg}
\vspace{-0.4cm}
\end{wrapfigure} 

\AGHlong, (Akademia Grniczo-Hutnicza) located in a historical city of Krakow, former capital of Poland, is one of the most renown Polish technical universities. AGH UST employs 1940 researchers and 1960 technological and administrative employees.

At AGH UST, 37,000 students are enrolled on different kinds of studying programmes on one of 16 faculties namely: intramural, extramural, doctoral and post-graduate, offering broad profile of education adapter to the current trends on the market (starting from classic ones as Mining, Metallurgy or Drilling, also Ceramics or Casting and finishing with novel ones as Information Technology, Electronics or Telecommunications). Excellent teaching employees and infrastructure, novel laboratories and equipment, broad cooperation with the industry and innovative business companies and constant care for the teaching quality makes the AGH UST for over 100 years one of the leading technical universities in Poland. 

In AGH UST, Academic Computing Centre CYFRONET AGH is operational, having at its own disposal supercomputers ranked well at TOP500 list, consisting of the most powerful ones in the world. This equipment in connection with generally acclaimed qualifications of the computer scientists from AGH UST are the reason that the University is perceived as one of the best in Poland and one in better ones in Europe, educating IT and research specialists. Currently, striving in the direction of the information society, a very important asset is the fact, that at the AGH UST there is locate one of the most important nodes of the Polish part of worldwide Internet network. Cyfronet participates also in the world computing grid LHC-Computing Grid and currently is in the course of organising of the computing grid for energetic purposes. Since 27th of April 2015, one of the most advanced HP-built supercomputer, Prometheus, being one of biggest and first of such installations, cooled directly by water, is available for free for the Polish scientists and their collaborators. 

\vspace{10pt}
\textbf{\AGHshort{} contributes to WP3, WP4, WP7 and WP8 with expertise in software engineering, distributed systems, high performance computing and the use of Erlang.}
\vspace{10pt}


\subsubsection*{Key Personnel Involved in the Project}

\paragraph{Dr Aleksander Byrski} will be the Principal Investigator at AGH. He works as associate professor and Deputy Head of the Department of Computer Science, Faculty of Computer Science, Electronics and Telecommunications, AGH University of Science and Technology. His area of research include metaheuristic computing, parallel and distributed computing and simulation in particular including multi-agent systems. He has lead several research and development projects (e.g. ParaPhrase - as AGH Principal Investigator) and participated in many others. He is an author of over 100 scientific publications including one internationally available monograph (printed by Springer). He participates actively in international collaboration by co-leading an international networking project connecting researchers from AGH and Catholique Universite de Louvain in Louvain-la-Neuve, Belgium and in participating in Erasmus+ exchanges. He worked as a guest editor of several special sections of renown journal (e.g. Future Generation Computer Science by Elsevier). He participates in Programme Committees of many international conferences. He supervised over 30 M.Sc. and B.Sc theses and two PhD theses and reviewed 5 PhD theses (including two international reviews and participation in an international PhD committee).  He actively reviews papers in international periodics.

\paragraph{Dr Wojciech Turek} works as assistant professor at the Department of Computer Science, Faculty of Computer Science, Electronics and Telecommunications, AGH University of Science and Technology. His area of interests include multi-robot systems, parallel and distributed simulations and computing, functional programming languages, transport systems. He has lead several research and development projects and was one of the most influential researchers in ParaPhrase project, he participated in many others scientific and development projects. He authored over 50 scientific publications. He actively participates in international collaboration according to Erasmus+ programme. He worked as a guest editor of several special sections of renown journal (e.g. Computing and Informatics published by Slovak Academy of Sciences). He participates in Programme Committees of many international conferences. He supervised over 20 B.Sc. and M.Sc theses. He actively reviews papers in international periodics.

\pagebreak
\paragraph{Dr Marek Kisiel-Dorohinicki} works as associate professor and Head of the Department of Computer Science, Faculty of Computer Science, Electronics and Telecommunications, AGH University of Science and Technology. His area of research include metaheuristic computing, software engineering, cybersecurity and hybrid computing. He leads a significant number of research and development projects mostly focused on criminal analysis and supporting of Polish and international security agencies such as Police, Border Guard and others, he also participated in many others scientific and development projects. He authored over 150 scientific papers including one internationally available monograph (printed by Springer). He actively participates in international collaboration according to Erasmus+ programme. He worked as a guest editor of several special sections of renown journal (e.g. Computing and Informatics published by Slovak Academy of Sciences). He participates in Programme Committees of many international conferences. He supervised over 50 M.Sc. and B.Sc theses and one PhD theses. He actively reviews papers in international periodics.

\subsubsection*{Relevant Publications}

\begin{itemize}
\item Wojciech Turek \emph{Erlang-based desynchronized urban traffic simulation for high-performance computing systems}, Future Generation Computer Systems, 2018 vol. 79 pp. 645--652.

\item Aleksander Byrski, Marek Kisiel-Dorohinicki \emph{Evolutionary Multi-Agent Systems : from inspirations to applications}, Springer International Publishing AG,  2017, (Studies in Computational Intelligence vol. 680).

\item Daniel Krzywicki, Wojciech Turek, Aleksander Byrski, Marek Kisiel-Dorohinicki \emph{Massively concurrent agent-based evolutionary computing}, Journal of Computational Science, 2015 vol. 11, pp. 153--162.

\item Wojciech Turek, Leszek Siwik, Aleksander Byrski \emph{Leveraging rapid simulation and analysis of large urban road systems on HPC}, Transportation Research. Part C, Emerging Technologies, 2018 vol. 87, pp. 46--57.

\item Daniel Krzywicki, Lukasz Faber, Aleksander Byrski, Marek Kisiel-Dorohinicki \emph{Computing agents for decision support systems}, Future Generation Computer Systems, 2014 vol. 37, pp. 390--400.
\end{itemize}

\subsubsection*{Relevant Research Projects}

\begin{itemize}
\item
Parallel Patterns for Adaptive Heterogeneous Multicore Systems (ParaPhrase, ICT-288570, 2011-2014, \url{http://paraphrase-ict.eu});

\item 
Integrating Distributed Data Infrastructures for Global Exploitation (INDIGO-DataCloud, RIA 653549, 2015-2017, \url{http://www.indigo-datacloud.eu});

\item 
Domain-oriented services and resources of Polish Infrastructure for Supporting Computational Science in the European Research Space (PLGrid Plus,  POIG.02.03.00-00-096/10, 2011-2015, \url{http://www.plgrid.pl/en/projects/plus});

\item
PROviding Computing solutions for ExaScale ChallengeS (PROCESS, 2017-2020, \url{http://dice.cyfronet.pl/projects/details/Process});

\item
European Plate Observing System (EPOS, 2014-2019, \url{https://www.epos-ip.org/about/epos-implementation-phase});







\end{itemize}


\subsubsection*{Infrastructure}
\begin{itemize}
\item Prometheus - Computing cluster - operating system: Linux CentOS 7, configuration: 2232 servers (HP Apollo 8000), processors: Intel Xeon (Haswell), RAM: 279 TB, storage: 10 PB, computing power: 2399 TFlops. Includes 144 Nvidia K40XL accelerators
\item Computing cluster  Zeus, operating system: Scientific Linux 5, configuration: HP BL2x220c, processors: Intel Xeon, RAM: 23 TB, computing power: 120 TFlops
Computing cluster  Zeus BigMem, operating system: Scientific Linux 6, configuration: HP BL685c, processors: AMD Opteron, RAM: 26 TB, computing power: 61 TFlops 
\item Zeus GPGPU - operating system: Scientific Linux 5, configuration: HP SL390s, processors: Intel Xeon, RAM: 3,6 TB, computing power: 136 TFlops, 2 cards NVidia Tesla M2050, 8 cards NVidia Tesla M2090
\item Zeus FPGA, operating system: Ubuntu, configuration: 2 x M-503 (Virtex-6 LX240T FPGA), processors: Intel i7, RAM: 12 GB, computing resources: 241 152 FPGA Logic Cells, 768 DSP48 Slices, 14 976 kbits Block RAM
\end{itemize}

\Participant{JMOIC}{(http://www.jelgava.lv)}

\begin{wrapfigure}{R}{2.2cm}
\vspace{-2cm}
\hfill \includeimage[width=2cm]{logos/JMOIC.jpg}
\vspace{-0.7cm}
\end{wrapfigure}

Jelgava City Municipality Institution ``Jelgava Municipality Operative Information Centre'' (\JMOICshort), was launched on 1 February 2016.  JMOIC is an operational and a strategical heart of Jelgava city providing information services to citizens, public utility providers and city administration for 24 hours a day, seven days a week. Any inhabitant of Jelgava can make a request for assistance by the telephone support. The main functions of Centre are to monitor the city public area and critical infrastructure using leading ITC solutions. It includes monitoring of the city public area, different kinds of infrastructural objects (street lights; pumping stations; meteorological stations; intelligent traffic light management, smart parking, fire, energy efficiency, schools, public security and buildings, etc), municipality infrastructure maintenance, measures for risk prevention and mobilising operational support for civil protection assistance in the event of major disasters (natural and man-made disasters, acts of terrorism and, technological, environmental accidents etc.) in the whole Jelgava region including peri-urban areas. 
JMOIC functioning mechanism is mainly supported by the phone contact centre (line 8787), different communication and information systems (video surveillance system, geographic information system, Smart City Monitor software, etc.), the databases (flood maps, road traffics accident map, drone imagines, etc.). It supports the work of Joint Civil Protection Commission for Jelgava Municipality and city development planning and infrastructure maintenance processes by identifying people needs and accumulating different data.

\JMOICshort{} applies leading European ICT/IoT solution Smart City Monitor for obtaining quantifiable holistic information about urban processes and its sustainability assessment in real time to support smart sustainable governance. The Jelgava Smart City project implementation requires connecting large number of diverse urban data sources both from smart sensors (energy-water-traffic meters, temperature, humidity, CO2, etc) as well as databases and spreadsheet files. The secure and reliable processing and transformation of this data into trustful real time information for decision makers and JPOIC operators are of key importance for prompt and preventive actions.   


\bigskip
\subsubsection*{Key personnel involved in the project}

\paragraph{Gints Reinsons} is a Head of the Jelgava City Municipality Institution (\JMOIClong) and also member of the Joint Civil Protection Commission for Jelgava Municipality. He was involved in the multiple cross-boarder projects such as the ECHO, THERMOS, BaltPrevResilience, Liquidation of Ecological Catastrophes and Pollution in the Territory of the Lielupe River Basin within the Latvia-Lithonia cross border cooperation program. Gints works with local level civil protection activities, including accident data analysis, prevention, response, and recovery. He has a Bachelor degree in economics.

\bigskip
\subsubsection*{Role in \TheProject{}}

\textbf{\JMOIC{} contributes to piloting of its smart city use case, including data and confidentiality and privacy requirements in \ref{wp:eval} and dissemination, Exploitation, Community building and Communications in WP8.}

\subsubsection*{Previous projects and activities}

Under the support of the European Union the JMOIC took part in numerous projects, including:

\begin{itemize}
\item	\textbf{Thermal Energy Resource Modelling and Optimisation System (THERMOS)} (H2020-EE-2016-RIA ID 723636). The project objective to develop the methods, data, and tools to enable public authorities and other stakeholders to undertake more sophisticated thermal energy system planning far more rapidly and cheaply than they can today. This will amplify and accelerate the development of new low carbon heating and cooling systems across Europe, and enable faster upgrade, refurbishment and expansion of existing systems.
  The project will realise these benefits at the strategic planning level (quantification of technical potential, identification of new opportunities) and at the project level (optimisation of management and extension of existing and new systems).
  
\item \textbf{The BaltPrevResilience project} (2014--2016) which were aimed to prevent and reduce the consequences of these accidents by creating a mutual platform for sharing within BSR and between local, national and EU levels of statistics, experiences and best practices and methodology for learning experiences and lessons. Three thematic seminars were arranged, each seminar was prepared with studies on the themes collection of evidence based knowledge, assessment of information and data and awareness raising and building resilience.

\pagebreak
\item \textbf{Liquidation of Ecological Catastrophes and Pollution in the Territory of the Lielupe River Basin} (2012--2013). Chemicals and oil products, which are transported by rail, highways and oil pipelines, create a potential threat to the quality of the environment in the territory of the Lielupe Basin. The project aimed to create a joint rescue team equipped for the prevention and elimination of environmental pollution to quickly respond to ecological catastrophes. Thought the project the early warning system was created. By participating in specially organized seminars, students and residents of high risk territories could find out how to respond in the case of an ecological catastrophe.

\item \textbf{Creating Flood Emergency Response Team in Latvia and Lithuania Cross Border Region} (2011--2013). Floods have provided unpleasant surprises in the Latvian and Lithuanian border region. The response from Jelgava and Siauliai is the development of a joint 30 person team, which is now ready to battle with the consequences of floods on both sides of the border. Joint theoretical and practical training, exchanges of experience in other EU countries, as well as effective equipment for pumping water allows a more effective response to floods, uniting the resources of the emergency services and local councils of both nations.
\end{itemize}

\newpage
\subsection{Third parties involved in the project (including use of third party resources)}

No third parties are involved in the project.

% ---------------------------------------------------------------------------
%  Section 5: Ethics and Security
% ---------------------------------------------------------------------------

\newpage

\section{Ethics and Security}

\subsection{Ethics}

The proposal raises no specific ethical concerns.

The Parties agree that any Background, Results, Confidential Information
and/or any and all data and/or information that is provided, disclosed or
otherwise made available between the Parties during the implementation of
the Action and/or for any Exploitation activities (``Shared Information''),
shall not include personal data as defined by Article 2, Section (a) of
the Data Protection Directive (95/46/EEC) and applicable local
implementing local legislation; or, as from May, 25th 2018, Article 4 of
the General Data Protection Regulation. The Data Protection Directive,
its implementing local legislation and the General Data Protection
Regulation are hereinafter collectively referred to as the Data
Protection Legislation.  Accordingly, each Party will ensure that all
data and information contained in Shared Information is anonymised such
that it is no longer personal data, prior to providing the Shared
Information to such other Parties. Each Party who provides or otherwise
makes Shared Information available to any other Party, (``Contributor'')
represents that, as per applicable Data Protection Legislation: (i) it
has the authority to disclose the Shared Information, if any, which it
provides to the Parties under this CA; (ii) where legally required and
relevant, it has a legal ground to provide the Shared Information; and
(iii) there is no restriction in place that would prevent any such other
Party from using the Shared Information for the purpose of this Action
and the exploitation thereof.

\subsection{Security}

Please indicate if your proposal involves:

\begin{itemize}
\item
activities or results raising security issues: NO
\item
'EU-classified information' as background or results: NO
\end{itemize}

%% Write macro to split Sections 4-5
\Split{4-5}

%% Finalise batch file
\immediate\write\BatchFile{exit}% 
\immediate\closeout\BatchFile% 

\newpage

\label{bibliography}
\addcontentsline{toc}{section}{References}

%\bibliographystyle{abbrv}
%\bibliography{bibliography_ustan}
%\bibliography{bibliography_scch}

\end{document}
