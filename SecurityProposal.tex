     % !TeX encoding = UTF-8
\documentclass[a4paper,11pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{IEEEtrantools}
\usepackage[normalem]{ulem}
\newcommand{\project}[1]{\textbf{#1}\xspace}
\newcommand{\SECURITY}{\project{Elysian}}
\newcommand{\TheProject}{\SECURITY}

%\ifdefined\final
%\else
%\newcommand{\final}{}
%\fi

\def \final {}

\input{preamble}
\input{participants}



\begin{document}
\pagenumbering{arabic} % for pageslts

\begin{titlepage}

\begin{center}
{\Huge \textsc{\TheProject}}
\end{center}

\begin{tabular}{lp{5in}r} %something strange about the spacing...
\textbf{Title of Proposal:}&\hspace*{-7cm}\textbf{Intelligent Security and Privacy for AI-Based Big Data Analytics } & \\[4ex] 
\textbf{Date of preparation:} &\hspace*{-3cm} \textbf{\today} & \comment{}{$
$Revision: 0.0$ $}\\[4ex]
\textbf{List of participants} & & \\[-1ex]

{{\textcolor{white}{https://www.overleaf.com/project/5e5e45121e493b000149fe20}}}
\end{tabular}

%% Participants Table
\newcounter{p}
\begin{center}
\begin{tabular}{|l|p{5in}|l|l|}\hline
\textbf{Participant no} & \textbf{Participant organisation name} & \textbf{Country}\\ \hline 
1 (Coordinator) & {\sc \longparticipant{1}} \hfill (\shortparticipant{1}) & \country{1}  \\ \hline
\forloop{p}{2}{\value{p} < \theparticipant}{%
\thep & {\sc \longparticipant{\thep}} \hfill  (\shortparticipant{\thep}) & \country{\thep}  \\ \hline}%
\theparticipant & {\sc \longparticipant{\theparticipant}} \hfill  (\shortparticipant{\theparticipant})& \country{\theparticipant}  \\ \hline
\end{tabular}\end{center}
%https://www.overleaf.com/project/5e5e45121e493b000149fe20

\tableofcontents

\end{titlepage}

% \input{snags}
\newpage


%\pagenumbering{roman}

% ---------------------------------------------------------------------------    
%  Section 1: Excellence
% ---------------------------------------------------------------------------

%\pagebreak
%
%ToDo List:
%
%\begin{itemize}
%
%\item \textbf{Section 1}
%\begin{itemize} 
%\item Ensure consistent spelling throughout (e.g., optimisation vs optimization, words with - and without, etc
%\item 1.3.1 : Add challenges for security and privacy of big data analytics (basically, a few sentences to motivate why we do the work we propose)
%\begin{itemize}
%\item \sout{USTAN for formal modelling and refactoring}
%
%\end{itemize}
%
%\item 1.3.3 : Add the relevant technologies that will be developed over the course of the project, together with current and future TRLs
%\begin{itemize}
%\item \sout{USTAN for formal methods and refactoring}
%\end{itemize}
%
%\item 1.3.4: Add how this project goes beyond what is done in the previous ones (EVERYONE)
%
%\item 1.4 : Add the related work and advancement beyond state-of-the-art:
%\begin{itemize}
%\item USTAN for formal methods in 1.4.4, including security contracts
%\end{itemize}
%
%	
%\item 1.4.12 : Write Innovation Potential (UOD)
%\begin{itemize}
%\item Add a short description of the innovation potential for industrial partners (IBM, YAG, COGNI, SOPRA, FRQ)
%\end{itemize}
%\end{itemize}
%
%\item \textbf{Section 2}
%\begin{itemize}
%\item 2.1 : Finish writing up the Impacts 
%\begin{itemize}
%\item \sout{Write an explanation of what we are doing to address the short-temp impact nr 7 (COGNI)}
%\item Think about whether and how we are addressing the long term impact nr 2 (related to trust) (EVERYONE)
%\end{itemize}
%
%\item 2.1 : Write up Barriers for Impact (UOD)
%
%\item 2.1 : Finish the gender balance box (\sout{SCCH} and EVERYONE) \textcolor{red}{Where is this???}
%
%\item 2.3.1 : Add security, ML, refactoring and formal methods conferences and journals (EVERYONE)
%
%\item 2.3.1 : Add membership to the standardisation committees and general scientific and technical community (EVERYONE)
%
%\item 2.3.2: Write up Draft Exploitation Plans:
%\begin{itemize}
%\item \sout{COGNI Exploitation Plan}
%\item \textcolor{red}{Should be given in the order of participant number}
%\end{itemize}
%
%\item 2.3.2 : Write a joint exploitation plan (EVERYONE)
%
%\item 2.3.3 : Write up KPIs for Dissemination and Communication (USTAN and UC3M)
%\end{itemize}
%
%\item \textbf{Section 3}
%
%\begin{itemize}
%\item Check that the references at the end of this section are properly formatted. Webpages seem to have incomplete details! (EVERYONE)
%
%\item Write up WP5, make tasks consistent with the Gantt chart (COGNI)
%
%\item Write up WP6, make tasks consistent with the Gantt chart (UC3M)
%
%\item Write up WP7, make tasks consistent with the Gantt chart (SOPRA)
%
%\item Write up critical risks table (EVERYONE to provide 1-2 critical risks and mitigations related to their technologies)
%\begin{itemize}
%\item Think about political critical risks (Brexit, COVID, closures of universities/companies, currency exchange rate)
%\item Make sure to specify that in the case of disagreement, EU law and court of Brussels will be in charge
%\item Consider risks about tooling not being available
%\end{itemize}
%
%\item Think about people to add to the Advisory Board and contact some of them (EVERYONE)
%
%\item 3.4: Write the table for staff effort
%
%\end{itemize}
%
%\item \textbf{Section 4}
%\begin{itemize}
%\item 4.1 \textcolor{red}{Check that the descriptions are given consistently}
%
%\item \sout{4.1 Write up USTAN description (USTAN)}
%
%\item 4.1: \textcolor{red}{Write up UC3M description (UC3M)}
%
%\item 4.1: Write up UOD description (UOD)
%
%\item \sout{Make sure participants are listed in the same order as the table upfront!}
%\end{itemize}
%
%\item \textbf{Section 5}
%\begin{itemize}
%\item Think about security and ethics considerations (EVERYONE)
%\end{itemize}
%\end{itemize}
%
%\newpage
%%DONE:
%%
%%\begin{itemize}
%%\item Section 1.3.1 : Add challenges for security and privacy of big data analytics (basically, a few sentences to motivate why we do the work we propose)
%%\begin{itemize}
%%\item UC3M for security standards
%%\item COGNI for authentication
%%
%%\end{itemize}
%%
%%\item Section 1.3.3 : Add the relevant technologies that will be developed over the course of the project, together with current and future TRLs
%%\begin{itemize}
%%\item SCCH for privacy-preserving AI
%%\item YAG for source code analysis
%%\end{itemize}
%%
%%\item Section 1.4 : Add the related work and advancement beyond state-of-the-art:
%%\begin{itemize}
%%\item YAG for static code analysis in 1.4.1 (maybe add a picture of the tool in action too?)
%%\item SCCH for runtime analysis in 1.4.2. 
%%\item COGNI for authentication in 1.4.6
%%\end{itemize}
%%
%%\item Section 2.3.2: Write up exploitation plans
%%\begin{itemize}
%%\item SCCH Exploitation Plan
%%\end{itemize}
%%
%%
%%\begin{itemize}
%%\item Section 3: Write up WP3, make tasks consistent with the Gantt chart (UOD)
%%
%%\item Section 3: Make changes to the list of milestones and deliverables (UOD):
%%\begin{itemize}
%%\item Merge Mi6 and Mi7
%%\item Move D2.3, D3.3 and D4.3 to Mi12
%%\item Make some deliverables software instead of reports
%%\item Assign D7.2 and D7.3 to Sopra
%%\item Give leadership to some WP2 deliverables to YAG
%%\item Give leadership to some WP4 deliverables to SCCH
%%\item Add the Data Management Plan as the deliverable (M6) 
%%\item MOOC deliverable and milestoenes at M22 
%%\item Add milestones for Data Management Plan
%%\end{itemize}
%%
%%\item Section 2.3.2: Write up Draft Exploitation Plans:
%%\begin{itemize}
%%\item UOD Exploitation Plan
%%\end{itemize}
%%
%%\item Section 3.2: Write up tables of WTLs and PIs (UOD) -- EVERYONE TO CHECK
%%
%%\item Section 1.3.1 : Add challenges for security and privacy of big data analytics (basically, a few sentences to motivate why we do the work we propose)
%%
%%\begin{itemize}
%%\item IBM for symbolic execution
%%\end{itemize}
%%
%%\item Section 1.3.3 : Add the relevant technologies that will be developed over the course of the project, together with current and future TRLs
%%\begin{itemize}
%%\item IBM for ExpliSat
%%\end{itemize}
%%
%%\item Section 3.3: Draw a table of expertise (UOD)
%%
%%\end{itemize}
%
%
%
%
%\pagenumbering{arabic}
\setcounter{page}{2}

%%\subsection{Contributions from the Partners}

%%\subsubsection{University of St Andrews - Security Contracts and Verification}
%%\begin{itemize}
%%    \item Formal specification of security contracts for pieces of code
%%    \item Refactoring to introduce secure code in the applications
%%    \item Formal verification of properties of the code with regard to security contracts
%%    \item Automated verification and reasoning (model checkers, constraint solvers and theorem provers, sometimes used in combination), logics including distributed temporal logics 
%%\end{itemize}

%%\subsubsection{Dundee - Security for Big Data}
%%\begin{itemize}
%%    \item Identifying and addressing security risks in large-scale distributed databases, both open- and closed-source ones.
%%    \item Identifying and analysing security risks in distributed parallel processing.
%%    \item Security for machine-learning based big data analysis
%%    \item Security contracts for distributed parallel code
%%    \item Dissemination
%%\end{itemize}

%%\subsubsection{IBM - Vulnerability Detection and Data Fabrication}
%%\begin{itemize}
%%\item  Vulnerability Detection
   
%%We propose to extend the ExpliSAT symbolic execution technology to discover known security vulnerability patterns in C/C++ code and combine it with the tailored fuzzing techniques in specific areas for the purpose of assisting the symbolic execution engine to consistently make progress and thus overcome a known "path explosion" problem of symbolic interpretation technologies.
 
%%\item Data Fabrication for ML
 
%%We have recently started to explore a new and very challenging direction of synthetic data fabrication for improving robustness of ML models and AI-based applications. It includes  use cases like data enrichment (missing or not sufficient training data), poisoned data (malicious data that is used as part of training data set to cause machine learning model malfunction) and evasion attack (malicious data that causes malfunction of a trained model).
%%Most of the recent research work in academia in this field is done based on "unstructured data" like images, video or text. We believe that for real industry use cases fubrication of structured data to test and improve robustness of ML models is more relevant. Our idea is to combine our rule-based (CSP-based) data fabrication approach with machine learning techniques to fabricate synthetic data for ML use cases.
 
%%\item Data Fabrication Platform
 
%%It is actually our current DFP product that is used in SERUMS. It can be mentioned and used in the new proposal in combination with any of the two "new" direction listed above.
%%\end{itemize}

%%\subsubsection{SCCH - Privacy-Preserving AI}
%%\begin{itemize}
%%    \item Informational Privacy
    
%%    To constrain the information leakage from a data set, we motivate an information theoretic approach to  privacy where privacy is quantified by the mutual information between sensitive private information and the released public data.
%%    \item Optimal Privacy Secured Data Release Mechanism 
    
%%    A data release mechanism aims to provide useful data available while simultaneously limiting any reveled sensitive information. The data perturbation approach uses a random noise adding mechanism to preserve privacy, however, results in distortion of useful data and thus utility of any subsequent machine learning and data analytic algorithm is adversely affected. We introduce a novel information theoretic approach for studying privacy-utility trade-off suitable for different data types (including high-dimensional data, signals, and images) and for the cases with unknown statistical distributions. 
    
%%    \item Privacy Secured Knowledge Sharing
    
%%   While sharing of knowledge extracted from a relatively large set of labelled data owned by an organization with another organization owning a few or no labelled data, it is intended that 
%%    \begin{itemize}
%%        \item privacy of data is preserved;
%%        \item transferability of knowledge from source to target domain is evaluated for the design and analysis of transfer learning algorithms;
%%        \item privacy-transferability trade-off is optimized. 
%%    \end{itemize} 
%%    We aim at the development of techniques and tools for the study and optimization of privacy and transferability aspects of machine learning based AI systems. 
%%\end{itemize}
%%\subsubsection{Cognitive UX - Intelligent User Authentication-as-a-Service}
%%\begin{itemize}
%%\item AI-driven and eye gaze-driven behavioral authentication (Topic C and D)

%%We can bring in multi-factor authentication solutions and more specifically AI-driven behavioral authentication by adding another layer of security in the authentication process to verify the end users based on their interaction behavior, and/or their eye gaze behavior. For doing so, we currently utilize and can also bring in the project a range of eye-tracking and wearable technologies.

%%\item Authentication-as-a-Service (Topic C and D)
%%We are interested to conduct research and extend our product (Cognitive Authentication), which offers an integrated user authentication solution that allows service providers to set their own password policies, authentication types, get insights from end users' interaction data, etc.

%%\item User experience and usability evaluations and activities
%%\item System integration and testing
%%\item Dissemination activities on Usable Security, HCI, Authentication, Eye-tracking, and Intelligent User Interfaces
%%\end{itemize}

%%\subsubsection{Sopra-Steria Limited}

%%As a commercial partner we want to start investigation into the following areas:
%%\begin{itemize}
%%    \item Finance (open banking for CMA9) as data source for a universal banking solution that supports a secure banking interaction with appropriate trust and privacy as expected. Want to test existing and future code for security vulnerabilities.
    
%%    \item Space-based Open Internet capabilities with low Earth orbit satellites using an open meshed network for communication. Investigate how Delay/Disruption Tolerant Networking (DTN) and Solar System Internet (SSI) will impact our communication security and enable citizen services like social care and healthcare. Want to include security by design into the code and indirectly the deployed systems.
    
%%    \item Distributed healthcare (IoT) to support remote healthcare and intervention via open network technology. Test and repair the IoT code for vulnerability at the edge of the network.
    
%%    \item Sovereign Identity Clearance House using a Self-sovereign identity (SSI) token for essential communication as a citizen with the citizen's digital twin.
%%\end{itemize}

%%We propose three business areas to research the develop of automated tools for validating the security and privacy of data processing code, underlying systems' code and exposed online services used to provision these services to citizens. We want to introduce a practical solution for zero-trust architecture of the system while preserving the ever-increasing use of open and common internet-based communication channels between businesses and citizens without losing the trust and security that each citizen wants.

%%Sopra-Steria Limited will supply an insight into the real-world case studies that we could use to prove the research is achieving the proposed outcomes.

%%We also hold various advance technical capability that can be used to support the general project to ensure data engineering, machine learning and data science skills are available for use by the projects research life-cycle.

%%\subsubsection{University Carlos III of Madrid}

%%\begin{itemize}
%%\item C++ standardization process with 12 years of membership in the ISO C++ standards committee.
%%\item Integration in the C++ programming language of contracts in the form of preconditions, post conditions, invariants, \ldots
%%\item Identification and avoidance of software vulnerabilities (security and safety) derived from programming language features 
%%\item Patterns for parallel programming.
%%\end{itemize}

%%\pagebreak


\section{Excellence}

\begin{figure}[tp]
  \begin{center}
  \vspace{-5mm}
  \includeimage[scale=0.75]{DigitalFortress-Vision-v3.png}
%  \vspace{-3cm}
  \caption{The \TheProject{} Vision}
  \label{fig:vision}
  \end{center}
  \end{figure}
  %% PLEASE DO NOT CHANGE THE ABSTRACT _ THANKS, Juliana
Today's technological advances are such that big data processing can be used to facilitate rapid scientific breakthroughs and increase the economic impact of many businesses irrespective of their size. The complexity and critical nature of many data intensive systems, however,
means that we are still far from reliably achieving such a potential and we need innovative solutions that can make a difference. The world has huge quantities and variety of
data available, and their analytics is necessarily distributed with both data and computations spanning large quantities of distributed resources, some of which are on  the cloud. 
Data analytics requires processing algorithms to prepare 
and clean raw data, and is further reliant on advanced AI techniques, such as deep learning, for
extracting knowledge from the data.
There are huge economic and societal repercussions associated to data breaches in the light of regulations such as GDPR, and hence a fundamental need to
ensure security and privacy of data analytics at all times.
There are known considerations on security and privacy in cloud computing~\cite{cloudSecurity}, and big data analytics adds two further dimensions through
concerns over data storage and processing~\cite{bigdatasecurity}, and potential information leakages from machine learning models~\cite{mlSecurity}.
In addition, data intensive applications are typically built from layers of different libraries, where each layer corresponds to a level of abstraction (e.g. data storage, data transfer, data processing) and has security and privacy issues on its own and with adjacent layers. This has been identified as a problem by the EU Agency for Network and Information Security\footnote{\url{https://www.enisa.europa.eu/publications/bigdata-threat-landscape/at_download/fullReport}}. Furthermore, a formal treatment of the security properties of the applications, despite some success in reasoning about correctness of cryptographic algorithms~\cite{FM-crypto} and in specifying and enforcing privacy guarantees~\cite{FM-priv-guarantee}, is still lacking. Consequently,
%All this makes 
establishing security and privacy properties of big data analytics remains 
a hard challenge which requires urgent attention, extensive research and industrial collaboration.



%Formal methods are the only currently-known approaches that 
%could provide strong end-to-end security guarantees throughout system execution and across levels of abstraction. They have indeed had significant success over the years  in tackling security related problems, such as reasoning about the correctness of cryptographic algorithms~\cite{FM-crypto} or specifying and enforcing privacy guarantees~\cite{FM-priv-guarantee}. 
%Nonetheless, establishing security and privacy properties of big data analytics remains a hard challenge which requires urgent attention, extensive research and industrial collaboration. % recently.
%Big-data offers a great potential both for scientific breakthroughs and for economic impact on businesses. With huge quantities of data available in the modern world, modern data analytics is necessarily distributed in nature (with data and computations spanning large quantities of distributed resources, usually on Clouds) and, for the most part, based on advanced AI techniques such as deep learning for extracting knowledge from raw data. In such setting, ensuring security and privacy of the analytics becomes a critical issue, due to huge economic and societal impacts of data breaches in the light of ever stricter regulations such as GDPR. In addition to the security and privacy issues that are present in Cloud Computing setting~\cite{cloudSecurity}, big data analytics presents numerous additional issues due to the way data is stored and processed~\cite{bigdataSecurity}, and machine learning adds still another layer of potential problems with potential information leakage from the learning models themselves~\cite{mlSecurity}. In addition, data analytics applications are typically built from layers of different libraries, each of which deals with one level of abstraction (data storage, data transfer, data processing) and each of which presents security issues both in its own and in combination with other layers. It is, therefore, not surprising that establishing security and privacy properties of big data analytics is extremely hard and an objective for extensive research and industrial effort recently.

%%"The world's most valuable resource is no longer oil, but data" is the now world-famous quote from The Economist\footnote{\url{https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data}} that highlights the importance of big data analytics in the modern world. Advances in the techniques for analysing the vast amount of data available today from a variety of sources are having %tremendous effects
%%significant impact in a number of areas. Businesses use data to provide more personalised services to customers and improve their operation, and researchers use 
%analytics techniques 
%%data exploration and analysis for scientific discovery made possible only due to %from
%to tackle 
%%the ever-growing and detailed datasets being collected. 
%coming from sensors and simulations. 
%%It is hence not surprising 
%,Due to this, 
%%that data analysts and statisticians are amongst the most desirable professions in 2019\footnote{\url{https://www.cnbc.com/2019/01/07/these-are-the-best-jobs-to-have-in-2019-according-to-us-news--world-report.html}}. Modern data analytics is necessarily distributed in nature and, for the most part, based on advanced AI techniques such as deep learning.  This 
%introduces
%presents 
%completely new challenges for establishing the security and privacy %properties 
%of the data itself and of the analytics techniques used for processing it. A highly-distributed world of AI-based big data analytics 
%is, however, also much more vulnerable and exposed to
%there are many more 
%potential targets for cyber attacks then code running on single machines. Vulnerabilities arise %coming 
%%both 
%from the way big data is stored (usually in distributed filesystems or databases) and is processed (with multiple distributed agents interacting and exchanging potentially sensitive information). Security mechanisms that protect sensitive data in such settings are still fragile, and not advanced enough to provide sufficient trust in the whole data analytics process. 

\begin{mdframed}[backgroundcolor=blue!5]
\emph{\TheProject's main goal is to significantly increase the trust in modern data analytics systems by 
supporting the development of secure and privacy-preserving distributed code used in these systems, and 
%assisting 
%software developers in developing secure and privacy-preserving distributed data analysis code,
offering a combination of %combining 
tools for identifying and repairing security vulnerabilities with rigorous formal methods for verification of security and data privacy properties of the code (Figure~\ref{fig:vision}).}
\end{mdframed}


%% VJ: Add this somewhere else
%\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{Note: Security in AI vs. AI for Security}
%The main target of the \TheProject{} project are AI-based big data analytics applications, where large volumes of data are analysed and the prediction machine-learning models are built. This is reflected in the use cases that we consider, each of which is an application of advanced machine-learning techniques to analysing distributed datasets. In this respect, \TheProject{} aims to develop and apply security and privacy vulnerabilities detection and repairing to the problems from the AI domain. At the same time, many of the technologies that we will develop for vulnerabilities detection and repairing will themselves use advanced machine learning techniques. For example, static analysis for vulnerabilities detection will use XX while the framework for reducing privacy-leakage will be based on XX. So, in the \TheProject{}, we are truly combining 'Security for AI' and 'AI for Security'.
%\end{mdframed}
%%



%The emergence of big data, supported by distributed machine-learning based data analytics tools and techniques, presents new and unforeseen challenges for data security and privacy. Security vulnerabilities can come both from the individual components of the distributed data-analytics systems, as well as from the undeterministic way in which these components may interact. The Digital Fortress project aims to develop a novel methodology and the associated tool-chain for implementing secure distributed data processing applications. We will tackle the problem both from theoretical and practical aspect, implementing novel formally-verifiable security contracts that will specify security properties of the distributed code, and supporting this contracts with a novel code refactoring tools and dynamic vulnerability detection techniques. The focus on the project will be on machine-learning based analytics techniques, addressing the issues that come from both storage and processing of the data and learning models. As an additional layer of security, we will implement novel integrated behaviour-based user authentication schemes.


\subsection{Aims and Objectives}
\label{sect:objectives}

\eucommentary{\emph{Describe the specific objectives for the project1, which should be clear, measurable, realistic and achievable within the duration of the project. Objectives should be consistent with the expected exploitation and impact of the project (see section 2).}}


The specific \emph{aims} of the \TheProject{} project are:

\begin{description}
\item[Aim 1:] To produce a novel discipline for the development of distributed AI-based big-data analytics systems, spanning different software engineering layers, from new \emph{security coding standards}, new structured high-level \emph{security patterns}, new software development tool-support (in the form of \emph{refactoring} and \emph{vulnerability detection}) to rigorous methods for the \emph{formal specification and verification} of security properties, addressing security and privacy of both data storage and data processing;
%\item[Aim 1:] To produce novel rigorous methods for the \emph{formal specification and verification} of 
%security properties of
 % distributed AI-based data analytics systems, addressing security and privacy of both data storage and data processing;

\item[Aim 2:] To %build on the 
enhance existing and develop new efficient techniques for \emph{identifying generic code patterns} that
  represent known security vulnerabilities and information leakages in distributed AI-based systems, and to develop novel techniques for  
  \emph{repairing the identified vulnerabilities and leakages};

\item[Aim 3:] To build an additional security layer on top of the existing end user applications that will be based on intelligent, secure and privacy-preserving \emph{identity management schemes};

\item[Aim 4:] To integrate the \TheProject{}'s tools and techniques into a coherent \emph{self-healing methodology} for establishing
  security and privacy properties of the code and repairing any identified vulnerabilities and leakages, targeting both 
  existing distributed AI-based data analytics code and new provably-secure code;

\item[Aim 5:]  To demonstrate the applicability of the \TheProject{}'s tools and
 methodology in building secure real-world applications from 
 several critical %the medical, aerospace and banking 
 domains, and to promote their long-term uptake by building a sustainable user community,
 covering both experts in security and normal application developers.

\end{description}

The corresponding concrete \emph{objectives} are: 
%\begin{description}

%\item[Objective 1:] To develop a novel concept of machine-readable \emph{security contracts}
%  that will be used to specify precisely security properties for parts of the C++ code, and to develop methods
%  for their formal specification and verification in the new and existing C/C++ code. \emph{This fulfils part of \textbf{Aim 1} and \textbf{Aim 4}}.
%  \comment{CB: It's not clear to me what machine-readable actually means, as, arguably, any arbitrary stream of bytes in a file is machine readable :D I think this needs clarified. There's a worry here that this is the same as what we have done on TeamPlay, where we also developed proofs, contracts and certificates for security, energy and time. What is the novelty here? There should probably be some tie-in, perhaps, to the work done on TeamPlay, CSL, for instance, or the proof system developed there. I think this needs to tie in with the refactoring tooling. One novel approach here is the refactorings... perhaps new refactorings that transform the application in such a way that it can meet a user-defined security specification or contract?  }
%  \comment{JB: If we are treating security contracts as a notion at several levels of abstraction and we can go between them, this would potentially be quite different from TeamPlay? It should not just be at the code level... }
 
 
 
 
%\item[Objective 1:] To develop a novel concept of \emph{secure} code patterns for distributed AI-based data analytics and to support these
%patterns with strictly-defined and formally verifiable multi-level 
%\emph{security contracts}, raising the level of abstraction
%for developing secure distributed applications and providing formal guarantees about their security properties. 
%\emph{This fulfils part of \textbf{Aim 1}}. 

%\item[Objective 2:] To develop novel techniques based on a combination of static analysis, dynamic symbolic execution and run-time monitoring for identifying security and privacy vulnerabilities across layers %in multi-layer AI-based 
%in big data applications, addressing the complete software development stack from low-level distributed file system operations to high-level machine-learning, guiding the verification of security and privacy properties in large-scale distributed settings. \emph{This fulfils parts of \textbf{Aim 1}, \textbf{Aim 2} and \textbf{Aim 4}}.
  
%\item[Objective 3:] To develop novel \emph{self-healing} techniques based on software refactoring and machine-learning based run-time adaptation for semi-automatic repairing of the identified security vulnerabilities in the code. %, together with the 
%Investigate novel privacy-preserving and knowledge transfer mechanisms for data analytics based on rigorous mathematical analysis of information leakage. \emph{This fulfils part of \textbf{Aim 2} and \textbf{Aim 4}}.

%\item[Objective 4:] To build on existing and/or develop new security \emph{coding standards} for C++, and to build tools for automatic code analysis in accordance with 
%these standards, providing guidance on how to develop new secure distributed data analytics applications. \emph{This fulfils parts of \textbf{Aim 1}, \textbf{Aim 2} and \textbf{Aim 4}}.

%\item[Objective 5:] To augment the existing security layer of distributed data analytics with AI-driven identity management schemes, implementing intelligent biometric technology for multi-factor authentication and continuous user identification, and blockchain technology for user access management. \emph{This fulfils parts of \textbf{Aim 3} and \textbf{Aim 4}}.

%\item[Objective 6:] To develop a novel methodology based on formally verified security contracts, vulnerability detection \& mitigation, 
 % privacy-preserving and secure authentication, fully supported by semi-automated code refactoring techniques for
  %increasing the security of %the 
 % existing/new % and developing secure new 
 % distributed data-analytics code. \emph{This fulfils
%    part of \textbf{Aim 4}}.

%\item[Objective 7:] To demonstrate that \TheProject{}'s technology improves security and prevents %reduced
  %information leakage in %distributed AI-based 
  %data analytics code, through a variety of real-world security-sensitive
  %use cases from different domains
  %such as healthcare, banking and air-traffic management. % from healthcare, banking and aviation application domains. 
  %\emph{This fulfils part of \textbf{Aim 5}}.

%\item[Objective 8:] To build a sustainable user community involving a variety of stakeholders that will ensure the long-term
 % uptake, development and commercial success of the tools and techniques developed over the course of the \TheProject{}
 % project. \emph{This fulfils part of \textbf{Aim 5}}.

%\end{description}

%\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{End Users \& Software Developers.}
%Most of the technologies to be developed within
%%that will be developed over the course of 
%\TheProject{}  aim to support \emph{software developers}
%%are aimed at assisting \emph{software developers} 
%with secure coding and deployment. %developing and deploying their code. 
%%This includes the 
%The \TheProject{} methodology envisages user-friendly secure authentication mechanisms as well as %, as well as the 
%tools to detect and repair security vulnerabilities, to formally verify properties of the code, and to analyse code for conformance to coding standards. % and authentication mechanisms. 
%\TheProject{} will benefit \emph{end users} by increasing their trust in big data analytics systems.
%%. However, \TheProject{} will also benefit the \emph{end users} of the big data applications.
%The secure software will enhances the end users experience for citizens' services by improvement of the systems used for air traffic management, digital banking, healthcare, open banking and citizen services uses daily in Europe. The \TheProject{} improvements in code directly improves the systems they integrate into to form better solutions.
%The chosen use cases and their domains will provide evidence of the impact \TheProject{} can  have on both.
%%This is most notable in the considered use cases, that will directly benefit the end users by XX. 
%%Additionally, the authentication mechanisms will allow these users to XX. 
%\end{mdframed}

%\label{sect:objs-detailed}

\TOWRITE{UOD and USTAN}{This needs to be done after
the objectives are defined above   We need about 1-2 paragraphs
per objective, just to flesh it out.}

%\subsubsection{Detailed Description of the Objectives}

\subsubsection*{Objective 1: Develop new \emph{secure code patterns} supported by formally-verifiable \emph{security contracts}.}
% Secure Patterns for Distributed Data Processing
\vspace{-6pt}
Code patterns are a well-established abstraction for designing and implementing complex software systems, and have been endorsed by a number of major IT companies such as Intel and Microsoft. Patterns allow structuring of the code at a high level of abstraction and make formal reasoning about the code much easier, as they can provide additional information to the underlying formal models. On the other hand, a formal treatment of the security properties of the code requires a multi-level approach, where a high-level human-understandable description of the properties of the code is transformed into a precise, \emph{machine-readable} description that is structured and can be used for formal reasoning. In \TheProject{}, we will develop novel \emph{secure design patterns} for distributed large-scale data processing that will specify how to combine different levels of abstraction in distributed applications (from low-level libraries for distributed filesystems and databases to high-level machine-learning libraries and end user code). These patterns will give strict guarantees about the security properties of the underlying code. This will be achieved by supporting the patterns with \emph{security contracts} at different levels of abstraction - both describing the properties of code in natural language and in a machine-readable lower-level language suitable for automated reasoning. We will also develop mechanisms to automatically prove the properties specified in the security contracts for C++ and Java applications, and mechanisms to translate between different representations of security contracts. Software developers will then be able to choose security-certified patterns suitable for their data analytics task, and will be presented with formally guaranteed security properties of the resulting code in an easily-understandable way. \emph{This fulfils part of \textbf{Aim 1} and \textbf{Aim 2}}.
%Formal treatment of the security properties of the code requires a multilayer approach. On one hand, end users want to see a description of the security properties in a clear, high-level and human-understandable way. On the other hand, formal reasoning, including automatic proving of the properties, requires significantly lower-level approach. The language for formal reasoning needs to be concise and machine-readable. This necessitates different approach to describing properties at different levels. In the \TheProject{} project, we will develop a strict and precise notion of security properties at different levels of abstraction. We will develop a concept of \emph{security certificates}, which will describe in natural way the properties of the code. We will also develop \emph{security contracts}, which will be specified in low-level, machine-readable way appropriate for automated reasoning. We will also develop mechanisms to automatically prove the properties specified in the security contracts for pieces of C++ and Java code and also mechanisms to translate security certificates into security contracts and vice versa. The end users will then be able to specify the required properties using high-level language, and these properties will be automatically checked by lower-level mechanisms.
%\cbcomment{See above comments about similarities with teamplay. CSL is a source-level annotation language that allows the programmer to describe security propoerties in a clear high-level human-understandable way with certificates. Idris provides the proofs and formal reasoning and contracts. I'm also not keen on multiple languages. Is there a reason for both C++ and Java?}


%\subsubsection*{Objective 2: Identification of Security and Privacy Risks in AI-Based Data Analytics}
%To develop novel techniques based on a combination of static analysis, dynamic symbolic execution and run-time monitoring for identifying security and privacy vulnerabilities across layers %in multi-layer AI-based 
%in big data applications, addressing the complete software development stack from low-level distributed file system operations to high-level machine-learning, guiding the verification of security and privacy properties in large-scale distributed settings. \emph{This fulfils parts of \textbf{Aim 1}, \textbf{Aim 2} and \textbf{Aim 4}}.
\subsubsection*{Objective 2: Develop a novel infrastructure for identifying security and privacy vulnerabilities.}
\vspace{-6pt}

A typical AI-based data analytics application is built from multiple layers, combining reliable, fault-tolerant data storage mechanisms (such as distributed databases and distributed file systems) with generic data processing frameworks such as MapReduce~\cite{mapreduce}, and more specific libraries for machine learning, such as Spark MLLib~\cite{mllib}. Many of these libraries are open source and do not have well-established security properties. Security risks and privacy leaks can come from any layer, as well as from the interaction between different layers. In addition to that, machine learning presents additional difficulties, as the security risks can arise both from training the models and from using the built models, as well as from the models themselves. In \TheProject{},  we will develop methods to \emph{identify security risks} coming both from the individual agents involved in the process of storing and analysing big data, and also from the interaction between different agents in distributed settings. For identifying security risks, we will use \emph{static source code analysis}, dynamic \emph{symbolic execution} (where a program is executed abstractly) covering multiple possible inputs of the program that share a particular execution path through the code, and \emph{run-time} machine-learning based monitoring of the application \emph{execution}. We will also develop methods based on stochastic modelling and information theory for identifying and quantifying leakage of private information coming from building and applying distributed machine learning models. \emph{This fulfils parts of \textbf{Aim 1}, \textbf{Aim 2} and \textbf{Aim 4}}.

\subsubsection*{Objective 3: Self-Healing Mechanisms for Repairing Vulnerabilities and Addressing Privacy-Leakage.}
Whilst identifying security risks and privacy leaks is critical for security of distributed data analytics and knowledge sharing across parties, it is equally important to develop methods for \emph{repairing} the identified risks and leaks. These methods should be as transparent to the software developers as is practically possible, not requiring them to be security experts whilst still allowing them to develop efficient and secure code. At the same time, software developers 
using these novel repair methods
%user 
should be aware of the changes made to the code, allowing them to intervene and apply their domain-specific knowledge to the code transformations with the aim to improve security and information privacy. In \TheProject{}, we will develop novel \emph{self-healing} techniques for semi-automatic transformation of the code to eliminate security risks and privacy leaks. Self-healing will be based on both incremental, developer-driven formally proven \emph{code refactorings} of the source code and machine-learning based \emph{run-time adaptation} mechanisms. Any potential privacy-leakage will be quantified and addressed using optimal data release mechanisms. We will also address the problem of transferring knowledge (that has been extracted from the private data of a party) from one party to another party whilst still preserving privacy of sensitive information contained in the data. \emph{This fulfils part of \textbf{Aim 2} and \textbf{Aim 4}.}

%% We will move part of this to the background work & advancement beyond STOA
%%\subsubsection*{Objective 3: Symbolic Execution for Discovering Security Vulnerabilities}
%%\vspace{-6pt}
%\vjcomment{IBM to write this part}
%%Particular execution path through the code. The execution treats these inputs symbolically, “returning” a result that is expressed in terms of symbolic constants that represent those input values.
%%A known advantage of the symbolic execution technique is its ability to avoid giving false warnings; any error found by symbolic execution represents a real, %%feasible path through the program, and can be witnessed with a test case that illustrates the error.
%%In the \TheProject{} project, we will extend the ExpliSAT symbolic execution technology of IBM to discover known security vulnerability patterns in C/C++code. Moreover, to avoid known limitations of the symbolic execution approach (e.g. path explosion, memory growth, etc.), we will develop a technology that leverages the combination of white box fuzzing and symbolic execution to find exploitable bugs. The idea is to perform symbolic execution as the main technique to discover vulnerabilities combined with tailored fuzzing techniques in specific small areas for the purpose of assisting the symbolic execution engine to consistently make progress.

%% VJ : Also move this to the background & advancement beyond STOA
%\subsubsection*{Objective 4: Identifying and Controlling Privacy-Leakage in Distributed Machine Learning}
%\vspace{-6pt}
%The datasets containing sensitive information can't be publicly shared as a privacy-risk posed by several types of attacks exists. A novel information theoretic framework is introduced for privacy-preserving distributed machine learning such that privacy-leakage is quantified by the mutual information between sensitive data and released data. At the core of privacy-preserving framework lies a stochastic model approximating the uncertain mapping between released noise added data and private data such that the model is employed for variational approximation of informational privacy. The suggested privacy-preserving framework consists of three components: 1) Optimal Noise Adding Mechanism; 2) Modeling of Uncertain Mapping Between Released Noise Added Data and Private Data; and 3) Variational Approximation of Information Privacy. 
%There is an interest in sharing knowledge extracted from the data owned by a party with another party while simultaneously preserving the privacy of private data of both parties. An analytical framework is introduced to study and optimize the privacy-preserving transfer of knowledge extracted from a large set of labelled private data owned by a party to another party owning a few labelled data samples. An information theoretic approach is considered to quantify transferability of knwoledge from source to target domain in-terms of mutual information between source and target data. The privacy secured knowledge sharing framework facilitates development of transfer and multi-task machine learning algorithm while optimizing the privacy-transferability tradeoff.                

\subsubsection*{Objective 4: New Security Coding Standards and Analysis for Conformance to These Standards.}
The source code for software infrastructures and applications is a common source of security vulnerabilities. This is specially relevant in compiled languages, such as C or C++, where the source code is translated into a native binary executable (that is directly executed without intermediate virtual machines). It is possible that some vulnerabilities may be introduced because of bad software practices. However, there is also a category of vulnerabilities that are introduced due to specific features of the programming language itself.
In \TheProject{}, we will develop new coding standards that can help to mitigate vulnerabilities in typical big data applications with special attention to vulnerabilities derived from  the programming language. %Then we will apply 
These coding standards will then be used to identify vulnerabilities that may be found in distributed data storage components and in distributed pattern-based machine learning software components. \emph{This fulfils parts of \textbf{Aim 1}, \textbf{Aim 2} and \textbf{Aim 4}}.

\subsubsection*{Objective 5: New Intelligent Identity Management and Trust Schemes}
\vspace{-6pt}

In \TheProject{}, we will develop an Identity-as-a-Service (IDaaS) technology that will allow software developers to easily integrate, deploy and manage methods for identity, authentication and access management depending on custom requirements and policies. In particular, we will develop and evaluate novel multi-factor authentication methods and intelligent biometrics based on a combined analysis of physiological, face and eye gaze data analytics for continuously identifying end users. In addition, we will develop a blockchain solution for access management, as well as data analytics aiming to provide intelligent insights from users' behaviour and biometric data allowing software developers to deliver a personalised security experience and thus increase user acceptance and trust. Finally the developed IDaaS technology will provide a fluid end user experience based on state-of-the-art identification, authentication and access control methods such as easy-to-use tokens, usable, single-touch user approvals, as well as intelligent biometrics based on end users' physiological, face and eye gaze features. \emph{This fulfils \textbf{Aim 3} and parts of \textbf{Aim 4}}.

\subsubsection*{Objective 6: Methodology for Development of Secure Applications}
\vspace{-6pt}

To make \TheProject{} technology usable by non-experts in security, it will be necessary to provide both low-level techniques and high-level abstractions. Low-level techniques include identification, repairing of the security/privacy vulnerabilities and complex formal verification. High-level techniques include presenting the security properties to the software developer using high-level abstractions (or \emph{patterns}). These high-level patterns allow us to bridge the gap between these low-level techniques and the high-level software-development level. It is essential to develop a structured methodology, accompanied by an associated tool chain, which will provide guidance to software developers helping them develop their code and provide trust in the code's security properties. In \TheProject{}, we will develop a novel methodology for developing provably secure distributed AI-based data analytics applications. The cornerstone of our methodology will be \emph{software refactoring} as a method for user-guided semi-automatic structured code rewriting, %of the %end user code, 
which will enable easily understandable step-by-step transformation of the initial code into its provably secure semantically-equivalent version. We will also integrate identity management, vulnerabilities identification, self-healing and property proving techniques into tools for static and dynamic analysis of the code and develop a common easily-accessible front-end that will be tailored to domain experts (but who might not be experts in security). In this way, the programmer will be fully included in the development loop, having full control over the analysis and transformations that will be performed to make their code more secure. %and avoid any privacy breach. 
%cbcomment{I wonder if the refactoring is downplayed too much. If the proposal focused more on the idea of the end user, refactoring becomes vital to that. Also the idea of refactorings to transform programs to make them more secure. Perhaps meeting the specifications/contracts that are described in Objective 1. Also, maybe the refactoring use some kind of security pattern in the rewriting, which would be a novelty. }
\emph{This fulfils part of \textbf{Aim 4}}.

\subsubsection*{Objective 7: Demonstrating \TheProject{} Tools and Technologies on Real-World Use Cases.}
%  from automotive, machine learning and IoT domains.}
\vspace{-6pt}
We will validate the \TheProject{} methodology for developing secure applications together with the associated tools and technologies, on realistic use cases taken from our partners at \SOPRAshort{} and \FRQshort{}.
%In particular, the \TheProject{} technologies will be applied to the development
%of a \emph{completely new} distributed data analytics application by XX.
Consequently, all of the technologies that will be developed over the course of the \TheProject{} project will be extensively tested in a realistic setting, enabling us to identify new issues and security risks as they arise, and guiding our road-map for future technological adoption. The use cases will demonstrate that we are able to deal with large-scale AI-based distributed data analytics in a secure and privacy-preserving way, processing large volumes of data with increased trust in the safety of the operations. In addition, \TheProject{}'s tool chain supporting
%by using 
a user-guided refactoring-driven methodology %, supported by a tool chain, we will 
will help to reduce the cost of development, deployment and maintenance of distributed AI-based solutions. 
%distributed systems.
%Thanks to our \emph{data fabrication} technology, we will be able to test parts of the system
%and a system as a whole on large volumes of synthetic, but realistic,
%data, both during development and during deployment.
\emph{This fulfils part of \textbf{Aim 5}}.

\pagebreak
\subsubsection*{Objective 8: Long-Term Uptake of \TheProject{} Technologies} 
\vspace{-7pt}
\TheProject{} aims to ensure long-term uptake of the technologies that we will develop by engaging with relevant users, developers and adopter communities, by following an Open Science approach, and by providing a road-map for the future development and exploitation of the \TheProject{} technologies. We have built in specific user community building activities, including  workshops, tutorials, webinars and training sessions that will serve to actively promote the use of the \TheProject{} technologies. Whenever feasible, our software and results will be made publicly available in open source/open data repositories. We will actively engage with potential users from the chosen domains through exhibitions, demonstrations, video animations and presentations at existing events and conferences such as ApacheCon, through the annual EU ICT conference, through exploiting our excellent high-level contacts with the DataStax and MongoDb. Innovation Centres and other relevant organisations, through contacts with national government agencies and through other relevant expert networks.
\emph{This fulfils part of \textbf{Aim 5}}.

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{End Users \& Software Developers.}
Most of the technologies to be developed within
%that will be developed over the course of 
\TheProject{}  aim to support \emph{software developers}
%are aimed at assisting \emph{software developers} 
with secure coding and deployment. %developing and deploying their code. 
%This includes the 
The \TheProject{} methodology envisages user-friendly secure identity management schemes as well as %, as well as the 
tools to detect and repair security vulnerabilities, to formally verify properties of the code, and to analyse code for conformance to coding standards. % and authentication mechanisms. 
\TheProject{} will benefit \emph{end users} by increasing their trust in big data analytics systems.
%. However, \TheProject{} will also benefit the \emph{end users} of the big data applications.
The secure and usable software aims to enhance the end users' experiences, technology acceptance and trust within citizens' services by improving the systems used for air traffic management, digital banking, healthcare, open banking and citizen services used daily in Europe. The \TheProject{} improvements in code directly improves the systems they integrate into, in order to form better solutions.
The chosen use cases and their domains will provide evidence of the impact \TheProject{} can have on both.
%This is most notable in the considered use cases, that will directly benefit the end users by XX. 
%Additionally, the authentication mechanisms will allow these users to XX. 
\end{mdframed}


%\pagebreak
\subsection{Relation to the Work programme}
%% SU-DS02-2020 Need to check this...
\TheProject{} directly addresses the challenge that is posted by \textbf{H2020-SU-DS-2020}. By exploiting the partners' expertise in formal methods, identity management, security, privacy and AI-based distributed data analytics and by integrating the developed solutions into a semi-automated code analysis, refactoring and run-time adaptation based software development methodology, we aim to \textbf{integrate state-of-the-art approaches for security and privacy management in a holistic and dynamic way} while \textbf{relying on Artificial Intelligence and automation to reduce the level of human intervention necessary}. Our techniques for identifying and repairing security and privacy vulnerabilities, addressing both storage and processing of data in big-data analytics applications will address the problem of \textbf{storage and processing of data in different interconnected places}. Integration of all the methods into a coherent technology, comprising also run-time vulnerabilities detection and run-time adaptation to address the identified problems, will allow the end users to \textbf{constantly forecast, monitor and update the security of their ICT systems}. Collectively, detecting and repairing security and privacy risks in data storage and processing will allow \textbf{monitoring and mitigation of security risks, including those related to data and algorithms}. Our inherent focus on distributed systems, with a particular focus on privacy and security of machine learning frameworks and models, both during training and usage, will improve \textbf{collaboration and sharing of information related to security and privacy management}. 


The specific focus of \TheProject{} is on Part \textbf{c) advanced security and privacy solutions for end users or software developers} of the proposal call. Within that, our main aim is to \textbf{develop automated tools for formally checking the security and privacy of data, systems, online services and applications}. By building on the well-established software engineering concept of \emph{patterns} for data processing, and by supporting the techniques that will be developed over the course of the project by semi-automated tools for analysing and transforming the user code, as well as the methods for secure and usable identity management, we will \textbf{support end users or software developers (possibly including developers of AI solutions) in their efforts to select, use and create trustworthy digital services}. By evaluating the \TheProject{} techniques on real-world applications coming from aerospace, banking and healthcare domains, we will address \textbf{real application cases}. Our refactoring-based programming methodology will contribute to \textbf{automatic code generation}. Supporting the secure patterns for distributed data processing by techniques for the formal specification and verification of their security properties, we are contributing to the creation of \textbf{trustworthy data boxes}, \textbf{certification and assurance} and \textbf{cyber insurance}.
%
Section~\ref{sec:impact} describes in detail how \TheProject{} meets the expected impacts that have been defined for the \textbf{H2020-SU-DS-2020} proposal call.

%As described in Section~\ref{sec:impact} (Expected Impacts), \TheProject{} will meet all the expected impacts that have been defined for \textbf{H2020=SU-DS-2020}, developing appropriate metrics to measure its impact on
%\begin{itemize}
%\item Reduced number and impact of cybersecurity incidents;
%\item Efficient and low-cost implementation of the NIS Directive and General Data Protection Regulation;
%\item Effective and timely co-operation and information sharing between and within organisations as well as self-recovery;
%availability of comprehensive, resource-efficient, and flexible security analytics and threat intelligence, keeping pace with new vulnerabilities and threats;
%\item Availability of advanced tools and services to the CERTs/CSIRTs and networks of CERTs/CSIRTs;
%\item An EU industry better prepared for the threats to IoT, ICS (Industrial Control Systems), AI and other systems;
%\item Self–recovering, interoperable, scalable, dynamic privacy-respecting identity management schemes.
%\item Availability of better standardisation and automated assessment frameworks for secure networks and systems, allowing better-informed investment decisions related to security and privacy;
%\item Availability and widespread adoption of distributed, enhanced trust management schemes including people and smart objects;
%\item Availability of user-friendly and trustworthy on-line products, services and business;
%better preparedness against attacks on AI-based products and systems;
%\item A stronger, more innovative and more competitive EU cybersecurity industry, thus reducing dependence on technology imports;
%\item A more competitive offering of secure products and services by European providers in the Digital Single Market.
%\end{itemize}
%\eucommentary{Indicate the work programme topic to which your proposal relates, and
% explain how your proposal addresses the specific challenge and scope
% of that topic, as set out in the work programme.}


%{\color{blue}{
%    Specific Challenge:
%    
% In order to minimise security risks, ICT systems need to integrate state-of-the-art approaches for security and privacy management in a holistic and dynamic way. Organisations must constantly forecast, monitor and update the security of their ICT systems, relying as appropriate on Artificial Intelligence and automation, and reducing the level of human intervention necessary.
%
%Security threats to complex ICT infrastructures, which are multi-tier and interconnected, computing architectures, can have multi-faceted and cascading effects. Addressing such threats requires organisations to collaborate and seamlessly share information related to security and privacy management.
%
%The increasing prevalence and sophistication of the Internet of Things (IoT) and Artificial Intelligence (AI) broadens the attack surface and the risk of propagation. This calls for tools to automatically monitor and mitigate security risks, including those related to data and algorithms. Moreover, storage and processing of data in different interconnected places may increase the dependency on trusted third parties to coordinate transactions.
%
%Advanced security and privacy management approaches include designing,
%developing and testing: (i) security/privacy management systems based
%on AI, including highly-automated analysis tools, and deceptive
%technology and counter-evasion techniques without necessary human
%involvement; (ii) AI-based static, dynamic and behaviour-based attack
%detection, information-hiding, deceptive and self-healing techniques;
%(iii) immersive and highly realistic, pattern-driven modelling and
%simulation tools, supporting computer-aided security design and
%evaluation, cybersecurity/privacy training and testing; and (iv)
%real-time, dynamic, accountable and secure trust, identity and access
%management in order to ensure secure and privacy-enabling
%interoperability of devices and systems.
%
%\begin{itemize}
%\item[(c)]: Advanced security and privacy solutions for end users or software developers
%
%Proposals should develop automated tools for checking the security and privacy of data, systems, online services and applications, in view to support end users or software developers (possibly including developers of AI solutions) in their efforts to select, use and create trustworthy digital services. Proposals should address real application cases and at least one of the following services: automatic code generation, code and data auditing, trustworthy data boxes, forensics, certification and assurance, cyber insurance, cyber and AI ethics, and penetration testing.
%
%The outcome of the proposal is expected to lead to development up to Technology Readiness level (TRL) 6; please see Annex G of the General Annexes.
%
%The Commission considers that proposals requesting a contribution from the EU of between EUR 2 and 5 million would allow this specific challenge to be addressed appropriately. Nonetheless, this does not preclude submission and selection of proposals requesting other amounts.
%
%Type of Action: Research and Innovation Action
%
%\item[(d)]: Distributed trust management and digital identity solutions
%
%With particular consideration to IoT contexts, applicants should propose and test/pilot innovative approaches addressing both of the following points: (i) distributed, dynamic and automated trust management and recovery solutions; and (ii) developing novel approaches to managing the identity of persons and/or objects, including self-encryption/decryption schemes with recovery ability. Proposals should address real application cases.
%
%The outcome of the proposal is expected to lead to development up to Technology Readiness level (TRL) 5-6; please see Annex G of the General Annexes.
%
%The Commission considers that proposals requesting a contribution from the EU of between EUR 3 and 6 million would allow this area to be addressed appropriately. Nonetheless, this does not preclude submission and selection of proposals requesting other amounts.
%
%Type of Action: Research and Innovation Action
%\end{itemize}
%
%}}


\subsection{Concept and Approach}

\eucommentary{Describe and explain the overall concept underpinning the project. Describe the main ideas, models or assumptions involved. Identify any trans-disciplinary considerations;}

%% Why do we need distributed data analytics?
In the modern world, data is everywhere around us. It is estimated that by 2025, the amount of data collected from various sources will grow to 175 zetabytes\footnote{One zetabyte is $10^{21}$ bytes.}. %If we were to store 175 zetabytes of data on DVDs and stack them together, this stack would be long enough to circle the Earth 222 times (\url{https://www.bernardmarr.com/default.asp?contentID=1846})}. 
This volume of data offers a huge economic and scientific potential - businesses can analyse purchasing habits of their customers and offer personalised recommendations, and scientists can analyse ever increasing datasets to obtain groundbreaking and potentially life-saving results (e.g.~secure and privacy preserving contact tracing apps that can be trusted by people that belong to vulnerable groups). Indeed, AI-based data analytics have become one of the most prominent areas of computer science. As a consequence, a variety of systems that support complex analytics techniques on raw data have been developed, with some examples being Spark MLLib~\cite{mllib}, Tensorflow~\cite{tensorflow} and Azure AI\footnote{https://azure.microsoft.com/en-gb/resources/microsoft-ai-platform-whitepaper}. Many of these systems are \emph{parallel and distributed} by nature, requiring rapid  response from very expensive computations and having data distributed over numerous sources. Typical execution platforms are Computational Clouds that offer substantial computational and storage potential at relatively cheap price. 
%. The requirement for parallelism comes from the fact that performing data analytics on large volumes of data is usually computationally very expensive, and doing such computations in parallel on many-core systems can significantly reduce response time and deliver results much faster. The distributed aspect of data analytics comes from both distributed nature of the sources of data (where some data cannot be moved between different sites due to legal or practical reasons), and by the fact that distributed systems (such as clouds) offer much more computational power at relatively cheap price than even the high-end local highly-performance systems. 
%
The digitisation of the modern world, with more and more devices connected to the Internet, has increased the prevalence of cybersecurity attacks. Rapid development of AI algorithms and techniques have allowed these attacks to become more and more sophisticated and harder to detect and to deal with. The distributed nature of data analytics techniques just exacerbates this problem. In the distributed world, security risks come both from the operation of individual agents as well as from the interaction between these agents. At the same time, the mechanisms for ensuring security and preserving privacy, which are currently used in big data analytics systems, are fairly basic~\cite{basicSecurityBigData}. Moreover, a \emph{formal} treatment of security properties of distributed data analytics code is almost completely lacking. Being able to formally guarantee that the data processing code satisfies certain security properties, in terms of not exposing security vulnerabilities and not accidentally allowing a breach of privacy of the data it works on, is of paramount importance. Yet there are very few systems that can guarantee these kind of properties of the code and that can, moreover, automatically and strictly check that the end user code conforms to the security specification or one of the desired security standards. Lack of guarantees about security properties, coupled with very complex schemes of interaction between data processing and data storage agents in a distributed world makes establishing the security of distributed data analytics one of the most complex and overlooked issues currently. This is especially true in light of the massive increase in software production over the last decades, with up to 110 billion lines of code being written each year. As applications become more complex, the detection and repairing of security and privacy vulnerabilities becomes more complicated and expensive. The \TheProject{} project aims to tackle this crucial issue of security of AI-driven big data analytics systems.

%The digitisation of modern world, together with advances in the software systems, has had one undesirable consequence - the prevalence of cybersecurity problems. As more and more devices are connected to the internet, there are more and more potential targets for cyber attacks. Rapid development of AI algorithms and techniques allowed these attacks to become more and more sophisticated and harder to detect and to deal with. Distributed nature of many of the data analytics techniques just exacerbates this problem. In the distributed world, all of the potential security issues for individual data storage/processing agents are present, as well as many additional risks that come from interaction of these agents in the distributed world and from movement of data between different distributed nodes. The mechanisms for ensuring security and preserving privacy that are currently used in the big data analytics systems are fairly basic. Moreover, \emph{formal} treatment of security properties of the code is completely lacking. Being able to formally guarantee that the data processing code satisfies certain security properties, in terms of not exposing security vulnerabilities and not accidentally allowing breaching of privacy of the data it works on is of paramount importance. Yet there are very few systems that can guarantee these kind of properties of the code and that can, moreover, automatically and strictly check that the end user code conforms to the security specification or one of the desired security standards. Lack of guarantees about security properties, coupled with very complex scheme of interaction betweeen data processing and data storage agents in distributed world makes establishing security of distributed data analytics one of the most complex and overlooked issues currently. The \TheProject{} project aims to tackle this crucial issue of security of AI-driven big data analytics systems.



%\vspace{-6pt}

\subsubsection{Challenges for Security and Privacy}
% massively-parallel heterogeneous systems}

Ensuring security of distributed AI-driven big data analytics presents a number of challenges:

\begin{itemize}
\item \textbf{Security of Systems for Storage and Processing of Big Data.} Large volumes of raw data on which data analytics is performed are usually stored in distributed NoSQL databases, such as Cassandra (~\url{https://cassandra.apache.org}), or simply as flat files in some form of distributed file system, such as Hadoop's HDFS (\url{https://hadoop.apache.org}). Systems for processing such data include pattern-based general parallel frameworks such as MapReduce~\cite{mapreduce} or Spark (\url{https://spark.apache.org}). Primary concerns for all these systems are robustness, scalability, fault tolerance and performance. Because they are rarely designed with security in mind, the security mechanisms in them are usually relatively basic. In addition to the problem of protecting the data stored and processed on a single node, distributed data analytics presents additional security issues which occur when the data is on the move between different nodes and when distributed processing nodes access distributed data. Additional problems arise from interaction between processing frameworks and data storage systems. \textbf{We must ensure both the safety of storage and processing of data on local nodes in a distributed system and that the processing across nodes and movement of data between nodes does not expose potential security and/or privacy vulnerabilities that can be exploited in cyber attacks.}

\item \textbf{Enforcement of Secure Coding.} Software components are continuously developed at both infrastructure and application levels. Coding guidelines play a key role in avoiding the introduction of vulnerabilities in the code during the development process, which can compromise the system's security. Many such vulnerabilities, especially in native languages as C++, are derived from the language definition itself. Moreover, programming languages are not static entities. For example, the C++ programming language has had four versions during the last 9 years. \textbf{We must enforce that new and existing code adheres to coding standards that avoid the introduction of new vulnerabilities derived from programming languages features}. 

\item \textbf{Vulnerability Detection in Source Code.} The importance of cyber security has become more and more signiﬁcant. The root cause of most cyber-attacks is vulnerabilities in source code. Vulnerabilities exploited by attackers compromise the conﬁdentiality, integrity, and availability of information systems. Early detection of vulnerabilities is an eﬀective way to reduce the loss and potential damage. Despite the efforts of experts, vulnerabilities remain a huge problem and will continue to exist in the long term. This can be justiﬁed by the fact that an increasing number of vulnerabilities are published every year. Vulnerability detection is a method to discover vulnerabilities in software. \textbf{We must ensure that new and existing C/C++ code is free as much as possible from known vulnerabilities.}

%\item \textbf{Security of Pattern-Based Distributed Parallel Processing.} In order to analyse large volumes of data and extract useful information and knowledge from it, it is essential to use parallel processing. This is not only in order to get the results in reasonable time - the requirement for parallel processing also comes from the way in which big data is stored. The space requirements for large data sets usually surpass the storage capabilities of individual machines and, therefore, this data has to be distributed. %Furthermore, bringing all data to one place for analysis is usually prohibitively expensive, even when it is theoretically possible. 
%Therefore, it is standard to have different processing agents on different distributed nodes which work together in analysing large data sets. Parallel patterns, such as Google's MapReduce that is used in Hadoop, provide high-level framework for data analytics where parallelism is provided for free. However, for the same reasons as with data storage, these systems potentially expose many additional security risks, coming from their interaction over networks as well as from the individual processing agents. Moreover, these frameworks usually need to interact with distributed filesystems/databases and higher-level domain-specific libraries, increasing the potential for vulnerabilities. \textbf{We must ensure that distributed parallel processing used in AI-based data analytics does not leak sensitive information from the data it processes to malicious users.}

\item \textbf{Optimisation of Privacy-Preserving Data Release Mechanism.} A data release mechanism aims to provide useful data %available 
whilst simultaneously limiting any revealed sensitive or private information. %Different methods such as k-anonymity, l-diversity, t-closeness, and differential privacy have been developed to address the privacy issue. 
Differential privacy is a formal framework to quantify the degree to which the privacy for each individual in the dataset is preserved whilst releasing the output of a data analysis algorithm. Differential privacy guarantees that an adversary, by virtue of presence or absence of an individual's data in the dataset, cannot draw any conclusions about an individual from the released output of the analysis algorithm. Differential privacy, however, does not always adequately limit inference about participation of a single record in the database. Differential privacy requirement does not necessarily constrain the information leakage from a data set. Correlation among records of a dataset would degrade the expected privacy guarantees of a differential privacy mechanism. These limitations of differential privacy motivate an information theoretic approach to privacy where privacy is quantified by the mutual information between sensitive information and the released data. The data perturbation approach uses a random noise adding mechanism to preserve privacy, however, results in distortion of useful data and thus utility of any subsequent machine learning and data analytics algorithm is adversely affected. There remains the challenge of studying and optimising privacy-utility tradeoff especially in the case when statistical distributions of data are unknown. \textbf{We must, therefore, develop new \emph{informational} privacy mechanisms that add the optimal amount of noise to prevent privacy leakage, whilst still allowing for extracting useful machine-learning models from the data.}

%These limitations of differential privacy motivate an information theoretic approach to privacy where privacy is quantified by the mutual information between sensitive information and the released data. The data perturbation approach uses a random noise adding mechanism to preserve privacy, however, results in distortion of useful data and thus utility of any subsequent machine learning and data analytics algorithm is adversely affected. There remains the challenge of studying and optimizing privacy-utility tradeoff especially in the case when statistical distributions of data are unknown. \textbf{We must, therefore, develop new differential privacy mechanisms that add the optimal amount of noise to prevent information leakage while still allowing for extracting useful machine-learning models from the data.}

\item {\textbf{Optimisation of vulnerability detection in the source code with static analysis.}} Detection of vulnerabilities in the source code of applications is achieved though static analysis. This approach generates a huge amount of information to the user. On the one hand, a single vulnerability can be detected in different sequences of code, each of which will generate a warning to the user and result in duplicate warnings. On the other hand, when there is a doubt that a code sequence is suspicious, a warning will also be raised to the user, possibly generating a false positive. A manual step is then needed to analyse the local context of each potential vulnerability and qualify the warnings before they can be used to fix real security issues. When it comes to 
self-healing and big data analytics applications, this manual phase must be automated and additional information is required from static analysis to feed the self-healing process with relevant qualified decision making information.
\textbf{We must ensure that static code analysis provides relevant and qualified decision making information to optimise self-healing and vulnerability detection.} 

\item \textbf{Refactoring Tool-Support for Security Programming.}
Writing applications that are inherently secure is a challenging problem for the average developer, who typically lacks the required skill-set to ensure code is secure. This usually requires not only in-depth knowledge about the code itself, but also in-depth knowledge about security techniques. Manually rewriting applications to make them secure means that there is more risk for things to go wrong: key algorithmic properties of security may be missed, or worse, may be implemented incorrectly, resulting in code that is even less secure than its starting point. Providing the developer with refactoring tool-support means that they can rely on the tool to aid them in rewriting the application to be more secure, where the refactorings abstract away the low-level complexities of writing secure algorithms. 
\textbf{We must provide new techniques and tools to enable developers to refactor their code to make it more secure.}

\item \textbf{Formal Methods.}
Formal methods include a variety of approaches to reason about the behaviour
of computational entities by modelling it logically or mathematically. For example, by modelling the behaviour of a piece of code and the abilities of adversaries, we can prove that the code is secure against all possible attacks (up to modelling assumptions). In the context of refactoring, we can prove that the (functional) behaviour of refactorings
is (observationally) equivalent. We can further verify refactorings against security requirements of interest, and identify a refactoring which satisfies a security property automatically in an approach towards correct self-healing.
%Given two pieces of code \textsf{A} and \textsf{B}  even though functional properties are satisfied by  and that the properties satisfied by them 
Formal methods
are the only currently-known approaches that can provide strong end-to-end security and privacy guarantees throughout system execution and across levels of abstraction. However, significant challenges remain to be able to
successfully use formal methods for security and privacy assurance in practice.
Some security goals may be logically complex (e.g. non-interference) and hard to formulate (e.g. privacy concerns).
\textbf{We need scalable and flexible formal methods that can capture different abstractions in real systems, can be used to prove the correctness of refactorings and self-healing, and can adapt to the environment, security and privacy goals of systems.}
%They have indeed had significant success overthe years in tackling security related problems, such as reasoning about the correctness of cryptographic algorithms or specifying and enforcing privacy guarantees .

\item \textbf{Intelligent Identity Management.} 
Identity management is a cornerstone of security in modern computing systems. Two important quality dimensions of effective identity management relates to its \textit{security aspects} for the service-provider and \textit{usability aspects} for the end users. The security level determines its strength against adversary attacks, whereas usability levels are commonly determined by task execution efficiency and effectiveness. However, identity management has become a complex and time-consuming task for any organization today due to constant cybersecurity threats and strict regulatory, and new directives such as the Payment Services Directive (PSD2) that require strong, multi-factor authentication solutions. At the same time, end users demand a seamless security experience with regards to identity, authentication and access management. In addition, there is an increased need for service providers to continuously verify, after successful login, the identity of the user to prevent fraudulent behavior, \textit{e.g.}, verifying that the voice of an online banking customer aligns to the authenticated customer. \textbf{We must, therefore, provide new methods and tools to enable the software developer to easily integrate, deploy and manage multi-factor authentication, continuous user identification, and end user access management solutions by reducing the complexity of development, but at the same time provide a positive security experience to the end users.}

\end{itemize}


The challenges identified above require a new approach that tackles these issues in a coherent and
holistic way. 

\subsubsection{Achieving The \TheProject{} Vision}
\label{sect:elysian_vision}

\begin{figure}[tph!]
  \begin{center}
  \vspace{-5mm}
  \includeimage[scale=0.55]{DigitalFortress-ConceptualDesign-v4.png}
%  \vspace{-3cm}
  \caption{Overview of the \TheProject{} Methodology}
  \label{fig:overview}
  \end{center}
  \end{figure}

Figure~\ref{fig:overview} gives a high-level vision of the \TheProject{} methodology for authenticating and developing secure and privacy-preserving applications that use distributed AI-based data analytics. Our assumption is that the end user starts with their code that is based on data analytics, but for which safety properties have not been firmly established. The users will be presented with the \TheProject{} tool-chain, supported by the visual front-end developed by \USTANshort{} and \YAGshort{}. The code development will proceed in a number of phases.

\begin{itemize}
\item \emph{Vulnerabilities detection phase}, where the tool chain will identify potential vulnerabilities in the code, both in terms of potential targets for cyber attacks (security) and in terms of potential leakage of information (privacy). The identification will combine static code analysis that will be developed by \YAGshort{} and \UCMshort{}, dynamic symbolic code execution mechanisms that will be developed by \IBMshort{} and machine-learning based run-time monitoring that will be developed by \SCCHshort{}. Combining these three complementary methods will allow us to discover a wide range of potential vulnerabilities in the code, as well as to avoid the common problems when each of them is used alone (such false positives, time complexity and vulnerabilities that arise at run-time). The general techniques for detecting vulnerabilities will be adapted to the specific problems of data storage and processing in distributed data analytics by \UODshort{} and \SCCHshort{}. 

\item \emph{Self-healing phase}, where the identified vulnerabilities will be repaired using a combination of formally verified semi-automatic  refactoring techniques that will be developed by \SAshort{}, and run-time adaptation based on run-time monitoring that will be developed by \SCCHshort{}. Our aim here is to provide both source-to-source code transformations, the output of which will be the repaired code that is semantically (functionally) equivalent to the original code, but with the identified (static) vulnerabilities repaired and also run-time repairing that will, over a series of steps, restore the system into a stable and secure state where cyberattacks are happening at run-time (e.g. man-in-the-middle attacks). In the process of self-healing, we will also apply novel methods and techniques for optimising the privacy-preserving data release mechanisms to address the issue of privacy-leakage. The developed core methods will be adapted to the problem of AI-based distributed data analytics by \UODshort{}. 

\item \emph{Formal verification phase}, where we will use formal methods on the resulting code to verify that the required security properties of the code are satisfied. We will also use static code analysis to verify that the code conforms to the required security standards. We will build on existing security standards 
%such as \textcolor{red}{XX} 
and extend them with new rules, applicable to the problem of distributed data analytics. The formal verification models will be built by \SCCHshort{} and \SAshort{}, based fundamentally on abstract state machines (ASMs) expanding available tools that automatically extract ASMs from source code, while the analysis for the conformance to standards will be developed by \UCMshort{} and \YAGshort{}. The output of this phase will be the final secure code, together with a \emph{digital certificate} specifying what standards the code conforms to and the security properties that it satisfies. If a digital certificate cannot be obtained, either because there are problems with conformance to standards and/or security properties, then the code will be fed back to the \emph{self-healing phase} until it does.
\end{itemize}

In addition to security vulnerabilities, \TheProject{} will also address \emph{privacy} concerns. Distributed data analytics, with multiple processes % agents 
fetching data from multiple storage sites and exchanging data between themselves, can lead to a leakage of confidential data. This is especially important in light of EU regulations such as the General Data Protection Regulation (GDPR). Our mechanisms for identifying and repairing vulnerabilities will ensure that the %end user 
code conforms to the parts of the GDPR regulation that are relevant to data movement between sites (e.g. Articles 20, 25, 33 and 37). 

Finally, to add an additional layer of security, we will specifically address the problem of \emph{identity management}, by developing and integrating novel and secure authentication and continuous user identification mechanisms, by developing and integrating blockchain-based user access control, and by providing diagnostics and repairing of the authentication mechanisms that are already a part of distributed data analytics. These parts will be led by \COGNIshort{}. All of the developed techniques will be demonstrated on the identified use cases in the areas of air traffic management (\FRQshort{}), digital banking (\SOPRAshort{}) and healthcare (\SOPRAshort{}). 

\begin{figure}[tph!]
  \begin{center}
  \vspace{-19mm}
  \includeimage[scale=0.5]{Elysian-DataStorage-v1.pdf}
  %DigitalFortress-WP2.pdf}
  \vspace{-2.3cm}
  \caption{Data Storage and Processing in \TheProject{}}
  \label{fig:storageprocessing}
  \end{center}
  \end{figure}

Figure~\ref{fig:storageprocessing} shows a more detailed view of the different software layers that a typical big data analytics application is built on. Each of these layers presents its own potential security issues that have to be considered in the \TheProject{} project: 
\begin{itemize}
\item At the lowest level, the data that needs to be analysed are usually stored in distributed databases or filesystems that can span multiple sites and organisations. Each of the individual nodes that stores the data will have a local database/filesystem manager process through which the communication with other nodes commences. This will mostly be invisible to the actual end user application, but it has to be addressed as a source of potential security vulnerabilities. The lowest level accessible to the application is the level of language drivers and APIs to the distributed database/filesystem. Through the calls to this API, the application communicates with different nodes of the distributed database/filesystem. The examples of this layer are Hadoop HDFS (\url{https://hadoop.apache.org}) distributed filesystem bindings for Java/C++ and Cassandra API. 
\item On top of the data layer is a layer of parallel patterns for distributed computations. Parallel patterns are usually implemented as high-order functions that implement common parallel behaviour, with the most well-known example being Google MapReduce~\cite{mapreduce}. Parallel patterns allow the end users to specify just the problem-specific parts of their computation, while they get the benefits of its parallel execution (and, hence, faster response times) for free. Unfortunately, parallel patterns are another possible source of vulnerabilities, due to the fact that they involve intensive communication and exchange of data between different distributed nodes. 
\item On top of the parallel patterns, there are domain-specific machine learning libraries (such as Keras (\url{keras.io}) and Spark MLLib) that implement specific %machine-learning 
techniques in terms of instances of parallel patterns. These libraries combine the data processing and data storage frameworks and potentially expose additional vulnerabilities. 
\item Finally, the end user applications combine all these layers to conduct specific data analysis that is needed. 
\end{itemize}
\TheProject{} aims to investigate sources of vulnerabilities and the transformations required to mitigate them in each of these different layers, as well as the vulnerabilities that arise from interactions between different layers. In addition to that, \TheProject{} will also develop \emph{secure software design patterns} that will give guidelines about secure combining of different layers of abstraction. These design patterns, together with analysis for conformance to the secure coding standards, formal verification of security \& privacy properties and refactorings, vulnerabilities detection and self-healing mechanisms, will ensure trustworthiness of the security 
of the end data analytics product.

\begin{mdframed}[backgroundcolor=blue!5]
\textbf{Security in AI and AI for Security.}
The main target of the \TheProject{} project are AI-based big data analytics applications, with use cases being applications of advanced machine-learning techniques for analysing distributed datasets. In this respect, \TheProject{} aims to develop and apply security and privacy vulnerabilities detection and repairing to the problems from the AI domain. At the same time, many of the technologies that we will develop for vulnerabilities detection and repairing will themselves use advanced machine learning techniques, e.g.~in developing stochastic models governing the mappings between sensitive/private data and the released data. Information-theoretic machine learning techniques will be developed to quantify the privacy-leakage in terms of mutual information between private data and released data. Supervised machine learning will also be used to automate the post processing of the huge amount of information that static analysis raises and to reduce the amount of SAST false positives, greatly reducing the cost of analysis that has so far been performed mostly manually. So, in the \TheProject{}, we are truly combining 'Security for AI' and 'AI for Security'.
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\textbf{Programming Languages Support.} \TheProject{} mainly targets big data analytics applications written in C++, which is heavily used in many of the infrastructures for  big data processing, including state-of-the-art distributed databases such as MongoDB and is known to deliver excellent performance on parallel hardware. %Furthermore, two out of three 
Two of our proposed use-cases are based on C++. However, taking into account the popularity of Java and Python in big data analytics, \TheProject{}  will also target Java, and to a smaller extent Python. 
Our formal approach is language independent.
%will cover all languages. 
The table below lists the languages that different technologies in  \TheProject{} will support. % over the course of

\begin{tabular}{|c|c|c|c||c|c|c|c|}
\hline \hline 
\textbf{Technology} & \textbf{C++} & \textbf{Java} & \textbf{Python} & \textbf{Technology} & \textbf{C++} & \textbf{Java} & \textbf{Python}  \\
\hline
Static Vulnerability Detection & \checkmark & \checkmark & \checkmark & Identity Management & \checkmark & \checkmark & \checkmark \\
Dynamic Vulnerability Detection & \checkmark & & & Security Patterns & \checkmark & \checkmark &  \\
Runtime Monitoring & \checkmark & \checkmark & \checkmark & User Interface & \checkmark & \checkmark & \checkmark \\
Refactoring for Self-Healing & \checkmark & \checkmark & & Coding Standards & \checkmark &  &  \\
Runtime Self-Healing & \checkmark & \checkmark & \checkmark & \TheProject{} Methodology &\checkmark & \checkmark & \checkmark \\
Informational Privacy & \checkmark & \checkmark & \checkmark & Formal Verification & \checkmark & \checkmark & \checkmark \\
\hline \hline 
\end{tabular}
\end{mdframed}


%\subsubsection*{Key assumptions}

%\vjcomment{This may not be necessary.}

%\TheProject{} makes a number of fundamental assumptions that will be tested and verified in the course of the project,
%and which form the basis for a register of technical risk.  The most significant assumptions are:

%\begin{enumerate}[{A}1)]
%\item XX
%\end{enumerate}

\subsubsection*{Transdisciplinary concerns}
\TheProject{} will bring together many different fields in computing, such as security, code analysis and transformation, formal methods, software engineering, parallel programming and artificial intelligence. Cooperation between these different areas will allow us to bring the strengths of each of them into our technologies, e.g.~by combining rigorous methods for proving properties of the code with usually less formal code transformation techniques. Furthermore, the use cases that will be developed over the course of the project belong to significantly different domains and the application of our technologies to them will enable interdisciplinary research and applications, combining computer science with healthcare, air-traffic management and economy. Finally, all of our technologies will be user centred, with the main aim being the ease of use by non-experts in security and/or large-scale distributed and parallel processing. Massive Open Online Courses (MOOCs) that we will develop will allow application of educational technology to promote the exchange of knowledge and to train the application domain experts in secure large-scale programming. 

\subsubsection{Positioning of the project}
\eucommentary{Describe the positioning of the project e.g. where it is situated in the spectrum from 'idea to application', or from 'lab to market'. Refer to Technology Readiness Levels where relevant.}

In line with the expectations of the H2020-SU-DS-2020 call, \TheProject{} aims
to achieve overall Technology Readiness Level (TRL) 5-6 (``technology
validated in relevant environment (industrially relevant environment in
the case of key enabling technologies)'').

\begin{center}
  \begin{tabular}{|p{4.9in}|l|l|}
    \hline
    \textbf{Key Enabling Technology} & \textbf{Current TRL} & \textbf{Final TRL} \\
    \hline
%     &  & \\
    \hline USTAN ParaFormance refactoring tool-suite & TRL 4-5 & TRL 5-6 \\  
    \hline YAG YAG-Suite vulnerability detection tool - Machine learning augmented SAST for security and privacy breaches in Java, C/C++ and Python source code & TRL 4 & TRL 6 \\  
    \hline YAG YAG-Suite vulnerability detection tool - Code mining & TRL 5 & TRL 7 \\  
    \hline SCCH eKnows software analytics tool - Module for semi-automated extraction of ASMs from source code & TRL 4 & TRL 6 \\
    \hline IBM ExpliSAT Symbolic Interpretation Tool for security vulnerability detection in C/C++ code & TRL 4 & TRL 6 \\
    \hline COGNI Cognitive Identity - Integration and management of multi-factor authentication and continuous user identification methods & TRL 4 & TRL 6 \\
    \hline COGNI Face and Eye Gaze Analytics - Intelligent face and eye gaze data analytics for continuous user identification & TRL 4 & TRL 6 \\
    \hline
  \end{tabular}
\end{center}

%\noindent
%The specific advances that will be made are described in more detail in Section~\ref{sec:novelty} (Page~\pageref{sec:novelty}).


\subsubsection{Linked research and innovation activities}
\label{projects}

\eucommentary{Describe any national or international research and innovation activities which will be linked with the project, especially where the outputs from these will feed into the project;}


%\vspace{-8pt}
\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\teamplay (ICT-779882).}
The TeamPlay project aims to develop new, formally-motivated techniques that will allow execution time, energy usage, security, and other important non-functional properties of parallel software to be treated effectively as first-class citizens. This methodology was built into a toolbox for developing highly parallel software for low-energy systems, as required by the internet of things, cyber-physical systems etc. The TeamPlay approach allows programs to reflect directly on their own time, energy consumption, security, etc., as well as enabling the developer to reason about both the functional and the non-functional properties of their software at the source code level. \emph{Instead, \TheProject{} takes a different direction, focusing on formally verified refactorings that satisfy security goals in making code secure and contribute to a self-healing approach. Also, \TheProject{} focuses purely on security issues, specifically aimed at big data, rather than the embedded domain. %Furthermore \TheProject{} aims to address wider issues in security, by developing new tools and techniques that apply at different levels of abstraction. 
%the software development cycle pertaining to security. 
Finally, \TheProject{} will produce new formal techniques for reasoning about secure transformations across system layers.  }

\end{mdframed}

%...
%\vspace{-8pt}

%%\begin{mdframed}[backgroundcolor=blue!5]
%%\paragraph{\rephrase (ICT-644235).}
%\vspace{-12pt}

%%The \rephrase project aims to study the software engineering process as
%%a whole for heterogeneous parallel machines
%%using C++.  It considers neglected, but important, issues such as
%%effective testing, debugging, maintenance and quality assurance for
%%multicore/manycore machines, and involves major industry players such as IBM.
%%\emph{\TheProject will, however, substantially extend the work that has been done in \rephrase by developing new refactorings that transform the code in a way that makes it more secure; producing new tools and techniques that discovery security vulnerabilities; produce new formal techniques to reason about the general soundness and correctness of the refactorings and vulnerability methods.}
%%\end{mdframed}

%\vspace{-2pt}

%%\begin{mdframed}[backgroundcolor=blue!5]
%%\paragraph{\paraphrase (ICT-288570).}
%\vspace{-12pt}
% \TOWRITE{CB,VJ}{Write about ParaPhrase} 
%%The \paraphrase project introduced a new structured design and implementation process for
%%heterogeneous multicore/manycore architectures, in which developers exploit a variety of
%%parallel patterns to develop component based applications in Erlang and C++. These
%%component based pattern-applications may then be re-mapped to meet the
%%application requirements and hardware availability.
%%\emph{Unlike \TheProject{}, \paraphrase did not develop new tools or techniques for any aspect of security, only focussing on multi-core development instead.}
%%\end{mdframed}

%\vspace{-2pt}
\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\grow (EU No.690199).}
The \grow project is the first continental-scale Citizens’ Observatory to monitor a key parameter for science, continuously over an extended period, and at an unmatched spatial density. \grow has, for the first time, used crowd sourced ground observations from low-cost sensors to validate soil moisture information from satellites, including the new generation of high-resolution satellites, Sentinel-1. 24 \grow communities in 13 European countries create an unprecedented network of 6,502 ground-based soil sensors and a data set of 516M rows of soil data. The communities where supported by a MOOC educational program and a series of workshops. \emph{\TheProject{} will learn from and build on the educational program of \grow, building on the concept of a MOOC and workshops for end users and the programming community at large.}
\end{mdframed}

%%\begin{mdframed}[backgroundcolor=blue!5]
%%\paragraph{\weobserve (EU No.776740).}
%%\textbf{WeObserve} is an umbrella project promoting the use of Citizen Observatories for environmental monitoring.  This innovative project aims at demonstrating the societal and economic benefits of involving citizens in environmental decision making and cooperative planning, supporting Europe’s leading role in integrating citizen science and building resilient communities. Together, these projects will empower and enable citizens to become the ‘eyes’ of the policy makers and to complement existing environmental monitoring systems.\emph{\TheProject has a very different scope and focus to \weobserve but none the less there are lessons to be learnt form that project in terms of interoperabilty, standards, dissemination and the promotion of good practice.}
%%\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\satie (EU No.832969).}
\satie adopts a holistic approach to threat prevention, detection, response and mitigation in the airports, whilst guaranteeing the protection of critical systems, sensitive data and passengers. \FRQshort{} provided a use case for this project.  In order to handle a combination of physical and cyber threats, SATIE develops an inter operable toolkit which improves cyber-physical correlations, forensics investigations and dynamic impact assessment at airports. Having a shared situational awareness, security practitioners and airport managers collaborate more efficiently to the crisis resolution. Emergency procedures can be triggered simultaneously through an alerting system in order to reschedule airside/landside operations, notify first responders, cybersecurity and maintenance teams towards a fast recovery.
\emph{\TheProject will substantially extend the work that has been done on a deeper technical level based on the information platform MosaiX.}
\end{mdframed}

%%\begin{mdframed}[backgroundcolor=blue!5]
%%\paragraph{\sinapse (EU No.892002).}
%% The \sinapse project aims at proposing an intelligent and secured aeronautical datalink communications network architecture design based on the Software Defined Networking (SDN) architecture model augmented with Artificial Intelligence (AI) to predict and prevent safety services outages, to optimize available network resources and to implement cybersecurity functions protecting the network against digital attacks. The SDN controller will be built on the existing \FRQshort{} NetBroker products. It is based on the open source ONOS SDN framework with some ATM-Grade Network specific applications added. \emph{Even if \TheProject has a different scope, it will track the results and lessons learned from \sinapse and record them if necessary.}
%%\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\blogdas (FWF: [P26452-N15]).} 
\blogdas was supported by the Austrian Science Fund and it developed a behavioural theory of distributed adaptive systems and an associated abstract machine model based logic. The theory comprises: i) a general language-independent characterisation by a set of intuitive postulates that are satisfied by all known system formalisms; ii) an abstract machine model that provably satisfies the postulates; and, iii) the mathematical proof that all systems stipulated by the postulates can be faithfully represented by the abstract machine model. The behavioural theory enables system specifications in general using a concrete language for the abstract machine model. The logic allows system developers to formally characterise desirable properties that system specifications must meet and to mathematically verify these properties. %In particular, static verification with this logic permits to give assertions for states that result after many adaptations of the system.
\emph{The results of this project overcome the imminent restriction of using interleaving to circumvent true asynchronous behaviour, providing better suited theoretical foundations and tools (concurrent ASMs and logics) to formalise and verify contracts for distributed systems as required for WP4 as well as to specify anomaly detection algorithms and associated repairability proof obligations within the context of WP2.}  
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\primal (FFG: 873979).}
The \primal project, supported by the Austrian Research Promotion Agency, is concerned with the development of privacy-preserving machine learning industrial applications. The project focuses on the development of algorithms and a software framework for differentially private distributed deep learning, transfer learning, and multi-task learning with applications to industrial use cases. \emph{\TheProject{} will go beyond \primal in considering an information theoretic approach to optimise privacy-leakage in distributed AI.}
 \end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\sthreeai (FFG: 872172).} The Austrian Research Promotion Agency supported \sthreeai project for building secure collaborative artificial intelligence systems ensuring privacy, protection against hostile attacks and guaranteeing for the intended performance of the system. \sthreeai will develop theoretical frameworks and analysis tools interfacing with mathematics, deep learning and information security regarding new deep model architectures and related privacy learning strategies, new defence strategies against enemy attacks, and new methods for assessing trustworthiness. \emph{\sthreeai has synergy with \TheProject{} regarding privacy-preserving deep learning, however, unlike \TheProject{}, \sthreeai does not consider an information theoretic approach to study the concepts of privacy-leakage and knowledge-transferability across distributed parties in a rigorous and unified manner}.    
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\serums (EU 826278).} \serums is coordinated by \USTANshort{} and also includes \SOPRAshort{}, \IBMshort{} and \SCCHshort{} in the consortium. \serums aims to develop new smart, patient-centric healthcare systems that integrate home, workplace, and personal data and medical care with centralised hospital, special consultant and general practitioner provision. The goal of the \serums project is to put patients at the centre of future healthcare provision, enhancing their personal care and maximising the quality of treatment they can receive, while ensuring trust in the security and privacy of their confidential medical data. Like \TheProject{}, \serums also targets distributed systems where confidential data might be exchanged between different devices over untrusted networks. The focus of \serums is, however, on privacy of the data in the healthcare domain. %The data analytics aspect is not very prominent there and it does not consider cloud environments. 
\emph{In contrast, \TheProject{} considers much larger scale of hardware, is focused on data analytics executed in cloud settings and also addresses the crucial problem of security in terms of cyber threats and attacks. \TheProject{} brings an emphasis on the correctness of self-healing and secure code refactorings, with a powerful formal framework that tackles verification across system layers. \TheProject{} also considers a wider range of application domains, having use cases in banking and air traffic management.}

\end{mdframed}


%%\begin{mdframed}[backgroundcolor=blue!5]
%%\paragraph{YAG-Suite (\YAGshort{} R and D) } The YAG-Suite a is product developed by \YAGshort{} and resulting from the company's research and innovation project. The project started from scratch and developed a new source code scanning technology which detects and identify vulnerabilities in the source code with SAST and bring new automation capabilities to qualify the warnings raising from SAST with machine learning, in a supervised training approach. The machine learning allows to better contextualize the vulnerability detection as well as it significantly reduces the false positive rates and provide decision making information to the user for cost efficient remediation.    
%%The company has been labelled "Jeune Entreprise Innovante" (Young Innovative Company). The project has been partially subsidised by BPI France for its feasibility phase, as well as from the Region of Brittany and Rennes Métropole. The project also received partial support with industry loans from TOTAL Développement Régional and AIRBUS Développement. 
%%\end{mdframed}

\subsubsection{Overall approach and methodology}

\eucommentary{Describe and explain the overall approach and methodology, distinguishing, as appropriate, activities indicated in the relevant section of the work programme, e.g. for research, demonstration, piloting, first market replication, etc.;}

The development of the \TheProject{} technologies will be user-driven, with a primary consideration being the applicability to the realistic use cases and usability by the end users, which are in our case developers of big data analytics applications. The project will start with a short requirements capturing phase, where we will make decisions about the storage, processing and machine-learning libraries that will be considered in the project, and we will scope out the plan for each of the technologies and their interaction in the overall methodology. These requirements will be revisited and refined after the first phase of the project, taking into account the experience and the results of that phase.

The \TheProject{} technologies themselves will be developed and evaluated in three incremental phases. In the \emph{first} phase, we will develop the initial versions of the tools and techniques, link them together in the initial version of the \TheProject{} methodology for developing secure big data analytics and evaluate them on the initial versions of the use cases. In this phase, we will limit ourselves to the security issues arising from deployment of applications on private clouds. This will allow us to develop initial versions of the tools in a more tractable setting, where fewer opportunities for cyber attacks may arise. In the \emph{second} phase, we will extend the technologies, project methodology and use cases to target the deployment in public clouds, where far more cyberthreats are present. Finally, in the \emph{third} and final phase, we will develop and evaluate the final versions of the techniques, methodology and use cases, targeting the deployment on a hybrid clouds that span both private and public clouds, therefore allowing more possible threats than either of these settings alone. With this in mind, we will aim to use the Agile development as our primary software-engineering methodology, allowing for each technology to use its own methodology that is most suitable for development.

\TheProject{} comprises 6 technical work packages: WP2 on foundational mechanisms for identifying security risks and self-healing; WP3 on security and privacy in the context of distributed AI-based big data analytics; WP4 on establishing security properties of the code, including formal verification and analysis for compliance to standards; WP5 on intelligent identity management; WP6 on methodology for developing secure applications by end users; and WP7 on use cases and evaluation. There are also WP8 on dissemination, exploitation and community building and WP1 on project management. The relationship between the project objectives and these work packages is shown below

\vspace{-8pt}
\begin{center}
\begin{tabular}{|l|l|l|}\hline
\textbf{Objective} & \textbf{Purpose} & \textbf{Contributing WPs} \\\hline \hline
Objective 1 & Secure Patterns for Distributed AI-based Data Analytics & \textbf{WP3}, \textbf{WP4}, \textbf{WP6} \\\hline
Objective 2 & Identifying Vulnerabilities and Privacy Leakage & \textbf{WP2}, \textbf{WP3}, \textbf{WP4} \\\hline
Objective 3 & Self-Healing Techniques for Repairing Vulnerabilities and Leakage & \textbf{WP2}, \textbf{WP3}, \textbf{WP4} \\\hline
Objective 4 & Coding Standards for Developing Secure Distributed Applications & \textbf{WP4}, \textbf{WP6} \\\hline
Objective 5 & Intelligent Identity Management and Trust & \textbf{WP5} \\\hline
Objective 6 & Refactoring-Based Methodology for Secure Applications & \textbf{WP3}, \textbf{WP4}, \textbf{WP5}, \textbf{WP6} \\\hline
Objective 7 & Demonstrating the Tools and Techniques on Real-World Use Cases & \textbf{WP6}, \textbf{WP7}\\\hline
Objective 8 & Building a Sustainable User Community & \textbf{WP8}\\\hline
\end{tabular}
\end{center}

\paragraph*{Work Programme for Objective 1.}

Objective 1 aims to build secure patterns for distributed AI-based data analytics and to develop methods for formal specification and verification of security properties of those patterns. In WP3, we will develop methods for identifying and repairing security and privacy vulnerabilities for distributed AI-based data analytics (based on foundational techniques that will be developed in WP2). This will serve as a basis for the development of secure patterns that will be done in WP6 as a part of the \TheProject{} methodology for developing secure end user applications. In WP4 we will develop the methods for formal specification and verification of security and privacy properties of the underlying source-code, in the form of \emph{security certificates} that will specify these properties at a high level and also \emph{security contracts} that will be machine readable and, therefore, amenable for formal proving. 

\paragraph*{Work Programme for Objective 2.}

Objective 2 aims to develop techniques for identifying security and privacy vulnerabilities in C++ and Java code. In WP2, we will develop fundamental techniques for identifying security vulnerabilities, based on source code analysis, dynamic symbolic execution and run-time monitoring. In WP3, we will apply these techniques to identifying security vulnerabilities in distributed data storage frameworks (distributed databases and filesystems) and in machine-learning based distributed data analytics applications based on parallel pattern-based data processing frameworks. WP3 will also investigate quantifying potential privacy leakage in machine-learning applications and in exchange of knowledge built using these models. The developed diagnostics for identifying vulnerabilities and data leakages will be integrated into the overall \TheProject{} methodology in WP6.

\paragraph{Work Programme for Objective 3.}

Objective 3 aims to develop self-healing mechanisms for repairing security vulnerabilities and privacy leakage. In WP4, we will develop foundational code transformations for self-healing. These transformations will be based on the identified vulnerabilities, as well as on the known secure code patterns for distributed data analytics, as developed in WP2. In WP3, we will develop techniques for applying these transformations both to the end user code that uses distributed databases for data storage and that uses common pattern-based data analytics frameworks such as MapReduce. WP3 will also develop techniques for controlling data leakage using optimal noise-adding mechanisms. In WP6, the transformations will be integrated into the software-refactoring based methodology for developing secure pattern-based distributed data analytics applications. 

\paragraph{Work Programme for Objective 4.}

Objective 4 aims to extend the existing code standards for developing secure and privacy-preserving distributed applications. In WP6, we will build static analysis techniques that will check the compliance of the C++ code to the security standards such as CERT SEI C++ Coding Standard, but we will also consider other coding standards such as CppCore Guidelines and High Integrity C++. Based on the methodology for developing secure applications that will be developed in WP6, and in particular based on the secure patterns for composing distributed big-data analytics applications from different layers of abstraction, we will also aim to augment these standards extended them with new coding rules as identified over the course of the project. The outcome of this (in WP6) will be the extended security standards and guidelines for developing secure, large-scale data applications that will be developed in both private and public clouds.

\paragraph{Work Programme for Objective 5.}

Objective 5 aims to analyse, design, develop and evaluate an intelligent and user-centered IDentity-as-a-Service (IDaaS) technology which will allow software developers to integrate, deploy and manage multi-factor authentication, continuous user identification, and user access management solutions by reducing the complexity of development. In WP5, a User-Centered Design (UCD) methodology will be followed for the design and development of the IDaaS technology aiming to assure that the solution meets the end users' needs and expectations in terms of usability and security. The development life-cycle of the identity, authentication and user access management components will include multiple design iterations and a significant amount of evaluation, which will be largely based on studies with the active participation of real end users. These studies will be both formative (during the project’s course, with the aim of validating/refining the initial design) and summative (towards the end of the project, aiming to measure the quality and effectiveness of the final project results). The UCD approach will embrace three iterative cycles leading to three different system prototype releases (low-fidelity, high-fidelity, final release). Each iteration entails an analysis, design, implementation and integration, ending with an evaluation of derived prototypes providing thus valuable feedback for designing the next release. 

\paragraph{Work Programme for Objective 6.} 

Objective 6  brings together the different techniques developed in other parts of the project into a coherent and tool-supported methodology for repairing the existing and developing new secure applications. WP6 will develop the methodology, outlining all the steps in the software engineering process of building secure large-scale distributed applications. In addition, it will integrate the tools developed in WP2--WP5 with a new user interface, allowing seamless access to the technologies for identification and repairing of security and privacy vulnerabilities, checking for compliance to the security standards, formal verification of the security properties, security contracts and user-centered identity management. The tool-chain will be based on our existing tools for code analysis (provided by \YAGshort{}) and software refactoring (provided by \SAshort{}), which will allow semi-automatic restructuring of the end user code while keeping the end user in the loop and allowing them input into the transformation and verification process, where they could supply their domain-specific knowledge to drive the process. Finally, in WP6 we will also apply the data fabrication technology from \IBMshort{} to allow rapid generation of large volumes of synthetic data to stress-test the large-scale data analytics systems, helping in both the development of the technologies and methodology, and in testing the end user applications.

\paragraph{Work Programme for Objective 7.} 

Objective 7 demonstrates effectiveness of the \TheProject{} approach on real-world use cases from air traffic management, %space telecommunications,
banking and medical domains. In WP6, we will develop a tool-chain that will integrate all the different techniques developed in WP2 -- WP5. This will be used in WP7 to evaluate the \TheProject{} technology  on use cases from \SOPRAshort{} and \FRQshort{}. We will apply our techniques both on analysing/repairing the existing and on developing completely new distributed code from the chosen domains. While our ultimate goal is to test the techniques in realistic setting, \emph{data fabrication} methods that will be developed as a part of the methodology will allow us to immediately produce arbitrary amount of realistic data for rapid development and testing of our techniques. We will demonstrate increased security of the end user software whilst at the same time decreasing cost of the production, deployment and maintenance of such a system.

\paragraph{Work Programme for Objective 8.}

Objective 8 ensures long-term uptake and use of \TheProject{} concepts and technologies. In WP8, we will undertake a range of dissemination and communication activities including producing publications, documents and other materials, giving presentations and demonstrations, running dedicated workshops and tutorials to encourage uptake of \TheProject{} technologies, establishing and maintaining a communication strategy, producing user-level documentation and training material, including tutorials, webinars, production and distribution of promotional and informational materials, including press releases, flyers etc., and updating and managing the web portal with user-related material.  In addition a MOOC will be created about the work of the project and will be hosted by Futurelean (a partner of \UODshort{}). This has the potential of reaching 10s of thousands of users.  Uptake and use will be assisted by the inclusion of the highly-relevant use cases from the air traffic management, %telecommunications, 
banking and medical domains.


\subsection{Ambition}

\eucommentary{Describe the advance your proposal would provide beyond the state-of-the-art, and the extent the proposed work is ambitious. Your answer could refer to the ground-breaking nature of the objectives, concepts involved, issues and problems to be addressed, and approaches and methods to be used.}

\subsubsection{Advances Beyond the State-of-the-Art}
\label{sec:novelty}
%\label{sect:background}
%\label{sect:state-of-the-art}

%\eucommentary{}


\TOWRITE{ALL}{Add sections on your own technologies/update
what's here.}

The \TheProject{} project will revolutionise the development of secure and privacy-preserving big-data applications that are deployed on clouds. We will significantly advance the state of the art in different areas related to the security of distributed applications, including identification of security vulnerabilities in large-scale distributed settings, semi-automatic repairing of vulnerabilities, privacy-preserving machine-learning, security of data storage and data processing frameworks, pattern-based design of secure applications, certification and assurance, and identity management. Collectively, the advances that are made by the \TheProject{} will:
\begin{itemize}
\item \textbf{reduce the number and impact of cybersecurity incidents} through our advanced security and privacy vulnerabilities detection and semi-automatic repairing, as well as novel user-centered identity management schemes;

\begin{mdframed}[backgroundcolor=blue!5]
The combination of different techniques for vulnerabilities detection will allow us both to identify a wider range of potential threats and to reduce false positives in vulnerabilities identification. It is well known that static analysis of the code is able to identify many of the potential threats, but also that it returns a large number of false positives. Combining static analysis with symbolic execution, as outlined in Section~\ref{sect:identifying}, will contribute to the reduction of these false positives by focusing on the parts of the code where these potential threats lie and profiling them to establish whether security is really breached. Additionally, run-time monitoring, which focuses on the state of the running program, as opposed to detection of particular types of threats, will allow us to detect both current and future vulnerabilities and reinstate the running code in a safe state.
\end{mdframed}

\item \textbf{increase trust} in the security of big-data analytics applications by providing formal guarantees of the security properties of the code and certification for compliance to the security standards;
\item \textbf{ensure privacy and allow safe knowledge exchange} in AI-based data analytics by quantifying and reducing privacy leaks and allowing safe transfer of learning;
\item \textbf{reduce the effort} required for securing applications and services by developing a methodology and tool-chain to support the end user and guide them through the process of developing secure applications;
\item \textbf{increase automation}  of security and privacy by design development processes by providing tools which covers vulnerability detection and refactoring as well as self adaptation with machine learning;


\end{itemize}

\subsubsection{Identifying Security Vulnerabilities in Applications}
\label{sect:background-first}
\label{sect:identifying}

\paragraph{Static Code Analysis.}
\vjcomment{\YAGshort{} and \IBMshort{} to write}
In the application life cycle, different techniques make it possible to build and test the cyber security of software, from early stages to stressing the application in production with ethical hacking. In order to minimise the overall costs of testing, source code analysis should be performed regularly in the development process, to ensure that the algorithms and the management of sensitive data and resources do not generate vulnerabilities. The only technique which can produce results when the project is not yet ready for compiling is SAST (Static Application Security Testing). Source code scanners, such as HCL-Appscan, Checkmarx, Kiuwan and Coverity, rely on SAST to detect suspicious sequences of code that may reveal a vulnerability.
%
State-of-the-art source code scanners all face the same problem - they must warn the user about every potential vulnerability but only when a relevant vulnerability is detected. In reality, vulnerabilities are not so easy to find, because of the complexity of vulnerability detection, and scanner editors prefer to raise potentially irrelevant warnings (false positive) rather than missing real vulnerabilities (true negatives), and therefore generate a lot of false positives. The problem becomes more prominent when a single vulnerability shows up in several parts of the code, thus generating duplicate warnings. The users then need to carry out additional manual investigations, which prove to be very time consuming. 
%The current economical efficiency of static analysis is therefore limited and the induced HR costs can be very significant if all warnings are checked for their relevance.
%
\YAGshort{} advanced the state-of-the-art with its new source code vulnerability detection technology, using SAST to analyse the source code, and machine learning to automatically process the resulting information. While SAST detects vulnerabilities and raises warnings based on specific vulnerability modelling and detection rules, the supervised machine learning qualifies each individual warning based on the users' feedback and allocates them relevancy and criticality scores. Compared to other techniques, it takes into account uncertain information, local and global context of the application and a first level of end user business semantics (especially for privacy vulnerabilities), while adapting to the end user context from users expertise. In addition, the technology supports a first level of \emph{code mining} (data mining applied to source code analysis). Code mining extracts high value technical, security and end user business oriented information from huge amount of source code to drive vulnerability detection and automated qualification.

\begin{figure}[tp]
  \begin{center}
  \vspace{-5mm}
  \includeimage[scale=0.20]{Yag-Suite Dashboard.png}
  %\includeimage[scale=1.0]{Yagaan_Audit_center_screenshot.png}
%  \vspace{-3cm}
  \caption{Executive dashboard of the YAG-Suite}
  %\caption{Java sensitive data exposure diagnostics in the YAG-Suite}
  \label{fig:yagoverview}
  \end{center}
  \end{figure}


%\begin{mdframed}[backgroundcolor=gray!10]
%\TheProject{} will advance the state of the art in the area of identification of security vulnerabilities in several ways:
%\begin{itemize}
%\item \emph{Machine learning augmented SAST for security and privacy breaches detection in Java, C/C++ and Python source code.} with significantly extending the technology to detect, analyze and qualify big data analytics applications vulnerabilities in Java, as well as to analyze C/C++ and Python languages.
%
%\item \emph{Code mining} will be enhanced to apply to big data analytics applications in Java, to feed dynamic analysis and self healing, as well as to support C/C++ and Python languages.
%\end{itemize}
%\end{mdframed}


\paragraph{Dynamic Analysis and Symbolic Execution.}
\vjcomment{\IBMshort{} to write}
Software vulnerabilities pose serious risk of exploit and result in system compromise, information leaks, or denial of service. As mentioned in the previous section, various static analysers are used to detect potential vulnerabilities in C/C++ code. Most of them suffer from high amount of false alarms mainly due to the fact that they do not use a symbolic execution approach but rather a more simplistic syntactical and semantical analysis of code constructs. This simplistic approach has the advantage of being more scalable as opposed to the inherent scalability deficiencies of symbolic execution which exhaustively analyses all potential execution paths. Furthermore the fact that real life programs call out to system functions and other functions residing in libraries, with no access to their source code, creates the need to generate abstractions or simulate their execution flows depending on the nature of the function. This entails a wide variety of challenges ranging from the mitigation of state explosion to simply being able to accurately simulate a large number of operations. There are a number of symbolic execution tools such as JPF for Java and Otter for C, both with limited capabilities to scale up to real life programs.
%
Dynamic analysis systems, such as “fuzzers” natively executes an application while providing ‘tailored’ inputs when flaws are detected, and significantly assist in reproducing and mitigating the flaws. 
%However, generating the tailored “input test cases” to drive execution, typically requires an exhaustive set of test cases that required manual effort to generate. 

%\begin{mdframed}[backgroundcolor=gray!10]
%\TheProject{} will advance the state of the art in the area of software vulnerability detection by combining symbolic execution with fuzzing and advanced static analysis capabilities. We will use a known analysis technique that is based on reachability analysis, and discard paths that cannot reach a target location. Then we plan to use static analysis augmented with heuristics to detect and abstract code segments that are deemed difficult for a symbolic execution and/or fuzzing to cope with, such as cryptographic code. A combination of symbolic execution and fuzzing approaches will help the symbolic execution engine to make progress in code segments that are difficult for symbolic interpretation due to the “path explosion” problem or missing source code. The overall goal is to create a combined tool with symbolic execution, fuzzing and static analysis capabilities that would outperform any other existing tool in the marketplace.

%\end{mdframed}


\paragraph{Run-time Analysis and Adaptation}
%\vjcomment{\SCCHshort{} to write}
%Current practice is for security to be ensured at run-time, using efficient monitoring techniques coupled with dynamic run-time checks such as ISR~\cite{isr}, ASLR~\cite{aslr} and CFI~\cite{cfi}. These methods are, however, expensive in terms of computational complexity and energy consumption. 
According to the general behavioural theory of concurrent systems~\cite{BorgerS16} concurrency can be captured by families of sequential ASMs indexed by agents, such that each agent proceeds stepwise evaluating its actions in a consistent virtual state and bringing in its updates into some later (not necessarily the next) virtual state. Extending the thesis such that families of parallel ASMs~\cite{FerrarottiSTW16} are captured is rather straightforward. Reflective (self-adaptive) extensions~\cite{abs-2001-01873} should not cause serious problems to be integrated, as the concurrency model does only depend on integrating results of steps of the involved agents. This provides the necessary theoretical foundations  needed to formally specify the novel algorithms that we envisage for run-time analysis and adaptation based defence of modern distributed data analytics systems. Among others we can capture within this model are the semantics of bulk synchronous parallel computations~\cite{FerrarottiGS19}, and thus the MapReduce algorithms which are prevalent in data analytics. 
%
The common state-of-the-art approach for security analysis (also at run-time) is to start from an attacker model and system vulnerabilities~\cite{Biskup09}. With respect to possible attacks at run-time the project will focus on unauthorised intrusion with the potential of causing (planned or carelessly) damage to the system, i.e. identity management and access control are primary objects of interest. For this the project will take an approach based on anomaly detection (see~\cite{BaileyCL14} for an overview concerning security breaches) and counter-measures based on the adaptation of the security controllers. The anomaly detection will be based on the learning of the protocol language used in the interaction with the system~\cite{Lampesberger13}, and the counter-measures will change the rules of the identity manager and the access control~\cite{BaileyCL14}. Furthermore, the research on anomaly detection will not only focus on the recognition of anomalies that may indicate unauthorised and potentially malicious intrusion and the adequate countermeasures that repair a violation, but also on the early anticipation of damaging activities and the start of counter-measures before any damage has occurred. 
%For instance, components may produce incorrect Byzantine behaviour or not react anymore. 
In order to cope with such vulnerabilities, the challenge is to identify  states that are close to states that will require counter-measures and sequences of update sets by individual agents that led to these states, such that the combination of both indicate the potential creation of a non-acceptable state within a small number of steps. 

%\begin{mdframed}[backgroundcolor=gray!10]
%\TheProject{} will advance the state of the art by addressing means for interaction and synchronisation, by which desirable security properties of the concurrent system at run-time can be enforced. For instance, by means of transaction operators~\cite{BorgerSW16} strict serialisability and recoverability can be enforced, but there are weaker properties that might be applied to ensure exclusive or priority access to shared locations. Also messaging instead of location sharing will be exploited, for which it will be shown how to integrate the interface theory for concurrent systems developed in~\cite{BauerHW11}. By means of sharing, messaging and synchronisation monitoring algorithms (e.g., see~\cite{DiekertL14} for the handling of run-time verification and monitoring) for the detection of security risks situations will be integrated into the systems. \TheProject{} will further advance the state of the art concerning the handling of security requirements by adding preemptive counter-measures.
%\end{mdframed}



%\paragraph{Security of Multi-Library Applications}
%Including third party libraries into the application code presents additional security risks, as these libraries may include intentional or unintentional vulnerabilities resulting in unwanted behaviour or data leakage of sensitive  information. For example, an open source JavaScript library used in a website may contain malicious code that collects data and sends it to the third party. This is especially problematic for large-scale big data analysis applications, as they are typically composed of calls to several different libraries, as outlined in Section~\ref{sec:XX}. The state of the art solutions to this problem is to use security testing, static analysis and dynamic analysis, such as sandboxing~\cite{jsand}, to detect vulnerabilities and malicious packages before integrating them into the application code. In~\cite{Cova}, a combination of anomaly detection and emulation systems is built to detect malicious code. OWASP~\cite{OWASP} presents the risks in using third party  packages, how to determine if these packages are vulnerable or contain malicious code and how to deploy these packages. Other possible solutions to the problem of integrating third party libraries are based on analysis of the libraries, building control flow graph, looking for copies of buggy code, and profiling packages~\cite{Hanna, XinSun}. These solutions miss many vulnerabilities or malicious dormant code as a result of obfuscation, packing, and the ease in rebuilding new variants of the malicious code, or just the rapid release of new buggy version of packages. There are cases in which it is not beneficial to fix a vulnerability in the software or in a third party code integrated into the software. For example, in cases where the patch is hard to deploy or where fixing the vulnerability has significant impact on performance. In fact, 99\% of attacks are believed to exploit known and fixed vulnerability~\cite{GartnerVulnerability} for which the fix was not deployed. One famous example is the WannaCry attack that was based on a fixed and updated CVE. To protect the software in these cases, \emph{virtual patching} may be used. A virtual patch is a set of rules that implements complex logic to prevent malicious traffic from reaching the application. The challenge is to come up with a set of accurate rules that filter exactly the malicious inputs without blocking other inputs. This work is currently done manually, and it would heavily benefit from integrating a predictive model to automatically detect inputs that may utilise the vulnerability.
\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of identification of security vulnerabilities in several ways:
\begin{itemize}
\item \emph{Machine-learning augmented SAST for security and privacy breaches.} We will significantly extend our technology to detect, analyse and qualify application vulnerabilities, extending our tooling to target the big data analytics applications written in Java, C/C++ and Python. We will also enhance the \emph{code mining}, applying it to big data analytics applications in Java, to feed dynamic analysis and self healing, as well as to support C/C++ and Python languages.
\item \emph{Combining symbolic execution with fuzzing and advanced static analysis capabilities.} We will use a known analysis technique that is based on reachability analysis, and discard paths that cannot reach a target location. Then we plan to use static analysis augmented with heuristics to detect and abstract code segments that are deemed difficult for a symbolic execution and/or fuzzing to cope with, such as cryptographic code. A combination of symbolic execution and fuzzing approaches will help the symbolic execution engine to make progress in code segments that are difficult for symbolic interpretation due to the “path explosion” problem or missing source code. The overall goal is to create a combined tool with symbolic execution, fuzzing and static analysis capabilities that would outperform any other existing tool in the marketplace.
\item \emph{Identifying what desirable security properties of the concurrent systems can be enforced at run-time.} For instance, by means of transaction operators~\cite{BorgerSW16} strict serialisability and recoverability can be enforced, but there are weaker properties that might be applied to ensure exclusive or priority access to shared locations. Also messaging instead of location sharing will be exploited, for which it will be shown how to integrate the interface theory for concurrent systems developed in~\cite{BauerHW11}. By means of sharing, messaging and synchronisation monitoring algorithms (cf.~\cite{DiekertL14} for the handling of run-time verification and monitoring) for the detection of security risks situations will be integrated into the systems. \TheProject{} will further advance the state of the art concerning the handling of security requirements by adding preemptive counter-measures.
\end{itemize}
\end{mdframed}


\subsubsection{Security in Pattern-Based Big Data Analytics}
\label{sect:patterns}
%\vjcomment{UOD to write}

The concept of \emph{parallel patterns} dates back to the early 90s when it was first applied to distributed programming based on MPI~\cite{cole89skeletons}. A parallel pattern is a higher-order function that implements some common parallel behaviour (e.g.~independent processing of different parts of some data structure), doing all the work related to creation, synchronisation and management of parallel threads/processes. The end user only needs to supply their problem-specific code, getting the parallelism for free. 
%In this way, it is possible for a user to parallelise their computation (provided that its structure conforms to one of the available patterns) and hence increase performance without having to deal with low-level details of parallel implementation. The obvious appeal of the concept of parallel patterns resulted in different major IT companies developing their own pattern libraries (Intel Thread Building Blocks~\cite{tbb}, Microsoft Parallel Patterns Library\footnote{\url{https://docs.microsoft.com/en-us/cpp/parallel/concrt/parallel-patterns-library-ppl?view=vs-2019}} etc.)
In the area of \emph{big data processing}, parallel patterns have become very popular with the emergence of the MapReduce programming model~\cite{mapreduce}. This model allows the development of large-scale, distributed, fault-tolerant data analytics applications without any knowledge of distributed systems. The most popular implementation of the MapReduce model is in the Hadoop framework for big data storage and processing. Being a generic model that allows many more specific classes of distributed computations to be implemented on top of it~\cite{bigdatabook}, MapReduce was used as a building block for several machine-learning based data analytics frameworks and libraries, such as MLLib~\cite{mllib} for Apache Spark, Keras and Tensorflow~\cite{tensorflow}.
%These libraries also work on the principle of patterns as they provide generic parallel machine-learning methods that can be used in end user data analytics applications, provided that the end user supplies their problem-specific parameters and operations.
%
Frameworks for AI-based big data processing in general are designed mostly with scalability and fault-tolerance in mind, with the main goals of i) being able to implicitly scale to larger and larger datasets distributed over a large number of machines; and, ii) accommodating for longer running computation by automatically restarting the parts of computation that fail. It is, however, a very well known fact that large-scale distributed processing exposes a number of potential security vulnerabilities~\cite{bigdatasecurity}. In big data processing, the data that needs to be analysed by a single application is often distributed across a large number of machines. Furthermore, the machine that stores and the machine that processes the data is most often not the same. All this results in a requirement for frequent data and computation movement, which gives opportunities for the threats such as DoS attacks, Man-in-the-Middle attacks, replay attacks and eavesdropping. This is especially true as many times the large-scale data processing applications are executed in public clouds, due to cost efficiency and maintainability. As a result of this, there are many tools and frameworks that aim to improve the security of big data storage and processing, dealing with the aspects of user authentication (e.g.~Apache Knox\footnote{https://knox.apache.org}), result verification (e.g.~Cluster BFT~\cite{bft}, Secure MR~\cite{securemr}, Accountable MapReduce~\cite{accountablemr}) and log analysis~\cite{loganalysis}. 
%
As discussed in Section~\ref{sect:elysian_vision}, one of the biggest issues in machine-learning based big data processing is that the applications are built using different layers of abstraction. While there is a lot of work in ensuring security of each individual layer, there is not much work in identifying and repairing security issues that arise from the interaction between these different layers. This is a fundamental problem for security of big data processing, since even if perfectly secure frameworks are used for each layer, their interaction may bring unforeseen security vulnerabilities and targets for attacks. The \TheProject{} project will address the problem of security of big data processing in a holistic way, considering not only all layers of a typical AI-based data analytics application, but also interactions between these different layers.

%Very little attention has been paid to the security issues. This is especially an issue for large-scale data analytics because typical applications for doing that need to combine several layers of functionality, possibly mixing several different libraries and systems. For example, a typical analytics application will use a machine-learning library such as Keras, which will in turn use more general frameworks for distributed machine learning, such as Tensorflow~\cite{tensorflow}. Tensorflow itself uses lower-level pattern-based frameworks, such as Hadoop MapReduce, which in turn operate on distributed filesystems and databases. While each of the individual layers may or may not have guarantees for safety, interaction between these different layers presents plethora of opportunities for potential cyber attacks. Coupled with the fact that the computations are performed in a distributed fashion, meaning there will possibly be data movement between different nodes and communication between processing agents running on different nodes, all of which can expose additional vulnerabilities that are not present in single-core or even locally-distributed settings, it is clear that ensuring security for distributed machine-learning based data analytics is a very complex problem.
 
\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of 
security of distributed pattern-based data analytics by:
\begin{itemize}
\item \emph{Security for data storage in big data analytics.} With the advanced security diagnostics based on a combination of static analysis, dynamic analysis and run-time monitoring, we will identify a wider class of security vulnerabilities that arise in usage of distributed databases and filesystems in big data applications. We will also implement methods to mitigate these vulnerabilities by appropriately transforming the code.
\item \emph{Security of distributed data processing frameworks.} Data processing frameworks such as Hadoop MapReduce, expose even more possible security vulnerabilities than the systems for data storage, as they need to move the data around between different distributed processing nodes, as well as to communicate between themselves. In \TheProject{}, we will identify security vulnerabilities coming from using the distributed pattern-based data processing frameworks in the end user applications. As far as is practically possible, we will also analyse the source code of these frameworks and identify potential security vulnerabilities within them, too.
\item \emph{Interaction between machine learning libraries and distributed data storage/processing frameworks.} We will investigate the problem of identifying and repairing security vulnerabilities that can arise from the interaction of machine learning libraries and lower level mechanisms based on distributed data storage and processing. Using machine learning libraries introduces another layer in the hierarchy of data analytics applications, with its own set of potential vulnerabilities, making the overall problem even harder than if only data storage and processing is concerned. 
\end{itemize}
\end{mdframed}


\subsubsection{Privacy in AI-based Data Analytics}
\label{sect:privacy}
%\vjcomment{UOD and SCCH to write}
Protecting privacy of the data is a key issue for data analytics, especially when it is based on machine learning as it poses several types of security risks. Different methods such as k-anonymity~\cite{10.1142/S0218488502001648}, l-diversity~\cite{10.1145/1217299.1217302}, t-closeness~\cite{DBLP:conf/icde/LiLV07}, and differential privacy~\cite{10.1561/0400000042} have been developed to address these issues. Differential privacy is a formal framework to quantify the degree to which the privacy for each individual in the dataset is preserved while releasing the output of a data analysis algorithm. It guarantees that an adversary, by virtue of presence or absence of an individual's data in the dataset, cannot draw any conclusions about an individual from the released output of the analysis algorithm. Differential privacy, however, does not always adequately limit inference about participation of a single record in the database~\cite{10.1145/1989323.1989345}. Differential privacy requirement does not necessarily constrain the information leakage from a data set~\cite{Calmon_privacyagainst}. Correlation among records of a dataset would degrade the expected privacy guarantees of differential privacy mechanism~\cite{DBLP:conf/ndss/LiuMC16}. These limitations of differential privacy motivate an information theoretic approach to privacy where privacy is quantified by the mutual information between sensitive information and the released data~\cite{5288525,Calmon_privacyagainst,6482222,7888175,DBLP:journals/corr/abs-1710-09295}. 

%
%The data perturbation approach uses a random noise adding mechanism to preserve privacy, however, results in distortion of useful data and thus utility of any subsequent machine learning and data analytics algorithm is adversely affected. 
%There remains the challenge of studying and optimizing privacy-utility tradeoff especially in the case when statistical distributions of data are unknown. Information theoretic privacy can be optimized theoretically using a prior knowledge about data statistics. However, in practice, a prior knowledge (such as joint distributions of public and private variables) is missing and therefore a data-driven approach based on generative adversarial networks has been suggested~\cite{Huang_2017}. The data-driven approach of \cite{Huang_2017} leverages recent advancements in generative adversarial networks to allow learning the parameters of the privatization mechanism. However, the framework of \cite{Huang_2017} is limited to only binary type of sensitive variables. 
%A similar approach~\cite{8919758} applicable to arbitrary distributions (discrete, continuous, and/or multivariate) of variables employs adversarial training to perform a variational approximation of mutual information privacy. The approach of approximating mutual information via a variational lower bound was also used in~\cite{NIPS2016_6399}.         

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of privacy-preserving data analytics by:
\begin{itemize}
\item \emph{Introducing a novel information theoretic approach for studying privacy-utility tradeoff} that will be suitable for multivariate data and for the cases with unknown statistical distributions. We will consider entropy of the noise as a design parameter for studying privacy of a data release mechanism. %A relevant optimisation problem here is to minimise the privacy-leakage quantified by the mutual information (between private and released data) while simultaneously minimising the amount of data distortion. 
The optimisation of tradeoff between minimising privacy-leakage and minimising data distortion can be analytically solved for a known data distribution, but this distribution may not be known in practical applications. Our framework will optimise the tradeoff curve \emph{without knowing data distribution}. The novelty of our approach lies in i) its generality for any unknown data distribution; ii) deriving optimal noise-adding mechanisms analytically; and, iii) computing privacy-leakage analytically without relying on the training of black-box models (e.g. adversarial networks~\cite{8919758}) for approximating distributions. 
%This is done as follows: 1) The probability density function of noise, that for a given noise entropy level, minimizes data distortion is analytically derived. 2) The privacy of sensitive data is preserved via adding random noise (sampled from derived optimal distribution) to the data observations. Only the noise added data observations are meant to be publicly released. 3) A stochastic model is learned to model the relationships between private and public data. 4) A lower bound on privacy-leakage is derived as a functional of distributions characterizing the data model. 5) An approximation to privacy-leakage is provided via maximizing its lower bound w.r.t. distributions characterizing the data model.         
\end{itemize}

\end{mdframed}
  

%The proposed approach to study and optimize privacy-utility tradeoff is novel. The are three significant features of the proposed framework. First is its generality for any unknown data distribution, the second is deriving optimal noise adding mechanism analytically, and the third is to compute privacy-leakage analytically without relying on the training of black-box models (e.g. adversarial networks~\cite{8919758}) for approximating distributions. 




\subsubsection{Formal Specification and Proving of Properties}
\label{sect:formal}
\vjcomment{USTAN to write}
\cbcomment{Should build on what we've done for teamplay here? Might want to just pick a tractable subset: secret variables?}

\vjcomment{Definitely we need to elaborate on what is done in TeamPlay and how we go beyond that}

The need to ensure the dependability and correctness of data intensive applications has never been greater. 
Formal methods include a variety of approaches to reason rigorously about the behaviour of computational entities and their application is particularly important in critical domains. 
The use of formal methods to ensure security has been recommended since the 1970s, with for instance work by Needham and Schroeder on authentication protocols~\cite{needham1978} stressing the importance of formal analysis of detailed models of secure systems. Formal methods were key to many discoveries over the years, including
a FREAK SSL/TLS vulnerability in 2015 \cite{freak2015} which may have affected a third of all deployed SSL/TLS servers.
Formal methods can provide strong end-to-end security 
and privacy guarantees throughout system execution and across layers of abstraction, but currently lack the expressive power to
address a variety of concerns in the context of big data analytics applications.
%However,significant challenges remain to be able to successfully use formal methods for security and privacy assurancein practice. Some security goals may be logically complex (e.g. non-interference) and hard to formulate (e.g.privacy concerns).We need scalable and flexible formal methods that can capture different abstractions inreal systems, can be used to prove the correctness of refactorings and self-healing, and can adapt to theenvironment, security and privacy goals of systems.
%Recent advances to formal methods for security are enabling their application at a scale 
%previously impossible. They are important because
%irrefutable importance of the quality of secure systems and the 
%Establishing whole-system security for end user software is crucial but particularly challenging for distributed systems.
The scale of these applications requires  methods for security and privacy assurance that are modular, compositional and incremental. 
%As these systems evolve over time, 
%and are often built using existing service components exporting an API that can be used by components developed later,  
%security cannot be verified once but has to be done incrementally as systems evolve.
We need  to specify and ensure the security of the whole system and not just individual components or abstraction layers within a system. Distributed temporal logics, such as {\sc{mdtl}} \cite{Kue-TCS2006}, 
%are similar to separation logics, 
interpreted over (prime) event structures, have been defined with the intention to capture complex systems at different levels of abstraction and enable reasoning across levels. The logic (comparable to separation logic) has been extended to a stochastic domain \cite{BowVia14}, but remains 
under explored for the context of systems considered in \TheProject{}. 
Event structures have been widely used and studied in the literature, have been used to give a true-concurrent semantics to process calculi such as CCS, CSP, SCCS and ACP, and can be understood as unfoldings of models such as Petri nets or runs for concurrent ASMs. We will explore this later combination further in \TheProject{}.
%The advantages of prime event structures include their underlying simplicity and how they naturally describe fundamental notions present in behavioural models including sequential, parallel and iterative behaviour (or the unfoldings thereof) as well as nondeterminism. 
%In this context, 

%Techniques based on formal methods, such as model checking and symbolic interpretation, do not report false  positives, but have potential problem with scaling. AFL~\cite{AFL} uses use fuzz testing to dynamically profile the code to expose potential security vulnerabilities.

%Develop a formal high-level multi-level language for specification of security properties of the code;•Define abstraction/refinement as a bidirectional transformation between specifications across levels;•Define a notion of behavioural equivalence (in terms of security and privacy-preserving properties) betweencode fragments;•Develop an integrated verification framework to prove the correctness of the equivalences and transformationand explore an interplay between runtime verification and exhaustive formal verification;

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of formal specification \& verification of security properties by:
\begin{itemize}
\item \emph{Defining a uniform approach for specification of security patterns and properties}. Based on concurrent ASMs~\cite{BorgerS16} (described earlier in \ref{sect:identifying}), we will
define a specification language able to capture security properties at different levels of abstraction, and will formalise notions of behavioural equivalence to enable us to prove the equivalence between specifications and hence formalise refactorings.

\item \emph{Providing an integrated verification framework}. We will continue to  exploit the interface between Isabelle and Z3 (e.g. \cite{BowCam2020}) to obtain a versatile tool for the specification, analysis, composition, and computation of the behaviour of distributed applications, and search for the optimal solution (e.g. a refactoring) that satisfies certain criteria (e.g. security guarantee) 
%which could be obtained automatically 
hence conforming with self-healing.
\item \emph{Defining a family of logics for security and privacy reasoning}. Based on {\sc{mdtl}} and stochastic extensions, we will define a logic expressive enough to capture and reason about security properties for our applications, and with a  fragment (e.g., FOL) suitable for capturing
information flow and privacy constraints. %first order logic for capturing privacy information flow.
%Exploring our experience in combining theorem provers with SMT solvers.
\end{itemize}
\end{mdframed}

\subsubsection{Coding Standards for Security}
\label{sect:codingStandards}
\vjcomment{UC3M to write}

There is an inherent impact from programming languages features in the
emergence of vulnerabilities in software. Recognizing this issue, ISO has
recently revised its technical report ISO/IEC  TR 24772 series on Guidance for 
avoiding vulnerabilities in programming languages. This series consists of a
language independent part~\cite{iso24772:1}, and a set of language specific
annexes including Ada (2020), or C (2020). Other programming languages are
under preparation such as Python, PHP, Spark, Ruby, Fortran, C++ or Java.
Another very relevant source for identifying vulnerabilities are the SEI CERT
secure coding standards for C~\cite{cert:c}, C++~\cite{cert:cpp}, and
Java~\cite{cert:java}. In the context of C++ there are multiple relevant coding
guidelines focused with different intensities in security and safety as
HIC++~\cite{hicpp} , the C++ Core Guidelines~\cite{cpp:core-guidelines}, or the
JSF C++~\cite{cpp:jsf}. Some rules of those guidelines are already supported by
some proprietary tooling (e.g.  Perforce) as well as by open source components
(e.g.  clang-tidy).  Other rules may be supported by library components as the
GSL (\url{https://github.com/microsoft/GSL}) or extensions to tools like 
ThreadSanitizer~\cite{dolz:2017}
or by the use of contract based programming~\cite{lopez-gomez:2019}.

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of security standards and static analysis to conformance to standards in several ways:
\begin{itemize}
\item \emph{Produce a security centric coding guideline}. 
Based on existing and ongoing coding guidelines for C++ we will develop a new
integrated coding guideline completely focused on the security aspect and with
a strong bias to specific characteristics of distributed storage and machine
learning data analytics.

\item \emph{Provide tooling for enforcing guideline conformance}. 
We will develop a single interface to multiple tools that enforce different aspect
of the new C++ coding guidelines, producing also new checks that need to be developed.

\item \emph{Check and refactor for specific components}. 
We will check and enforce the security guidelines in software components identified
in the project to validate that the approach is feasible and useful to
application and infrastructure developers. 

\end{itemize}
\end{mdframed}

%\subsubsection{Identifying Security Vulnerabilities}
%\label{sect:security}
%\vjcomment{This is the material for one of the old proposals. We need to check this and move the relevant stuff to the other sections (there seems to be lots of relevant stuff here).}
%
%Application security is becoming increasingly important in modern computer systems.
%Current practice is for security to be ensured at run-time, using efficient monitoring
%techniques coupled with dynamic run-time checks such as ISR~\cite{isr}, ASLR~\cite{aslr}
%and CFI~\cite{cfi}. These methods are, however, expensive in terms of computational
%complexity and energy consumption. As most of the security vulnerabilities come from
%code defects, bugs and logic flaws, the most cost-effective way to ensure security is
%to follow the secure code best practices~\cite{OWASP} and eliminate vulnerabilities
%before code is deployed. This is usually done with static and dynamic code analysers.
%Static code checkers, such as Appscan Source~\cite{AppScan} and Coverity~\cite{Coverity}
%%and Klocwork~\cite{Klocwork}, 
%are based on techniques for quality checking that
%have been extended to cover security vulnerabilities in the code. The main drawback of 
%these methods is their high rate of false positives they produce. Techniques based on
%formal methods, such as model checking and symbolic interpretation, do not report false 
%positives, but have potential problem with scaling. AFL~\cite{AFL} uses use fuzz testing 
%to dynamically profile the code to expose potential security vulnerabilities. 
%None of the above tools includes special algorithms for detecting vulnerabilities 
%resulting from parallel execution. Parallelism, due to undeterministic nature of the
%application execution, presents many additional problems from the security perspective. These problems have to be dealt with efficiently
%in future systems.
%
%
%Including third party libraries into the application code presents additional
%security risks, as these libraries may include intentional or unintentional
%vulnerabilities resulting in unwanted behavior or data leakage of sensitive 
%information. For example, an open source JavaScript library used in a website 
%may contain malicious code that collects data and sends it to the third party. 
%%Another example is binary packages which contain vulnerability which is detected by crackers and compromise users??? machines. 
%The state of the art solutions to this problem is to use security testing, static 
%analysis and dynamic analysis, such as sandboxing~\cite{jsand}, to detect 
%vulnerabilities and malicious packages before integrating them into 
%the application code. % is proposed to detect malicious JavaScript code, in addition, they 
%%suggest wrapping the components to control access to security sensitive operations. 
%In~\cite{Cova}, a combination of anomaly detection and emulation systems is built to detect malicious code. OWASP~\cite{OWASP} presents the risks in using third party 
%packages, how to determine if these packages are vulnerable or contain malicious code 
%and how to deploy these packages. %This problem is not met only in JavaScript packages, it is in many other 3rd party packages, especially, Android 3rd party libraries. 
%Other possible solutions to the problem of integrating third party libraries are
%based on analysis of the libraries, building control flow graph, looking for 
%copies of buggy code, and profiling packages~\cite{Hanna, XinSun}.%~\cite{Hanna, XinSun, Nora, Backes}.
%These solutions miss many vulnerabilities or malicious dormant code as a result of 
%obfuscation, packing, and the ease in rebuilding new variants of the malicious code, 
%or just the rapid release of new buggy version of packages.
%
%There are cases in which it is not beneficial to fix a vulnerability in the 
%software or in a third party code integrated into the software. For example, 
%in cases where the patch is hard to deploy or where fixing the vulnerability 
%has significant impact on performance. In fact, 99\% of attacks are believed to exploit known and fixed vulnerability~\cite{GartnerVulnerability} for which the fix was not deployed. One famous example is the WannaCry attack that was based on a fixed and updated CVE.
%To protect the software in these cases,
%\emph{virtual patching} may be used. %Virtual patching is a security policy 
%%enforcement layer which prevents the exploitation of a known vulnerability. 
%A virtual patch is a set of rules that implements complex logic to prevent 
%malicious traffic from reaching the application. 
%%The virtual patching mechanism will usually be integrated in the organization firewall. 
%The challenge is to come up with a set of accurate rules that filter 
%exactly the malicious inputs without blocking other inputs. This work is 
%currently done manually, and it would heavily benefit from integrating a 
%predictive model to automatically detect inputs that may utilize the 
%vulnerability.
%
%
%
%
%\TheProject{} will advance the state-of-the-art of security of parallel code in the following directions:
%\begin{itemize}
%\item XX
%\end{itemize}

\subsubsection{Identity Management}
\label{sect:auth}
\vjcomment{COGNITIVE to write}
Identity management entails solutions for user identity, authentication and authorisation functions. Core components of identity management are multi-factor authentication, continuous user identification, and access control. \textit{Multi-Factor Authentication (MFA)} has been introduced with the aim to provide a higher level of security and continuous protection of resources from unauthorized access through the use of more than two categories of credentials \cite{scheidt2005multiple, bhargav2007privacy}. Researchers and practitioners have attempted to provide various solutions for multi-factor authentication such as tokens based on push notifications on smartphones, Time-based One Time Passwords (TOTP), Quick Response (QR) codes, graphical Transaction Authentication Numbers (PhotoTANs) and voice recognition \cite{mare2016study, ometov2018multi, 10.1145/1053291.1053327}. \textit{Continuous user identification} is a technology aiming to verify the end user's identity in real-time (after successfully authenticating), while carrying out tasks. Several works proposed solutions based on users’ interaction behavior analysis on both desktop computers and smartphones \cite{10.1145/2991079.2991097, 10.1145/2702123.2702252, gascon2014continuous}, physiological data analysis such as body signals (heart rate, skin conductance, etc.) \cite{ometov2018multi, rui2018survey}, Electrocardiographic (ECG) recognition \cite{silva2011clinical}, face biometrics \cite{bhattacharyya2009biometric, dabbah2007secure}, and eye gaze data analytics \cite{jain2004introduction, bulling2012increasing}. \textit{Access control} is a mechanism that regulates access to the system resources. Current access control systems entail several challenges, such as third-party presence, lack of scalability, and lack of privacy \cite{10.1145/3350546.3352561, 10.1145/3316481}. In this context, Blockchain technology has specific features that can address several of the existing challenges in access control, i.e., its distributed nature addresses the problem of single-point of failure, elimination of third-parties and potential privacy leakage, monitoring and access to trustable and unmodifiable history logs.
%
Various organizations and companies also exist that are either adopting MFA, or continuous user identification (e.g., Cisco’s Duo Security, Futurae, Acceptto, Auth0, Saaspass). However, these are far from fully adopting solutions that combine MFA, continuous user identification and access control. These systems work towards providing different authentication options to users. However, these solutions are mostly dedicated and favor either the multi-factor aspect or the continuous user authentication aspect without providing a unified solution that combines the two under an agile system integration model. Despite sharing some similar second factors for authentication, our innovation lies on the combination of MFA, continuous user identification and access control under a unified and usable integration system. The strengths of the proposed unified integration system are concentrated in the rapid deployment to existing online services and the agility to adapt to existing ecosystems, aiming to protect sensitive resources and provide a seamless security experience to the end users.

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of identity management for big data analytics systems by:
\begin{itemize}
\item \emph{The proposed solution to combine MFA and continuous user authentication} under a unified, agile system integration model represents a change of paradigm in today’s state of the art which considers MFA and continuous user identification as two independent approaches that do not co-exist under one unified solution.
\item \emph{The developed intelligent biometrics} based on a combined analysis of physiological, face and eye gaze data analytics will advance the state of the art for seamlessly identifying end users.
\item \emph{The developed Blockchain solution} will provide novel methods for regulating data access to data as well as tracking the lineage and provenance of data. 
\item \emph{Evaluation studies} will provide new insights and knowledge to practitioners for building more usable and agile identity management schemes, and researchers to understand human behavior within such tasks.
\end{itemize}
\end{mdframed}

%\subsubsection{Security Patterns}
%\vjcomment{USTAN and UOD to write -- possibly merge with the previous subsections?}

%\vjcomment{I think this can be integrated into the section \ref{sect:patterns}}

%\begin{itemize}
%	\item Design pattern
%	\item parallel patterns or skeletons
%	\item abstractions over low-level implementation
%	\item provided as a library
%	\item structured approach
%	\item extended for security
%	\item new patterns X Y Z
%\end{itemize}

%\subsubsection{Security Contracts}
%\cbcomment{USTAN to write - we can build on what we've done for teamplay here}

%\vjcomment{Let us move this to \ref{sect:formal}}

%\begin{itemize}
%    \item Contract
%    \item CSL system
%    \item Proving properties of security
%    \item Reflecting proofs back to the programmer
%    \item Vulnerabilities
%    \item Secret variables
%    \item Branching and side channel attacks?
%\end{itemize}

\subsubsection{Refactoring}
%\vjcomment{USTAN to write}
%\begin{itemize}
%	\item Refactoring history
%	\item program transformation
%	\item refactoring used as a software engineering strategy
%	\item structural changes to the code
%	\item refactoring to introduce parallel patterns
%	\item will be extended to introduce security patterns, fix vulnerabilities, adhere to standard
%	\item some aspects may be proved correct
%\end{itemize}

In essence, refactoring is about changing the structure of a program's source code in such a way that the change does not alter the functional behaviour of the program. It is a software engineering practice that is used daily by developers, who restructure code to make it more amenable to maintenance and extension. Refactoring is also a technique that is usually practised \emph{manually} by developers. The downside of manual refactoring is that it becomes very error-prone over large code bases; it also requires the programmer to be an expert in the particular structural change that is being implemented. Refactoring tools, on the other hand, automate the process, allowing machine-checkable pre- and post-conditions to automatically ensure that the refactored code is functionally correct to the original. 
%
Refactoring has a long history, with the original refactoring work going back to the \emph{fold/unfold} system 1977~\cite{darlington77}. Since then, refactoring has been applied to a wide range of applications as an approach to program transformation~\cite{Mens:2004:SSR:972215.972286}, and has been applied to a wide range of applications as an approach to program transformation, with refactoring tools a feature of popular IDEs including, \textit{e.g.}, Eclipse and Visual Studio.
%There has been very little prior work on refactoring for improving energy consumption, with the exception of some preliminary refactorings for Erlang~\cite{Nagy:2019:TSG:3331542.3342570}. Previous work by Brown et al. on parallelisation \textit{via} refactoring has primarily focussed on the introduction and manipulation of parallel pattern libraries in C++~\cite{brownagricultural}, Haskell~\cite{kar30667} and Erlang~\cite{Brown:2014:CRP:2630180.2630190,DBLP:journals/cai/BarwellBHTB16, parco2015}. Another approach
%has been the automated introduction of annotations in the form of C++ attributes~\cite{cfs}{rio:2018}.
In previous work, refactoring has been used successfully to \emph{introduce parallel patterns and concurrency} into the code in C++~\cite{brownagricultural,DBLP:conf/pdp/JanjicBMHDAG16, mcts} and Erlang~\cite{hlpp,DBLP:journals/cai/BarwellBHTB16}. %Another approach
%has been the automated introduction of annotations in the form of
%C++ attributes~\cite{rio:2018}.
%Dig \textit{et al.}~\cite{dig} use refactoring to introduce parallelism in Java.  
%
%Parallel design patterns, or algorithmic skeletons, were suggested as solution to the difficulties presented by low-level approaches~\cite{Asanovic:2009:VPC,DBLP:journals/spe/Gonzalez-VelezL10}.
%A range of pattern/skeleton implementations have been developed for a number of programming languages; these include: RPL~\cite{DBLP:conf/pdp/JanjicBMHDAG16}; Feldspar~\cite{DBLP:conf/ifl/AxelssonCSSEP10}; FastFlow~\cite{fastflow:2017}; Microsoft's Parallel Patterns Library~\cite{ACM:book/msoft/CampbellM11}; and Intel's Threading Building Blocks (TBB) library~\cite{DBLP:reference/parallel/X11pz}.
%Since patterns are well-defined, rewrites can be used to automatically explore the space of equivalent patterns, e.g.\ optimising for performance~\cite{DBLP:conf/europar/MatsuzakiKIHA04,DBLP:conf/ipps/GorlatchWL99} or generating optimised code as part of a DSL~\cite{DBLP:conf/dagstuhl/Gorlatch03}. Moreover, since patterns are architecture-agnostic, patterns have been similarly implemented for multiple architectures~\cite{DBLP:conf/cgo/HagedornSSGD18,DBLP:conf/parco/ReyesL15}.
%
%This introduces a level of specialisation, and the possibility of choice between pattern implementations. Conversely, GrPPI~\cite{DBLP:journals/concurrency/AstorgaD0G17} is capable of invoking other libraries, and is thereby able to take advantage of the specialisations that they present without potentially laborious reimplementation.
%
%Elsewhere, approaches to automatic parallelisation have traditionally focussed on the transformation of loops. Examples include Lamport's early approaches in Fortran~\cite{DBLP:journals/cacm/Lamport74}, Artigas' approach for Java~\cite{kennedy}, on \textsc{doall} and \textsc{doacross} loops~\cite{DBLP:conf/pldi/BurkeC86,DBLP:conf/popl/LimL97}, the polyhedral model~\cite{DBLP:conf/ppopp/AncourtI91,DBLP:conf/IEEEpact/Bastoul04,DBLP:conf/IEEEpact/BouletF98} and more recently on the generation of pipelines~\cite{DBLP:conf/IEEEpact/TournavitisF10,DBLP:journals/taco/WangTFO14}. 
%
%The polyhedral model~\cite{DBLP:conf/ppopp/AncourtI91} also \textcolor{red}{looms large} (\cite{DBLP:conf/IEEEpact/Bastoul04,DBLP:journals/ijpp/GrieblFL00}).
%
%Other approaches to automatic parallelism have included a focus on coarsely dividing programs into sections that can be run in parallel~\cite{DBLP:conf/pepm/LiT15,DBLP:conf/ppopp/RulVB08}; less-abstractly on exploiting potential parallelism at the instruction-level~\cite{DBLP:conf/europar/StefanovicM00}; and on exploiting specialised hardware such as GPUs for automatic parallelisation~\cite{DBLP:conf/popl/GuoTS11,DBLP:conf/pppj/LeungLL09}.
% 
%\textcolor{red}{Other approaches: slicing for concurrency \& Rul's Extracting coarse grain parallelism (\cite{DBLP:conf/ppopp/RulVB08}); instruction level parallelisation (??); GPUs (\cite{DBLP:conf/popl/GuoTS11,DBLP:conf/pppj/LeungLL09}).}
%
%Whilst fully automatic approaches simplify parallelisation for the programmer by removing them from the process, such approaches can be very specific in both the parallelism they are able to introduce and the code to which they can be applied. Conversely, programmer-in-the-loop approaches, such as refactoring, allow the programmer to employ knowledge about both code and parallelism.
%
There is only limited previous work on refactorings for increased security. This includes proposed refactorings to increase the protection level of sensitive information stored in the Java code~\cite{maru2007} and refactorings to also alert its impact on code vulnerabilities~\cite{maru2008}.
Brown et al. proposed refactorings for removing \emph{anti-patterns} in Distributed Java Components~\cite{brown1998antipatterns}.  Improving software security using search-Based refactoring was proposed by Ghaith and Cinneide for Java programs, using the Code-Imp tooling~\cite{ghaith}. Finally, using refactoring rules to increase security for Object Oriented design was proposed by Khan and Khan~\cite{khan} . However, none of these proposed systems uses the concept of a \emph{security pattern}, static analysis for code vulnerabilities, formal reasoning or tool support for languages other than Java. 
 
\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of refactoring for security in several ways:
\begin{itemize}
\item \emph{Refactorings to introduce security patterns.} We will develop new software refactorings that will rewrite source code to introduce security aware patterns. The results of which will be equivalent software in terms of functionality, but with increased security.
%\item \emph{Refactorings targetting security of big-data applications.} XX.
\item \emph{Verifiable refactorings using formal reasoning.} We will provide correctness and general soundness proofs for some of the refactorings, using ASMs and verification techniques. This will give confidence that the refactorings will increase the security level of the source code and not change the behaviour of the result.
\item \emph{Refactorings that will repair vulnerabilities in the code.} We will take the results of the static analysis and self healing techniques and produce further refactorings and transformations that rewrite source-code making it more secure by eliminating or reducing code vulnerabilities. 
\item \emph{Refactorings that will transform code to make it conform to security coding standards.} We will produce new refactorings that will rewrite the source code so that it conforms to a security standard, therefore increasing the confidence that the program is secure
\item \emph{Refactorings guided by static and dynamic analysis.} The refactorings developed will be guided by both static and dynamic analyses, finding the security vulnerabilities as candidates for refactoring. 
\end{itemize}
\end{mdframed}

\subsubsection{Use Cases}
\label{sect:applications}
\label{sect:background-last}
\vjcomment{FREQUENTIS, DEMOCRITOS and SOPRA to write}

\begin{table}[!htb]
\begin{center}
\begin{tabular}{|c||c|c|c|}
\hline \hline
\multicolumn{4}{|c|}{Use Cases} \\ \hline
Layer & \textbf{Air Traffic Management} & \textbf{Digital Banking} & \textbf{Healthcare} \\
\hline \hline
\emph{Environment} & Public/Private Cloud & Public/Private Cloud & Public/Private Cloud  \\
\emph{Data Storage} & Azure DL, MySQL & Azure DL, Cassandra & Azure DL  \\
\emph{Data Processing} & Azure DL Analytics & Azure DL Analytics & Azure DL Analytics  \\
\emph{ML Framework} & Azure AI & Spark MLLib & Azure AI  \\
\emph{Prog. Language} & Java, JavaScript, Python & C++ & C++ \\
\hline \hline
\end{tabular}
\caption{Technologies Used in \TheProject{} Use Cases}
\label{tab:usecases}
\end{center}
\end{table}


\paragraph{Air Traffic Management}
\label{sec:atm}
- Air Traffic Management (ATM) is an aviation term for the amalgamation of various systems that supports an aircraft to depart from an aerodrome, transit required airspace, and land at a destination aerodrome. The system includes the following sub-systems Air Traffic Services (ATS), Airspace Management (ASM), and Air Traffic Flow and Capacity Management (ATFCM). With the rise of drones, it becomes essential to integrate Unmanned Traffic Management (UTM) with ATM. The increasing emphasis of modern ATM is on inter-operable and harmonised systems permitting an aircraft to operate with the minimum of performance change from one airspace to another. With the increase use of Internet-of-Things via sensors and on-board monitoring of the aircraft and required ground support infrastructure the ATM/UTM effectiveness and efficiency are now critical. The introductions of other new unmanned aircraft and drones into the airspace is requiring a rethink of the system and security required to enable the exchange of relevant information with the ATM/UTM without introducing any data breath issues. 
Various stakeholders in ATM are facing two key challenges: the requirement to migrate from ATS Message Handling System (AMHS) to System Wide Information Management (SWIM), and the requirement to integrate UTM with ATM. SWIM is an integral component of the digital transformation that is taking place in the Aviation industry. This digital transformation is having a major impact, as not only systems but also new processes and new staff expertise are required Today Air Navigation Services Providers (ANSPs) rely on closed systems with proprietary point-to-point communication for data exchange. SWIM enables these systems to operate as an overlay to IP networks using open standards and ATM-defined data exchange models (Aeronautical Information Exchange Model (AIXM), ICAO Meteorological Information Exchange Model (IWXXM), Flight Information Exchange Model (FIXM). 

\paragraph{Open banking}
\label{sec:banking}
- As the digital disruption moves the banking services closer to the end user more diverse software tools are deployed into the banking workflow via open banking solution. We plan to look at how we auto-generate data processing code for a processing hub for the Competition and Markets Authority Nine (CMA9) that forms the nine largest banks in the United-Kingdom. We will use the tools we develop during the research to secure the auto-generated code is secure and without unwanted vulnerabilities.

%\paragraph{Space-based Open Internet Technologies}
%\label{sec:spacenet}
%- As the digital access in the world moves to a more cost effective hybrid space and ground solution the use of Delay/Disruption Tolerant Networking (DTN) and Solar System Internet (SSI) technology opens the global communication system to penetrate into more remote areas and will but more social care and healthcare systems onto the open internet. We want to ensure that we create code to deploy within these edge computing devices that can auto-heal their own security vulnerabilities. This new internet will orbit at +/- 500 kilometers above the earth and will supply a coverage of communication across the globe. As this uses the DTN solution to communicate is required to ensure the system is secure at all times as it has data in-rest and in-motion at the same-time.

\paragraph{Distributed Healthcare (IoT)}
\label{sec:health:IoT}
- As more sensors enter into the Distributed healthcare (IoT) domain the code for these sensors, edge collection devices and core processing capability will becoming a life-threatening issue if any vulnerabilities cause unwanted outcomes for a patients health. We want to ensure auto-healing of our solution and indirectly auto-healing our citizens' healthcare ecosystem.

\subsubsection{Key Technologies Used in the \TheProject{} Project}
\label{sect:key-technologies}

\paragraph{\SCCHshort{} Machine Learning Tools.}
\label{sec:mlpp}
\emph{mlpp} is a C++ library with interfaces to several other languages. It 
is currently under heavy development. So far, it contains algorithms for 
linear regression, time series analysis, feature selection and causal 
dependency discovery. Its focus is on building interpretable models. It is hosted 
on SourceForge (\url{https://sourceforge.net/projects/ml-pp/}), and released 
as Open Source (GPL license); paid licenses for commercial use will be provided
with the first mature release.

\paragraph{\SCCHshort{} eKNOWS - Reverse Engineering Formal Models from Source Code.}
eKNOWS is a platform that integrated diverse reverse engineering and analysis tools developed by SCCH, with a strong focus on extracting knowledge from source code and related software engineering artifacts (http://codeanalytics.scch.at/). This platform is used at SCCH in diverse cutting-edge applied research projects concerning the analysis and evolution of software systems, documentation generation, and quality assurance and automated testing. 
We plan to extend the capabilities of the eKNOWS module for the extraction of ASMs specifications, which currently deals only with generic/standard Java programs as described in \cite{FerrarottiMP20,FerrarottiPMB19}. The extension will include the extraction of ASMs from C++ code and improvements on the abstraction process by taking into account the semantics of the libraries that are key in the distributed data analytic system implementations considered in the proposal. These models will from the base for the specification and verification of security contracts expressed in logics for ASMs.  

\paragraph{\IBM{} Security Vulnerability Detection Technology.}

IBM developed vulnerability detection technologies based both on static and dynamic methods. IBM developed Beam for static analysis of C/C++ code and extended it to look in particular for security vulnerabilities with a low ratio of false positive. In addition, IBM developed the ExpliSAT tool which uses symbolic interpretation for more precise program analysis including checks for security vulnerabilities such as buffer overflow detection. During the \rephrase{} project, ExpliSAT was extended to analyse parallel programs. To complement the static methods IBM uses also Fuzz testing technology that detects security vulnerabilities dynamically using genetic algorithms. In order to achieve accurate and scalable security vulnerability detection, IBM is now working on combining the static and dynamic analysis vulnerability detection tools.     

\paragraph{\SAshort{} \paraformance Refactoring and Code Analysis Tools.}


%The \paraformance Refactoring tool was originally developed for the EU FP7 \paraphrase{} project, and later extended and enhanced in both \rephrase{} and the Scottish Enterprise innovation project, \paraformance{}. 
\paraformance, developed by \SAshort{}, is a tool-chain designed to \emph{democratise} parallel programming by allowing software developers to quickly and easily write parallel software. %\paraformance enables software developers to find the sources of parallelism within their code, automatically (through user-controlled guidance) through a process of \emph{pattern discovery}. 
\paraformance offers refactoring support to allow parallel patterns to be introduced directly into the source code of the application, inserting the parallel business logic. \paraformance also has integrated safety checking features to eliminate most common sources of parallelism errors, such as race conditions and deadlocks, hence making the parallelised code thread-safe. %Finally, \paraformance is able to repair some of the parallelism errors detected by the safety checking to automatically make the application thread safe.
\paraformance will be extended in \TheProject{} by adding new refactorings that aim to increase the security levels of an application, this will be done by new refactorings that introduce security-aware patterns into the source code, repair security vulnerabilities and refactor the code to ensure it conforms to a security standard. Further more, the refactorings will be verified increasing confidence in their implementation.
%
% \paraformance will be extended in \TheProject{} by adding safety checks for the distributed code, in addition to the existing mechanisms for checking the code that is executed on shared-memory machines. We will also extend the tool with features to check for \emph{security} of the code. 

%\paragraph {\SAshort{} CSL: The Contract Specification Language}
%CSL (Contract Specification Language) was developed as part of the ongoing H2020 \teamplay{} project. It is designed to act both as an annotation library for C applications, allowing the developer to annotate their source code both to reflect non-functional properties of time, energy and security back into the source code, but also in order to express assertions ---or contracts--- about the non-functional properties. Such non-functional properties could be, for example, the worst-case execution time (or energy) of a block of code, the security vulnerability level of a variable, etc. The assertions themselves are expressed in terms of these properties and other normal variables within the source code. The properties can also be reflected back into the source-program as first-class citizens: normal programming variables with values in the source-level space.
%The assertions are then passed to an underlying proof system together with an abstract interpretation of the C program. This proof system (currently implemented using the dependently typed language, Idris) produces  a proof of whether or not the assertion can be met. CSL will be extended in \TheProject{} by adding ... 

\paragraph{\FRQshort{} MosaiX SWIM: Aviation Integration Platform}
\label{sec:swim}
The Frequentis Integration Platform is an end-to-end solution that provides aviation stakeholders (ANSPs, airports, airlines, met offices) with tools required to unlock and monetize their data by offering new services to their customers (i.e. airlines). It allows aviation stakeholders to easily interface with legacy and third-party systems, enabling data to be freed from traditional storage silos and fused with different sources. MosaiX SWIM is designed around a microservices architecture and an open messaging system in which all software components exist as independent artefacts and communicate in a decoupled way. This reduces vendor lock-in by allowing customers to substitute components and incorporate new technologies in the future. For the software components  Java, Elasticsearch, Phyton, and on client side JavaScript is used. The platform provides an efficient API Manager and tools for billing consumers of services using models such as hourly billing or consumption-based billing. In addition, it offers preconfigured and customisable metrics and dashboards, together with data-analysis tools.



\paragraph{\YAGshort{} Machine learning augmented Static Analysis.}

YAGAAN developed vulnerability detection technologies in the source code based on static methods. YAGAAN developed the YAG-suite for static analysis of Java and PHP applications based on Static Analysis (SAST) and machine learning in order to fight against the recurring problem of false positives. While SAST detects vulnerabilities and raise warnings based on specific vulnerability modeling and detection rules, the supervised machine learning qualifies each individual warning based on the users feedback and allocates them
relevancy and criticality scores. In progress from the state of the art, it takes into account uncertain information,the local and global contexts of the application and a first level of end user business semantics (especially for privacy vulnerabilities), while adapting to the end user context from users expertise. 

\paragraph{\YAGshort{} Code mining.}

In order to bring more automation to the process of application vulnerabilities auditing YAGAAN developed a new approach of investigating the properties and features of Java and PHP applications that is called "Code Mining". Code Mining relies on SAST and supervised machine learning and is integrated to the YAG-Suite, In the same way that data mining extracts valuable information from huge sets of data, Code Mining extracts high value technical, security and end user business oriented information from huge amount of source code to feed vulnerability detection and automated qualification.

\paragraph{\COGNIshort{} Cognitive Identity.}

\COGNIshort{} developed an integrated authentication platform which allows the seamless integration of a variety of authentication methods for secure and usable multi-factor authentication. The platform entails standalone authentication components and methods (i.e., usable tokens, OTPs, graphical passwords, intelligent eye gaze analytics), whose know-how will be used to implement and further improve the proposed identification and authentication methods.

\paragraph{\COGNIshort{} Face and Eye Gaze Analytics.}

\COGNIshort{} developed and evaluated an eye gaze-driven metric based on hotspot vs. non-hotspot segments of images for unobtrusively estimating the strength of user-created graphical passwords by analyzing the users' eye gaze behavior during password creation in real-time \cite{10.1145/3377325.3377537, 10.1145/3379336.3381460, 10.1145/3320435.3320474}. In addition, a prototype face recognition mechanism has been developed and evaluated aiming to identify end users based on real-time face analysis using low-cost Web-based cameras. \COGNIshort{} is currently working to further extend the eye gaze-based and face features and combine these with users' physiological data under a unified platform for continuous user identification.

\subsubsection{Innovation Potential}
\label{sec:innovationpotential}
\label{innovationpotential}

\eucommentary{Describe the innovation potential which the proposal represents. Where relevant, refer to products and services already available on the market. Please refer to the results of any patent search carried out.}

It is by now a well-established fact that big data science is bringing another revolution into the world of software development. The World Economic Forum in their report on Data Science in the New Economy from July 2019 states that \textbf{"as the Fourth Industrial Revolution unfolds, led by advances in technologies such as data science and artificial intelligence, the labour market is again changing in a fundamental fashion"}. The economic potential of data science is enormous. An interim report of European Data Market study prepared by IDC \& Open Evidence claims that \textbf{"the overall data market (i.e. the aggregate value of data-related products and services exchanged within the European economy) is estimated at already more than 50 billion Euro and will get to 111 billion in 2020 under high-growth conditions"}. According to the same report, the data driven innovation in the EU contributes to \textbf{almost 5\% of the EU GDP}. In recent times, and especially in the light of COVID-19 situation, data science has become even more important to facilitate extracting and exchanging information between remote organisations, and especially in situations where data cannot leave the place where it is stored. The growth of computational clouds, together with possibilities to deploy applications that span multiple private and public clouds has offered potential to really achieve the promise of data science. This has, however, also come with a risk. A study published by IBM Security of 524 security breaches that occurred between August 2019 and April 2020 shows that \textbf{the average cost of a single breach is \$3.86 M}. The same study shows that the time required to identify and contain a data breach was \textbf{280 days}. This clearly shows the enormous economic costs of security vulnerabilities and how unprepared the state-of-the-art technologies are to deal with these breaches. But the situation is even more complicated when we consider the increasingly strict regulations for ownership and sharing of the data, such as GDPR. In the UK, companies can potentially be fined \textbf{4\% of their total turnover for each privacy breach.} It is clear that writing secure and privacy-preserving data analytics will be of the utmost importance in the near and long-term future. It is equally important for the software developers of big data applications to be kept in a loop of a cycle of vulnerabilities identification and repairing, giving her a better understanding of the security issues and the methods to mitigate them, hence increasing trust into the working of the systems. \TheProject{} innovation potential lies in bringing all these benefits to the table, with the technologies to identify and repair vulnerabilities that serve as a base of a high-level, pattern-supported \emph{secure software development methodology} that will guide the developer through the process of implementing and deploying their application. Formal verification of security properties, together with the verification of the conformance to the coding standards will provide additional assurance to the security of the code. Hence, our technologies aim for both \emph{guiding the user step-by-step through the process of securing their code} as well as \emph{providing security and privacy by design for the new applications}.  %Additionally, significant innovation will come from the applications of the \TheProject{} technologies to the selected use cases from the air traffic management, healthcare and digital banking domains, allowing for XX. 
Furthermore, the \TheProject{} technologies for protecting privacy offer significant innovation potential in the area of preventing violation and abuse of rights in the digital age, as detailed in Section~\ref{sect:expectedImpacts} on Page~\pageref{box:gender}.

\subsubsection*{Individual Innovation Potential}

The anticipated partner-specific potential innovations are listed below and described in more detail in the individual exploitation plans in Section \ref{sect:exploitation-plan} (Page \pageref{sect:exploitation-plan}).

\begin{mdframed}[backgroundcolor=blue!5]
\IBMshort{} will extend its expertise in Code Vulnerability Detection technologies, combined with novel techniques for repairing the identified vulnerabilities that will be developed for \TheProject{} will significantly enrich the ability to develop secure and robust AI-driven big data applications. The IBM Cloud Pak for Data platform will benefit from the results of the \TheProject{} project in the areas of security and protection of Big Data analytics. In addition, we will enrich the existing IBM security solutions and products by exploiting advanced vulnerability detection and code repairing technologies of the \TheProject{} project to commercially benefit businesses across Europe.
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\SOPRAshort{} will feed \TheProject{} tools into a massive software development capability, enhancing the security of products delivered into areas such as government, transportation, public safety, banking and insurance. This capability will transfer the tools into the extensive client base of the company across twenty-five countries. The tools enable the creation of various new innovative solutions for the customer base they serve.
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\YAGshort{} will build on the YAG-Suite existing capabilities that cover Java and PHP source code to extend its technology in two complementary directions: %Extension of the YAG-Suite 
to detect in C , C++ and Python written applications the vulnerabilities that are currently detected in JAVA and PHP; and also extend the YAG-Suite detectable vulnerabilities to address the specific vulnerabilities related to 
%use case of 
big data analytics applications.

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\FRQshort{} aims to %harden 
improve the quality of software development, data analytics and services to a level that not only has them pass regulatory inspection, but rises to the challenge faced by operating live services in an environment that no longer relies purely on closed networks to achieve secure operation. This ties in with industrialization efforts currently under way. It is envisioned that lessons learned and \TheProject{} tools will directly flow into the MosaiX data platform, ensuring that our customers can benefit from the use of these technologies in a world where attacks are going to be far more sophisticated and wide-ranging than what is current. 
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\SCCHshort{}
%\TheProject{} will enable SCCH 
%to further 
will extend its expertise in ``Privacy Secured Distributed AI'' by developing novel information theoretic tools for preserving privacy in distributed AI systems. The research on privacy 
%by \SCCHshort{} within \TheProject{} 
will make it possible to provide i) guidelines for technology assessment and, ii) certification on the trustworthiness of AI systems. \SCCHshort{} will use its expertise on formal models and logics for distributed adaptive systems to introduce new techniques to capture and verify  security requirements. It will provide tool support through its eKNOWS platform for the extraction of (concurrent) ASMs models from source code, contribute to the effort to formalise and verify reparability proof obligations for (run-time adaptive) self-healing, and develop algorithms for run-time detection of damaging activities and the start of countermeasures before any damage has occurred. 
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\COGNIshort{} will extend its expertise in the interdisciplinary fields of human-computer interaction and security (HCI-SEC) and intelligent user interfaces. Innovation activities from \COGNIshort{} within \TheProject{} relate to the design and development of intelligent and user-centered identity management schemes in the project's use-case domains (air traffic management, open banking, distributed healthcare). The envisioned results will also be valuable in other domains, e.g., online education, that \COGNIshort{} is working in.
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{\USTANshort{}.}
\USTANshort{} has expertise in refactoring, formal verification and logics, and will explore \SCCHshort{}'s tool to extract ASMs from code, as well as formalise novel notions of self-healing for security across layers of abstraction for real applications. This will enrich some of the modules offered at undergraduate and postgraduate levels, notably the Erasmus+ Joint Masters programme on Advanced Systems Dependability, with Ireland (Maynooth University) and France (Universit\'e de Lorraine). 
\USTANshort{} will promote the project via overseas collaborators from the healthcare domain in the USA and Brazil, and through events and initiatives within the Scottish Informatics and Computer Science Alliance (SICSA). \USTANshort{} contributes to health informatics research initiatives in Scotland such as  Health Data Research (HDR) UK and has close connections to the DataLab, the Health and Care Division of the Scottish Government and several NHS Health Boards which could benefit directly from our project results.
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{\UODshort{}.}
\UODshort{} will extend its expertise in Big Data Analytics by addressing the crucial problem of security issues in such applications. This will have impact both on education, feeding directly into undergraduate and postgraduate modules and courses offered by \UODshort{}, as well as on our collaboration with various external scientific and industrial institutions. As an example, an important innovation will be exploiting the project results in the area of healthcare, where \UODshort{} has a long-standing collaboration with National Health Service (NHS) in Scotland. The project results will contribute to the security and protection of privacy of the data kept by NHS, on which advanced data analytics is applied to improve the diagnostics and prediction of optimal treatments. \UODshort{} will also exploit its connections with DataLab and local industry in Scotland
%, XX and XX 
to further promote the project results in similar end user applications. 
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{\UCMshort{}.}
\UCMshort will increase their capabilities in ensuring correctness
and finding software vulnerabilities related to programming language
features. This will impact both course offerings and cooperation
with industrial partners and other research institutions. The use
of contracts to ensure correctness will be promoted in their future
collaborations with partners such as BBVA (banking), CENER (energy) or
GMV (aerospace). In addition to that, \UCMshort has a strong track record
in delivering training in C++ in companies such as these and other customers. 
\end{mdframed}





\clearpage
\section{Impact}
\label{sec:impact}

\TODO{Look at this once the rest of the project is together.}


\eucommentary{Describe how your project will contribute to:\\
o the expected impacts set out in the work programme, under the relevant topic;\\
o improving innovation capacity and the integration of new knowledge (strengthening the competitiveness and growth of companies by developing innovations meeting the needs of European and global markets; and, where relevant, by delivering such innovations to the markets;\\
o any other environmental and socially important impacts (if not already covered above).}

\TheProject{} aims to achieve significant impact in most of the areas outlined in the \textbf{H2020-SU-DS-2020} proposal call, providing a tool-supported methodology for developing and deploying secure and privacy-preserving large-scale distributed AI-based data analytic applications. It will allow the software developers to develop their complex data analytics applications using high-level programming models based on design patterns, supporting them in the process of making these applications secure and reducing the number of security and privacy vulnerabilities.  

\subsection{Expected Impacts}
\label{sect:expectedImpacts}

\subsubsection{Short Term Impacts}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 1:}
Reduced number and impact of cybersecurity incidents.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} directly addresses this impact by developing tools to advance the state-of-the-art in the identification of security threats for distributed big data analytics applications, as well as building  self-healing mechanisms to repair the identified vulnerabilities. By combining techniques such as static analysis, dynamic analysis and runtime monitoring for identification of vulnerabilities, we will ensure that we can identify a wide range of threats that are both inherent to the application design, and also threats that can arise from malicious behaviour at runtime. By using a combination of code refactoring and runtime adaptation, we will be able to offer multi-layer security to the end user applications, providing protection both at design/implementation time and at runtime.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 2, 3 } and \textbf{4}; \textbf{Objectives 1, 2, 3, 5} and \textbf{6}.}
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 2:}
Efficient and low-cost implementation of the NIS Directive and General Data Protection Regulation.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} specifically looks into the privacy of sensitive data in big data analytics. We will address multiple aspects of data privacy in this setting, considering privacy of not only data in transit but also of the machine learning models, using noise-adding mechanisms to further ensure that the models themselves do not accidentally reveal sensitive information. We will explicitly consider compliance of the tooling for big data analysis to the GDPR and other regulations, considering all the layers from low-level distributed filesystem/database access to the composition of end user applications using high-level machine-learning libraries.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 2}  and \textbf{3}; \textbf{Objectives 2, 3, 4} and \textbf{5} }
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 3:}
Effective and timely co-operation and information sharing between and within organisations as well as self-recovery;
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
Self-recovery is one of the main themes of the \TheProject{} project, addressed by our self-healing technology based on code refactoring and runtime adaptation. We will, furthermore, integrate this technology into the overall software engineering methodology for developing secure and privacy-preserving AI-based big data analytic applications, making it transparent to the end user and therefore significantly easing the development of such applications. 
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 4:}
Availability of comprehensive, resource-efficient, and flexible security analytics and threat intelligence, keeping pace with new vulnerabilities and threats.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} will develop machine-learning based runtime monitoring mechanisms that will continually monitor the application execution for security threats. Together with the mechanisms for runtime adaptation that will ensure, over a number of steps, that the overall system is in a stable and secure state, this will ensure that we can not only identify the known vulnerabilities, but also, by continuously updating the learning model, identify any new threats as they arise. \TheProject{} will also build on the existing security standards for C++ and will continually extend them as the new types of threats and vulnerabilities are identified. This will ensure that the new vulnerabilities and threats are addressed in the upcoming coding standards. 

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 2, 3}  and \textbf{4}; \textbf{Objectives 2, 3, 4, 5} and \textbf{6} }


\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 5:}
Availability of advanced tools and services to the CERTs/CSIRTs and networks of CERTs/CSIRTs.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
Parts of the \TheProject{} technology (such as secure authentication, formal methods for verifying security properties, refactoring tools and security standards) will be released as open source products, while the others will be integrated into the products and services offered by our commercial partners at \IBMshort{}, \YAGshort{} and \SOPRAshort{}. This will ensure the availability of the tools developed over the course of the project to the security teams, the development teams and the security audit providers, that will be able to exploit them as a part of their security infrastructure. Furthermore, the \TheProject{} technologies will be promoted in numerous ways, including online courses for wide audiences (MOOCs), ensuring there is a wide outreach for our techniques.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aim 5}; \textbf{Objective 8} }

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 6:}
An EU industry better prepared for the threats to IoT, ICS (Industrial Control Systems), AI and other systems;
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} specifically targets distributed systems and AI-based data analytics, aiming to intercept and eliminate threats in such systems. As such, our techniques will be applicable to the general IoT and edge-computing settings as well. By focusing both on core techniques for identification and elimination of security threats, and the user-centered interface and pattern-based methodology for the development of secure applications, we will allow non-experts in security to develop future-proof secure and privacy preserving data analytics applications. Furthermore, mechanisms for formal proving of security properties of applications and for compliance to coding standards will provide further trust in the security of the developed applications.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 1, 2}  and \textbf{3}; \textbf{Objectives 1--6} }

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 7:}
Self–recovering, inter-operable, scalable, dynamic privacy-respecting identity management schemes.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:} %Bearing in mind that almost every citizen who is connected online owns several password-protected accounts, it is evident that identity management  represents nowadays one of the biggest Information Technology markets. 
\TheProject{} addresses core components of identity management by developing and evaluating methods and tools with the active participation of end users to advance the state of the art in: i) multi-factor authentication (MFA) with usable and secure token-based and biometric-based authentication methods; ii) continuous user identification based on AI-driven methods for real-time analysis of physiological, face and eye gaze data analytics; and iii) scalable and dynamic user access management based on Blockchain technology.  
By combining MFA, continuous user identification and access management with a decentralised Blockchain approach, under a unified, scalable, and privacy-preserving identity management solution, we aim to further enhance the security and privacy of current identity management systems. Simultaneously, with our methodology following the user-centered design approach, the developed solution aims at improving the usability, user experience, acceptance and trust towards identity management systems.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 3-5}; \textbf{Objectives 5-7} }
%The aforementioned components will be incorporated under a unified system integration model that combines multi-factor authentication with continuous user identification and access control.
\end{mdframed}

\pagebreak
\subsubsection{Medium to Long Term Impacts}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 1:}
Availability of better standardisation and automated assessment frameworks for secure networks and systems, allowing better-informed investment decisions related to security and privacy.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} will build source code analytic tooling to ensure the conformance of the C++ code to the state-of-the-art coding standards. We will also extend these standards  by providing new rules and guidelines specifically aimed at composing the large-scale distributed applications from multiple layers of libraries. This will contribute to better standardisation of the network systems. Furthermore, the formal methods for proving the security properties of the systems will allow additional formal assurance for the security of software, further allowing better-informed investment decisions with respect to security and privacy.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aim 1}; \textbf{Objectives 1} and \textbf{4}.}

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 2:}
Availability and widespread adoption of distributed, enhanced trust management schemes including people and smart objects.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
%\paragraph{How Will \TheProject{} Achieve This Impact:}
%The \TheProject{} project will produce a coherent, tool-supported and transparent methodology for developing secure and privacy-preserving big data applications. The methodology will be founded on the principle of software engineering patterns for code design and implementation and supported by the rigorous formal methods for verifying security properties of the code, as well as on code analysis and refactoring for compliance to the security coding standards. These standards, as well as the tooling to detect and repair security and privacy vulnerabilities, will be continuously evolving over the course of the project and beyond, making sure they are up-to-date with the new threats that appear. All this will enhance trust of the software developers in the security of the code that will be produced.

%\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aim 4}; \textbf{Objectives 6} and \textbf{7}.}
\paragraph{How Will \TheProject{} Achieve This Impact:} Several technologies of \TheProject{} (e.g.~identity management and intelligent user interface, etc.) will be designed and developed following a user-centered design approach with the active participation of software developers and end users in which evaluation feedback gathered through user studies will lead the development process. 
%Such emphasis on an integrated human-centred design and evaluation philosophy is rarely found in European projects of this kind. 
Since the results target real-world use and aspire to ultimately improve work processes of certain industries, such a user-centred design approach of evaluating the various technologies \emph{in their use} is considered essential. In addition, the proposed user-centered design approach will focus on improving the usability and user experience of security mechanisms, which is a cornerstone aspect within nowadays end user interactive systems, and consequently enhance user acceptance and trust towards the developed technologies.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 3-5}; \textbf{Objectives 5-7} }
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 3:}
Availability of user-friendly and trustworthy on-line products, services and business;
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} aims to make the development of complex, large-scale data analytics applications approachable to non-experts in security. The project will deliver a tool-supported methodology for development and deployment of such applications on public clouds, which will enable the software developers to easily write and adapt their products and services. The uses cases from healthcare, banking and air traffic management domains will demonstrate that our methodology can be use to enhance the security of the existing applications, as well as to develop new secure and privacy-preserving applications.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 4}  and \textbf{5}; \textbf{Objectives 6, 7} and \textbf{8} }

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 4:}
Better preparedness against attacks on AI-based products and system.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
The main target of the \TheProject{} technologies are AI-based products and specifically the emerging large-scale AI-based big data analytics. Our focus on multi-layered big data analytics applications that are deployed on distributed infrastructure and that combine the generic big-data analytics processing platforms such as Hadoop with higher-level machine-learning frameworks such as Azure AI will allow our technologies to be applicable to the widest range of AI-based products and systems. This will be further demonstrated by a variety of the domains of the use cases provided by \SOPRAshort{} and \FRQshort{}. 

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 1, 2} and \textbf{3}; \textbf{Objectives 1--6}}
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 5:}
A stronger, more innovative and more competitive EU cybersecurity industry, thus reducing dependence on technology imports.

\paragraph{Expected Impact 6:}
A more competitive offering of secure products and services by European providers in the Digital Single Market.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
Industry partners involved in the \TheProject{} project, specifically \IBMshort{} and \YAGshort{}, aim to deliver the technologies that will be a TRL of 6 and 7, which indicates the technologies or system prototypes demonstrated in industry relevant environment such as \SOPRAshort{} and \FRQshort{} use cases. This means that the \TheProject{} will produce technologies that will be close to being usable in practice. The technologies developed will be either open source, where feasible, or available as parts of commercial tool suites. This, coupled with the educational program using MOOCs to promote the \TheProject{} technologies, will contribute to the more competitive EU cybersecurity industry and reducing dependence on importing technologies from abroad.

\emph{\TheProject{} aims and objectives related to these impacts: \textbf{Aim 5}; \textbf{Objectives 6} and \textbf{7}}
\end{mdframed}


%\end{mdframed}
%\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{Expected Impact 6:}
%A more competitive offering of secure products and services by European providers in the Digital Single Market.
%\end{mdframed}

%\begin{mdframed}[backgroundcolor=gray!10]
%\paragraph{How Will \TheProject{} Achieve These Impacts:}
%XX
%\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 7:}
A wider understanding amongst developers of the issues involved in Code Security
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve These Impacts:}
One of the objectives of the \TheProject{} project is to develop a methodology for producing secure and privacy-preserving big data analytics applications. In this way, we will look at the software development process, from the design using security patterns to the deployment on distributed architectures, allowing us to understand the issues related to security and privacy that arise in each of the stages of the development. Dissemination and exploitation activities, and in particular the education program that will be implemented in MOOCs, will allow us to communicate the identified issues and solutions to the wider audience.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 4 } and \textbf{5}; \textbf{Objectives 6, 7} and \textbf{8}}
\end{mdframed}

%\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{Expected Impact 5:}
%A more competitive offering of secure products and services by European providers in the Digital Single Market.
%\end{mdframed}

%\begin{mdframed}[backgroundcolor=gray!10]
%\paragraph{How Will \TheProject{} Achieve This Impact:}
%XX
%\end{mdframed}


%\pagebreak
\paragraph*{Improving Innovation Capacity}
\noindent
\TheProject{} will significantly improve long-term innovation capacity by enabling European companies and individuals to develop complex and innovative new secure large-scale big data analytics applications for a variety of commercially-important and emerging markets, including healthcare, air traffic management and digital banking. A combination of different methods for identifying and repairing security risks and privacy leaks, coupled with the formal methods for proving the security properties and continuously evolving security standards will allow identification and repairing of not only current, but also the future security threats, making the software developed using our technologies robust and resilient in future too. The \TheProject{} methodology, together with the associated tool chain, will significantly reduce the level of expertise required from the developers in developing secure code, allowing non-expert in this area to develop software that is provably secure and automatically checked for the conformance to standards. This will bring a major boost to the European markets in software, as the security and privacy will become the most important factors when designing and implementing the new software, especially in the light of the increased distribution of data across domains in parallel with more and more strict regulations governing privacy and ownership of the data. SMEs involved in the project will use our software to create additional market opportunities and the project itself will support the growth of these companies by significantly extending their offer. Section~\ref{sec:innovationpotential} (Page \pageref{sec:innovationpotential}) describes the market opportunities and potential for innovation in more detail.

\paragraph*{Societal Impact.}
\noindent
\TheProject{} will have a major impact on society through its focus on delivering secure and privacy preserving data analytics code. Our methodology will allow for \emph{secure-by-design} application development, meeting the ever-growing societal needs for secure complex data analysis and promote ensuring of the privacy of the data in light of new and more strict regulations such as GDPR. One of our goals is to support companies in enforcing compliance to GDPR in their applications by developing \emph{privacy-by-design} technologies and methodology. With the use cases from various domains, we will also deliver significant impact in the areas not directly related to computer science. By investigating mechanisms for secure knowledge transfer between geographical areas, we will allow faster and more secure cooperation between institutions which is especially important in the Healthcare domain, where there is a growing trend towards decentralisation of the data and diagnostics and for shared learning that breaks the boundaries between institutions and countries. Remote collaboration and knowledge sharing is of critical importance to the world affected by the COVID-19 where knowledge exchange will need to be facilitated in the presence of strict social distancing rules.

\begin{mdframed}[backgroundcolor=blue!5]
\label{box:gender}
\textbf{Gender Imbalance.} One of especially important areas for societal impact that the \TheProject{} project is committed to is reducing the gender imbalance in Computer Science. By explicitly addressing the issues of preserving the privacy of data, we are hoping to help in reducing gender bias and under-representation of minorities. In addition, Data Science is one of the areas of Computer Science which has much better gender balance than the others. 
%
\TheProject{} is concerned with the protection of personal data (such as gender, ethnicity, etc). The framework for examining the right to privacy from a gender perspective is found in the Universal Declaration of Human Rights (Article 12), the International Covenant on Civil and Political Rights (Article 17) the Resolutions of the United Nations' General Assembly and Human Rights Council, and the international human rights legal framework. The UN Human Rights Council (HRC Resolution 34/7 has noted that \textbf{``violations and abuses of the right to privacy in the digital age may affect all individuals, including with particular effects on women, as well as children and persons in vulnerable situations, or marginalized groups''}). By protecting privacy, the \TheProject{} contributes to preventing violations and abuse of the right to privacy in the digital age.    %\vjcomment{Evidence of this?}. 
Providing the educational program that will bring together Data Science with areas that are traditionally dominated by men (such as Cybersecurity), we are hopping to contribute to reduce the imbalance in the latter. Finally, by explicitly looking at the privacy-preserving methods for training the machine learning models, we are hoping to reduce the well known bias in training sets for ML towards white men. \vjcomment{How?} \vjcomment{We also need a reference to the Stanford paper about equality.}
\end{mdframed}
 

\noindent
\subsubsection*{Possible Barriers to Achieving the Expected Impacts and Associated Mitigations}

\newcounter{barrier}

\begin{longtable}{|p{125pt}|p{320pt}|}%\hline

\hline \textbf{Possible Barrier}&

\textbf{Mitigation}\\ \hline
\endfirsthead

\multicolumn{2}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous
page}} \\ \hline
 \textbf{Possible Barrier}&
\textbf{Mitigation}\\ \hline
\endhead

\hline \multicolumn{2}{|r|}{{Continued on next page}} \\ \hline
\endfoot

\hline \hline
\endlastfoot


\addtocounter{barrier}{1}
\noindent
\emph{Barrier \thebarrier.}
\TheProject{} technologies are unable to significantly reduce incidence and/or impact of the cybersecurity incidents in AI-based big data applications.
&
\noindent
The consortium comprises a mix of partners with a long experience in developing technologies for security (\IBMshort{}, \YAGshort{}, \COGNIshort{} and \SCCHshort{}), privacy (\SCCHshort{} and \SOPRAshort{}) and %partners with 
expertise in big data analytics (\UODshort{} and \SOPRAshort{}). Adding to this the realistic use cases that will be considered (\SOPRAshort{} and \FRQshort{}), as well as the expertise in conformance to standards, refactoring, formal specification and verification of properties (\SAshort{} and \UCMshort{}), our consortium is ideally placed to tackle the problem presented. Therefore, the quantifiable benefits should be fully attainable.
\\ \hline
\addtocounter{barrier}{1}
\noindent
\emph{Barrier \thebarrier.}
Self-Healing techniques developed over the course of the project cannot repair the identified security and privacy vulnerabilities.
&
\noindent
The consortium 
%The project partners, especially \SAshort{}, \SCCHshort{} and \SOPRAshort{}, already have 
%has academic and industrial 
has experience in developing techniques for modifying code to increase its security and privacy. Furthermore, \TheProject{} will develop two complementary methods for self-healing, one based on static code modification (code refactoring based on established secure code patterns) and another based on runtime adaptation. This will ensure that each vulnerability is treated by different methods, significantly increasing the chance of it being addressed successfully. 
\\ \hline
\addtocounter{barrier}{1}
\noindent
\emph{Barrier \thebarrier.}
The \TheProject{} techniques are unable to deal with new threats that arise.
&
\noindent
\TheProject{} will include self-healing methods that are based on monitoring the overall health of the system, rather than identifying any particular threat. This, combined with ensuring conformance to general security coding standards and formal methods to verify security properties that are both also independent of any particular threat, will enable our technologies to address not only existing, but also new threats that arise.
\\ \hline
\addtocounter{barrier}{1}
\noindent
\emph{Barrier \thebarrier.}
Formal methods developed over the course of \TheProject{} cannot verify important security properties.
&
\SA and \SCCHshort{} have experience in integrating formal methods to avoid scalability issues (e.g. opting for SMT solvers instead of model checkers when possible). In addition,
\TheProject{} follows the vision that complementary techniques are used to check for security vulnerabilities
(e.g. static analysis, symbolic execution, runtime monitoring).
%and in case they are present tackle them through self-healing. 
We have tasks set specifically to determine whether security breaches have been missed by formal verification and vice versa.
% Combining static analysis with symbolic execution, as outlinedin Section 1.4.2, will contribute to the reduction of these false positives by focusing on the parts of thecode where these potential threats lie and profiling them to established whether security is really breached.Additionally, runtime monitoring w
%We will be able to will use a variety of techniques to check that code satisfies security properties and 
\\ \hline
\addtocounter{barrier}{1}
\noindent
\emph{Barrier \thebarrier.}
\TheProject{} does not contribute to better understanding of security and privacy issues.
&
The \TheProject{} methodology will include clear guidelines on developing secure and privacy-preserving big data analytics. In particular, security patterns will include guidelines on how to compose such applications from components addressing different layers of abstraction, identifying issues arising from each of the components 
and from interactions between components. This will lead to a better understanding of the security and privacy issues involved in developing such complex applications. The use cases will verify the understandability of our principles, as they will be developed by software developers that might not be experts in security.
\\ \hline
\addtocounter{barrier}{1}
\noindent
\emph{Barrier \thebarrier.}
The \TheProject{} methodology is not adopted by a wider community.
&
\noindent
The benefits of the \TheProject{} methodology will be explained clearly and supported by both mathematical (formal reasoning) and software engineering (design patterns) foundations. Evidence of the improvements in security and privacy will be communicated to possible users and the \TheProject{} technologies and methodology will be promoted to wide audiences by MOOCs. Furthermore, the project consortium contains experienced software
security developers who will be able to identify any barriers in the methodology itself to its more widespread adoption. The expert advisory board will also be consulted for further advice on promoting and facilitating adoption.

\end{longtable} 

%\draftpage
\subsection{Measures to Maximise Impact}
The impact of \TheProject{} will be maximised by:
\begin{inparaenum}[i)]
\item
demonstrating the applicability of the \TheProject{} technology in developing secure and privacy-preserving data analytics on real-world use cases taken from the domains of air traffic management, healthcare
and digital banking; % and telecommunications;
\item
incorporating security and privacy into the software development process from the design phase by using security design and implementation patterns;
\item
ensuring that security and privacy requirements are met, especially concerning the regulations for ownership and sharing of the personal data such as GDPR;
\item
engaging directly with and taking feedback from the user and developer community, including specific user community development activities;
\item
building on and using technologies that are used commercially in a large-scale data analytics, as well as extending the specific tooling offered by the industrial project partners from \IBMshort{}, \YAGshort{} and \COGNIshort{}.
\item
demonstrating the utility of the \TheProject{} technology and tools through deploying a variety of use case applications;
\item
extending the existing security coding standards to address the problems that appear in composition of big data analytics applications from different layers;
\item
deploying an exploitation plan that links with the strategic needs and internal development plans of the companies and other organisations that are involved in \TheProject;
\item
publishing results extensively, both in technical areas, and through press releases, radio interviews, news items and the general technical press;
\item
engaging with relevant networks of excellence, such as HiPEAC to ensure widespread dissemination of
project results and uptake by third parties;
\item
developing an educational program to promote the uptake of the \TheProject{} technologies by a wider audience of developers with diverse backgrounds in computing;
\item
exploiting nationally-funded opportunities for the formation of spin-off companies and for commercial exploitation of research results;
\item
utilising open development and dissemination mechanisms as far as is practically possible and consistent with the needs of good exploitation; 
and
\item
teaching the achieved results in advanced Masters and PhD courses that are offered by the academic beneficiaries at \SAshort{}, \UODshort{} and \UCMshort{}. 
\end{inparaenum}
%
The following section describes these measures in more detail.

\subsection{Dissemination and Exploitation of Results}
\label{sect:dissemination}

\eucommentary{Provide a draft 'plan for the dissemination and exploitation of the project's results' (unless the work programme topic explicitly states that such a plan is not required). For innovation actions describe a credible path to deliver the innovations to the market. The plan, which should be proportionate to the scale of the project, should contain measures to be implemented both during and after the project.
Dissemination and exploitation measures should address the full range of potential users and uses including research, commercial, investment, social, environmental, policy making, setting standards, skills and educational training.
The approach to innovation should be as comprehensive as possible, and must be tailored to the specific technical, market and organisational issues to be addressed\\
o Explain how the proposed measures will help to achieve the expected impact of the project. Include a business plan where relevant.\\
o Where relevant, include information on how the participants will manage the research data generated and/or collected during the project, in particular addressing the following issues:\\
o What types of data will the project generate/collect? o What standards will be used? o How will this data be exploited and/or shared/made accessible for verification and re-use? If data cannot be made available, explain why.
o How will this data be curated and preserved?}



\subsubsection{Draft Dissemination Plan}

We will focus on propagating our results both to the computer science
research community and to potential users of the \TheProject{} technology.
We will do this through a mixture of high-quality publication, presentations
and direct engagement with the user community.
%
The main research communities that we expect to target are:
cybersecurity, distributed systems, big data, AI, programming languages, parallel programming and formal methods and logic. 
% 
%We anticipate that the primary users of our technology will be:
%XX.

\paragraph{Scientific Publications:}  The main routes to good scientific dissemination are
through peer-reviewed publication, and through presentation of results at key scientific events.
\TheProject{} partners will therefore aim to produce high-quality \emph{peer-reviewed
research publications} in relevant leading
conferences, technical workshops and journals.
We will build on the existing good publication records of the \TheProject{} partners,
aiming to produce a sizeable volume of good quality publications in the course of the project. 
%
\noindent
The conferences that we propose to target include:
\vjcomment{Update these.}

\begin{quote}
\textbf{IEEE S\& P:} International Symposium on Security and Privacy;
\textbf{ACM CCS:} International Conference on Computer and Communications Security;
\textbf{ICSIC:} International Cyber Security \& Intelligence Conference;
\textbf{European Security Summit};
\textbf{ICSE:} International Conference on Software Engineering;
\textbf{ACM IUI:} International Conference on Intelligent User Interfaces;
\textbf{ACM UMAP:} International Conference on User Modeling, Adaptation and Personalization;
\textbf{USENIX SOUPS:} International Symposium on Usable Privacy an Security;
\textbf{IEEE BigData:} International Conference on Big Data;
\textbf{IEEE ICDE:} International Conference on Data Engineering;
\textbf{IEEE IPDPS:} International Symposium on Parallel and Distributed Systems;
\textbf{ACM PPOPP:} International Symposium on Practice and Principles of Parallel Programming;
\textbf{ICML:} International Conference on Machine Learning;
\textbf{IEEE CLOUD:} International Conference on Cloud Computing;
\textbf{IEEE/ACM CCGRID:} International Symposium on Cloud Computing and Grid;
\textbf{EUROPAR:} International European Conference on Parallel and Distributed Computing; 
\textbf{HCC:} The Healthcare CyberSecurity Conference;
\textbf{AIME:} International Conference on Artificial Intelligence in Medicine;
\textbf{PLDI: } ACM International Conference on Programming Languages, Design and Implementation;
\textbf{POPL: } ACM Conference on Principles of Programming Languages;
\textbf{CC: } ACM International Conference on Compiler Construction;
\textbf{ICFP: } ACM International Conference on Functional Programming;
\textbf{ESOP: } International Conference on Programming;
\textbf{SPLASH/OOPSLA: } ACM SIGPLAN Conference on Systems, Programming, Languages and Applications
\textbf{CGO: } International Symposium on Code Generation and Optimization;
\textbf{ ECOOP: } European Conference on Object-Oriented Programming;
%\textbf{HLPP: } International Symposium on High-Level Parallel Programming and Applications;
\textbf{HiPEAC: } High-Performance Embedded Architectures and Compilation Conference;
\textbf{PPDP: } International Symposium on
Principles and Practice of Declarative Programming;
\textbf{ICSE: } International Conference on Software Engineering;
\textbf{IJCAI:} International Joint Conference on Artificial Intelligence;
\textbf{AAAI:} Conference on Artificial Intelligence;
\textbf{ETAPS:} European Joint Conferences on Theory \& Practice of Software.
\textbf{IEEE/ICNS:} Integrated Communications Navigation and Surveillance Conference;
\textbf{IEEE/DASC:} Digital Avionics Systems Conference;

 \end{quote}

\noindent
We also propose to target relevant high-impact journals such as:
\begin{quote}
\textbf{TOPS:} ACM Transactions on Privacy and Security;
\textbf{TDSC:} IEEE Transactions on Dependable and Secure Computing;
\textbf{JISA:} Journal of Information Security and Applications;
\textbf{CSIAC:} The Journal of Cyber Security and Information Systems;
\textbf{TOSEM:} ACM Transactions on Software Engineering and Methodology;
\textbf{TiiS:} ACM Transactions on Interactive Intelligent Systems;
\textbf{JDSA:} International Journal of Data Science and Analytics;
\textbf{TPDS:} IEEE Transactions on Parallel and Distributed Systems;
\textbf{IJPP:} International Journal of Parallel Programming;
\textbf{TOPLAS: } ACM Transactions on Programming Languages and Systems;
\textbf{CC-PE: } Concurrency and Computation: Practice \& Experience;
\textbf{FGCS: } Future Generation Computing Systems;
\textbf{PPL: } Parallel Processing Letters; 
\textbf{SP\&E: } Software: Practice \& Experience; 
\textbf{TCS:} Theoretical Computer Science;
\textbf{FAC:} Formal Aspects of Computing.
\end{quote}

\paragraph{Project Web site:}  
A crucial component of the \TheProject{} dissemination strategy is a
high-quality project website. The public section will provide ample
and consistent information about all aspects of the \TheProject{}
project, with the goal of positioning the \TheProject{} website as a
prime information source for relevant scientific and technical
information.  The vast majority of the \TheProject{} deliverables are
public, and full access to these will be provided.  The web site will also contain lists of publications and links
to open-access repositories; copies of technical reports and white
papers; a news feed; technical documentation; downloadable software
and pre-installed virtual machines; video demonstrations; online
tutorials; information about project partners; copies of
presentations, podcasts and other material; data and results; plus
links to the Horizon 2020 programme in general and to related 
Horizon 2020 research projects in order to highlight the role played by \TheProject{} within the
broader EC research framework.

\paragraph{Project News Feed:}  We will set up open public mailing lists/twitter/facebook accounts that will
be used to communicate project news and results to interested parties, whether they are scientists, academics, developers
or the general public.  This news feed will be highlighted on the project web site. We will also create a LinkedIn project page to 
communicate the project information to the business-oriented audience.

\paragraph{Developer and User Community:} We will engage with the broader
developer and user communities by a series of focused activities, the most important of which will be organisation of massive online courses via MOOCs, allowing us to engage with a huge base developers and to promote the \TheProject{} technologies.
 We will also organise dedicated user community workshops, presentations at
 developer and other conferences, 
 the production of animation videos for the general community (which we have used in the EU H2020 project Serums), posters, delivering tutorials, staffing booths, and providing hands-on
 guidance in the use of our tools and technologies etc.  This direct engagement will be
 supported by the production of video demonstrations, training materials, tutorials and documentation that can
 be accessed through the project web site.  We will also aim to produce white papers and slide sets that
 can be used to explain the benefits of the \TheProject{} approach to prospective users, both developers
 and managers.

 \paragraph{Expert Working Groups and Networks of Excellence:}
In order to ensure good dissemination to the research and development
community, including industrial researchers, we propose to disseminate
our project results through the most relevant scientific/technological
networks and working groups, including HiPEAC, %  the TACLe Cost Action on Timing Analysis, the Ercim DES Working Group, 
IFIP Working Groups 2.11 \& 10.3, and other EU and national projects,
as well as national groups such as the UK's DataLab.
\SAshort{} is a full member of HiPEAC and a charter member of IFIP working
group 2.11, and has been heavily involved in activities organised by these and other expert groups.
These provide a high-level interface between academic and industrial interests, and a valuable
melting pot for ideas and new technologies. 

\paragraph{Standardisation Committees:}
% \khcomment{We should mention any committees that we are members of. Remove this section if not relevant.}
This proposal is tied to Standards from the beginning, through the end and beyond. It leverages the key leadership position of several members, acting as senior leaders, officers, working group experts, proposal authors, collaboration with other industry and academic experts.
\UCMshort{} has been a member of the ISO/IEC JTC1/SC22/WG21 (C++ standard committee) for more than a decade and has contributed to the C++11, 14, 17 and 20 standards. It also participates in the joint group with SC22/WG23 (Programming Language Vulnerabilities) where a specific guide for C++ vulnerabilities is under development.
\FRQshort{} is member of the EUROCAE Working Group 114 - Artificial Intelligence. The primary scope of this group is to prepare technical standards, guides and any other material required to support the development of systems and the certification of aeronautical systems implementing AI-technologies. In addition \FRQshort{} is involved in SAE's G-34, Artificial Intelligence in Aviation.

%% Doesn't make sense?
% Current C++ Standards only support CPU, though it is adding parallel programming support starting with C++ 11, enhanced through C++ 17 with parallelSTL. Yet it is still lacking many things that can support Heterogeneous computing, which has been already explored by SYCL and OpenCL. 
%
%

\paragraph{General Scientific and Technical Community:}
We will further engage with the general scientific and technical community
 by participating in relevant workshops, conferences and clustering events (including ones which
 will develop our technologies beyond the bounds of our own research communities), by engaging with the
 different EC-sponsored CORDIS information channels, by presenting at trade
 fairs, and through other dissemination activities.  We will use materials such as presentations,
 papers,  posters, demonstrations and the project web site to do this.


\paragraph{Education:} We will target 
the educational communities through the production of relevant educational materials, that will
also have a training benefit. Young researchers, software
 developers and application programmers will learn how to
 use our software technologies in their
 respective fields. This is aligned with recent EC
 initiatives on the subject such as ``Increasing the Attractiveness
 of Science, Engineering \& Technology Careers''.
 The academic consortium partners will integrate Bachelor,
 Master, Diploma, and PhD students into the \TheProject{}
 project whenever possible, e.g.~through PhD and MSc theses, student
 projects, academic courses, and research seminars. 
 We will also take advantage of opportunities to engage with broader
 audiences through guest lectures at other institutions and summer schools etc.
 These students will
 carry the methodology and techniques of the \TheProject{}
 project into their future work places in the ICT industry.
 In that way the \TheProject{} vision and the project
 results will disseminate into many research groups and
 companies working in the field of security. 
 \paragraph{MOOC:}In order to disseminate security and the work of the consortium to the wider public a MOOC (massively open online course) will be developed to reach an audience far beyond the academic and industrial community.  MOOCs provide access ---often to hundreds or thousands of people at a time--- to focused online courses with learning goals and paths that present the opportunity to learn together at a flexible pace. Participants start and complete the training course on the MOOC at the same time as a cohort, to create community and for peer learning. The provision of a MOOC will also be
used to enthuse and educate citizens about security and in particular the work of the project.  Work packages will provide accessible materials (text, videos, audio clips) highlighting the work that is being carried out. \UODshort{} brings a long‐term, established partnership to a market leading MOOC, FutureLearn, which will be supported far beyond the funded period. The FutureLearn MOOC is free and open, and brings a highly scalable platform into the project ecosystem, capable of sustaining more than 100,000 learners at one time.

% \paragraph{General Communication:} We will adopt a good general communication strategy
% aimed at maximising the outreach of the \TheProject{} project to the public at large.  We will issue
% regular press releases describing relevant events and research results, conduct
% radio/TV interviews, adopt an open policy to disseminating our research results through use of appropriate
% repositories for publications and the project web site, use public media such as
% \emph{twitter} to communicate project results, engage with public lectures and seminars,
% write general articles for newsletters, newspapers etc. as described in \ref{wp:dissem}.

\begin{quote}

\emph{Although all these dissemination activities
will be centrally coordinated, the full and active participation of
all partners is expected. Key project representatives from
the various \TheProject{} academic and industrial
partners will also arrange specific meetings
with scientific, commercial, industrial, and/or
governmental representatives to facilitate public
engagement.}
\end{quote}

%\pagebreak

\subsubsection{Draft Exploitation Plan}
\label{sect:exploitation-plan}
\vspace{-12pt}
The main avenue for the exploitation of the \TheProject{} results will be through integrating the developed technologies in the infrastructure software products offered by the industrial partners from \IBMshort{}, \YAGshort{} and \COGNIshort{}. The developed technologies are directly relevant to the strategies of these companies. A secondary avenue will be through the enhancements of services offered by use-case partners from \SOPRAshort{} and \FRQshort{} by adding a security layer on top of their existing end user applications. The third avenue will be through the additional value of consultancy services offered by \SOPRAshort{}, \SCCHshort{} and \UODshort{}, where the \TheProject{} technologies will ensure that the safe and privacy-preserving code can be delivered in significantly reduced time than before. The fourth avenue will be through participation in standardisation commitees and working groups and influencing the standards and guidelines produced by them (\UCMshort{} and \FRQshort{}). Finally, the last avenue will be through education, by extending the existing and building new modules and courses and educating a future-generation of software developers in developing secure data analytics (\SAshort{}, \UODshort{} and \UCMshort{}). All partners will have the right to use foreground IPR for the purposes of the project. In order to ensure that all contributions are recognised, exploitation plans will be shared with the consortium as a whole. Partners will not, however, have the right to veto or delay exploitation unless their own IPR is directly involved.


\horizontalline

\subsection*{Draft Exploitation Plan for \IBMshort{}}

\begin{wrapfigure}{R}{4cm}
\vspace{-1.4cm}
\hfill \includeimage[width=4cm]{logos/ibm.jpg}
\vspace{-0.6cm}
\end{wrapfigure}

IBM will aim to deliver \TheProject technologies into IBM cloud and analytics products and services, aiming to commercially benefit both citizens and businesses across Europe. \TheProject technologies are relevant to improve many IBM products and services, such as:
\begin{itemize}
    \item IBM Cloud Pak for Data. It is a fully-integrated data and AI platform that modernizes how businesses collect, organize and analyze data and infuse AI throughout their organizations. Built on Red Hat OpenShift Container Platform, IBM Cloud Pak for Data integrates market-leading IBM Watson AI technology with IBM Hybrid Data Management Platform, data ops, and governance and business analytics technologies. Together, these capabilities provide the information architecture for AI that meets your ever-changing enterprise needs. The enhanced code vulnerability detection technologies combined with novel techniques for repairing the identified vulnerabilities that will be developed in the \TheProject project will significantly enrich the ability to develop secure and robust AI-driven big data applications. In addition, the IBM Cloud Pak for Data is likely to benefit from the results of \TheProject in the areas of security and protection of Big Data and new big data analytics.
    \item IBM Security Portfolio. We will aim to enrich the existing IBM security solutions by exploiting the advanced vulnerability detection and code repairing technologies that will be developed and enhanced in the \TheProject project to commercially benefit businesses across Europe.
\end{itemize}

\horizontalline

\subsection*{Draft Exploitation Plan for \SCCHshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/SCCH.jpg}
\vspace{-0.8cm}
\end{wrapfigure}


\SCCHlong{} as a non-profit independent research institution (RTO) with a focus on applied software and data science and the mission to transfer cutting edge research from academics to industry by running contract research projects, pursues a strategy of closely collaborating with world-leading academic partners to establish and further develop key technologies together with industrial partners. This is achieved by conducting common applied research with its industrial partners, and strategic research aiming at scientific publications, PhD theses and patents. Therefore \SCCHshort{} aims to exploit  \TheProject{} project achievements as follows.

\paragraph{Industrial Exploitation.} \SCCHshort{} runs application-oriented projects with industrial partners (e.g. voestalpine Stahl GmbH, Rubble Master HMH GmbH, STIWA Holding GmbH, Siemens, KEBA AG) in the area of intelligent data analysis. The scope of applications span: a) predictive control via advanced supervised and semi-supervised learning methods like deep learning or kernel methods; and b) decision support systems and causal reasoning by inductive rule extraction, regression models, Bayesian networks and automated reasoning techniques. As an application oriented research institute, \SCCHshort{} commonly deals with very large and inhomogeneous data sources and has experience with data processing of big data and transfer learning techniques that enable the joint modelling from inhomogeneous data, collected from distributed sources. The \TheProject{} approach will enhance collaboration in ongoing and future multi-firm research projects, which would not be possible due to confidentiality of data of the project partners. \SCCHshort{} will also exploit \TheProject{} technology in IoT projects, especially cyber-physical-systems, where personal data of humans in the loop needs to be kept confidential. \SCCHshort{} will promote \TheProject{} technology as the basis for collaborative development of business clusters (e.g. Mechatronics) by a data driven approach: gain knowledge out of shared data while not compromising privacy. In the development of the Data Analytics Framework \SCCHshort{} plans to extend established machine learning frameworks (TensorFlow, Keras, scikit-learn) to find early adopters and foster uptake of developed technologies in the community. 
%
As part of application-oriented research projects with partners companies, \SCCHshort{} has built and applied reverse engineering and analysis tools with a strong focus on extracting knowledge from source code and related software engineering artifacts (\url{http://codeanalytics.scch.at/}). The results are available via eKNOWS, a platform for the analysis and evolution of software systems, documentation generation, and quality assurance and automated testing. The innovative approach to extract formal specifications from source code proposed in the \TheProject{} project will be integrated in the eKNOWS platform. This will give us a solid platform for the exploitation of \TheProject{} envisioned results in the area of formal specification of refactorings for increasing the security and privacy level of big-data applications, together with new verification techniques to show general soundness and correctness of such refactorings. More generally, we will be able to expand the applicability of software engineering tools developed within the eKNOWS platform for standard software systems, to big distributed data analytics systems. Given the expertise of \SCCHshort{} on applied research projects with company partners in the area of data sciences, as well as the increasing importance that security and privacy plays in that area, there is plenty of potential to exploit these innovations working together with our current and future company partners in Austria and Europe.

 \paragraph{Scientific Exploitation.} In parallel to common applied research, \SCCHshort{} has to conduct fundamental research to keep in contact with world-leading research. Furthermore \SCCHshort{}'s success is also measured by its scientific output like number of publications, PhD and master theses. \SCCHshort{} plans to submit publications and articles in highly relevant conferences and journals like Neural Information Processing Systems, International Conference of Learning Representations, Journal of Machine Learning Research, IEEE Symposium on Security and Privacy, IEEE European Symposium on Security and Privacy, Theory and Practice on Differential Privacy co-located with ICML, Journal of Privacy and Confidentiality, Springer Journal of Big Data, Science of Computer Programming, Theoretical Computer Science, Journal of Computer and Systems Sciences,  International Conference on Formal Engineering Methods, International Conference on Rigorous State Based Methods, International Conference on Software Engineering. The \TheProject{} approach will increase scientific output for \SCCHshort{} since it allows for publications that would not be possible due to confidentiality of the data the publications are based on.


\horizontalline

\subsection*{Draft Exploitation Plan for \SOPRAshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/Sopra-Steria-logo2.png}
\vspace{-0.8cm}
\end{wrapfigure}

\SOPRAlong{} will use the tools that we develop in \TheProject{} project and deploy them against the various software development solutions the company develops for use in various private and public companies within Europe. \SOPRAshort{} has software installations spread over twenty-five countries. The company covers business domains such as Aerospace, Defence plus Security, Energy plus Utilities, Financial Services, Insurance plus Social, Government, Retail, Telecommunication, Media plus Entertainment, Transport and IT consulting via our 40K+ consultants. The new security methodology produced as part of \TheProject{} will enhance the future delivery capability of \SOPRAshort{} plus customers our consultants support within the customers own systems. The outcome of this research will generate a significant enhancement to the speed we can generate new solutions, while ensuring the code is secure and of good quality. The improvement of the security of the software that \SOPRAlong{} uses and creates is a safeguard against security breaches or privacy leaks for our customers’ applications. These customers are critical cornerstones of the economy as they are responsible for services like banking, healthcare, public safety and in some cases the governments itself. If we had a way to ensure better and safer code is generated, we will mitigate these risks by improvement of the code base itself. This is a significant advance for the whole of Europe. Privacy Rights Clearinghouse published these figures via Forbes, reported over 4.8 billion in 2016, 2 billion in 2017 and nearly 1.4 billion in 2018. The issues is these are only the reported incidents. The true numbers are unknown and suspected to be factors more as companies are either not seeing the issues or not reporting them. {\project{Elysian}} is a step towards improvement of this situation.

\horizontalline

\subsection*{Draft Exploitation Plan for \FRQshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/FRQ_logo.png}
\vspace{-0.8cm}
\end{wrapfigure}

\FRQlong{} will broaden its expertise and extend current products in the field of privacy-preserving data sharing platforms.
\TheProject{} will be used to establish and strengthen important partnerships for cooperation in the creation of AI services and products to increase capacity in the aviation industry. Foster the transition of \FRQlong{} from a world leading voice communication provider to building the best safety-critical data communication solutions in ATM combination of emerging technology.
\FRQlong{} safety-critical communication and information solutions leverage more than seventy years of experience showcasing true leadership in this market. With deep cross-industry experience in civil aviation, defence, public safety, maritime and public transportation markets; the company has built upon its initial control centre solutions in these five areas of core competence. The outcomes of \TheProject{} will also be extended to other domains as mentioned above.

\horizontalline

\subsection*{Draft Exploitation Plan for \YAGshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/YAG-logo.png}
\vspace{-0.8cm}
\end{wrapfigure}

 \YAGshort{} will deliver \TheProject technologies into YAGAAN product and services, aiming to commercially benefit businesses across Europe. The YAG-Suite is the \YAGshort{} product which supports developers in detecting vulnerabilities in their source code along security and privacy by design development processes. It is in the aim of the Company to improve the YAG-Suite with \TheProject technologies that will be relevant to extend its addressable market and widen the tooled services it delivers, such as:
\begin{itemize}
    \item YAG-Suite improvement for developers. The enhanced static code analysis and security breaches detection technologies combined with novel techniques for vulnerabilities remediation that will be developed in the \TheProject project will enrich the YAG-suite capabilities.
    \item YAG-Suite improvement for privacy assessment of big data related applications: To support GDPR compliance assessments, \TheProject technologies will significantly enrich the YAG-Suite for \YAGshort{} to offer new source code privacy breaches detection capabilities and support European businesses in delivering better privacy compliant applications to the EU citizens. Additionally \TheProject will support \YAGshort{} in accessing the new market segment of big data application producers.
    \item YAG-Suite improvement for code auditors: As YAG-Suite not only is used by developers but also by service providers who deliver source code audits, the same capabilities will also benefit to audit service providers.
\end{itemize}
It is also in \YAGshort{} strategy to build business partnerships to support its partners in addressing new markets and disseminate \YAGshort{} technology, or to accelerate the growth of the startup, partnering with value added distributors. It is finally in the company strategy to build business alliances with integrators or technology providers: potential co-operations with \TheProject{} partners will be discussed during the project.

\horizontalline

\subsection*{Draft Exploitation Plan for \COGNIshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/COGNI-logo.png}
\vspace{-0.8cm}
\end{wrapfigure}

\COGNIshort{} will broaden its expertise and extend its products and services in the field of usable security, intelligent user interfaces and artificial intelligence aiming to help businesses in Europe to design, develop and integrate human-centered cybersecurity systems.
%
Cognitive Identity is \COGNIshort{}'s core identity management platform, which allows the seamless integration of a variety of authentication methods for secure and usable multi-factor authentication and continuous user identification in a variety of domains, including the digital banking domain for customer identification and verification, the digital health domain for patient authentication, the digital education domain for online examination invigilation purposes. Main aim of \COGNIshort{} is to improve and extend its current methods and components of the platform, specifically:

\begin{itemize}
    \item Usable and secure multi-factor authentication. Improve and extend its current multi-factor authentication portfolio with novel authentication methods that will be developed in the \TheProject project.
    \item Intelligent biometrics for continuous user identification. Improve and extend its currently developed and evaluated predictive models based on physiological, face and eye gaze-driven metrics and features.
    \item Scalability with Blockchain technology. Enhance its products
    and services with benefits provided by Blockchain technology such as decentralized and security features.
    \item Privacy-preserving biometrics. Enhance the continuous user identification methods for preserving the privacy of the users' biometric data used for continuous identification.
\end{itemize}

Furthermore, one of \COGNIshort{}'s main goals is to create a working environment rich in opportunities for product impact. To do so, its activities are highly driven by basic research and fundamental applied research with active participation in accredited conferences and workshops of the relevant fields. \COGNIshort{} plans to submit publications and articles in conferences and journals like USENIX Usable Privacy an Security, ACM SIGCHI, ACM Intelligent User Interfaces, ACM User Modeling, Adaptation and Personalization, ACM Transactions on Interactive Intelligent Systems, etc.

Finally, \COGNIshort{} organizes local and international workshops targeting industrial and research partners in which the Elysian tools and technologies will be promoted, e.g., through the International Workshop on Adaptive and Personalized Privacy and Security (co-located with the ACM Conference on User Modeling, Adaptation and Personalization) which is co-organized by members of \COGNIshort{} and \USTANshort{}. \COGNIshort{} will also draw upon its network of connections with entrepreneurs, researchers and academics related to the specific topic of Usable Security, Artificial Intelligence, Human-Computer Interaction, e.g., University of Cyprus, University of Patras, Novartis Switzerland, Ludwig Maximilians University of Munich, SAP SE Germany, University of Leeds, Shanghai Jiao Tong University, Technion Israel Institute of Technology, etc.

%In addition, the company will use the \TheProject tools and technologies in its identity management-related practice to help digital service providers across Europe to integrate, deploy and manage multi-factor authentication and continuous user identification solutions by reducing the complexity of development, but at the same time provide a positive security experience to the end users. 

\horizontalline

\subsection*{Draft Exploitation Plan for \SAshort{}}

\begin{wrapfigure}{R}{2.5cm}
\vspace{-1.4cm}
\hfill \includeimage{logos/st-andrews-logo.jpg}
\vspace{-0.9cm}
\end{wrapfigure}

\SAshort{} will work actively to exploit the foreground IPR that it produces in the course of the \TheProject{} project. During the course of the \TheProject{} project, we envisage the development and production of new and innovative refactorings for increasing the security level of big data applications, together with new verification techniques to show general soundness and correctness of the refactorings and identify optimal refactorings for self-healing to avoid particular security vulnerabilities. The intention is to release these tools as open-source software, allowing their widespread exploitation by the community. We will also work with the industrial collaborators to further exploit these tools and other foreground intellectual property that we will produce in the course of the project. 
Additionally, \SAshort{} will actively pursue avenues of commercial exploitation with the possible formation of a spin-off company to commercialise the project results, and apply for patents, etc. Dr Brown already has startup experience, with the former spin-off of ParaFormance technologies Ltd. from previous EU research on the H2020 RePhrase project. Scotland provides an excellent and vibrant base for commercial exploitation, with additional support available from the Scottish Enterprise and other sources.  In Edinburgh alone, 162 start-up/spin-off companies have been formed from academic research since 2007, and two new science parks are currently being developed. Edinburgh TechnoPole is a new 126-acre science and technology park that offers flexible accommodation to new and growing high-technology businesses. At St Andrews, the new Eden business campus will provide an excellent base for growing the new startup company. \SA has excellent connections within the UK and Scottish communities.

\textbf{Teaching.} \SA aims to exploit the results of the \TheProject{} project throughout its teaching profile. It offers advanced undergraduate and  MSc %, and PhD 
level courses in parallel programming, programming language design and implementation, concurrency, image processing, data analytics, critical systems engineering, artificial intelligence and verification. These courses will be able to benefit from the \TheProject{} results on refactoring, analysis tools, security, formalisation, security patterns and coding standards. Results will be integrated into the material that is taught, and contribute to an up to date curriculum. % up to date. 
The real use cases covered in \TheProject{} can inspire the design of new challenging practical coursework, exercises, essays and other forms of assessment. In addition, large-scale %full-year 
final year individual and/or group projects will be offered on topics that are relevant to \TheProject{},
such as, %These topics will include, for example, 
refactoring, formal reasoning and logic,  code-level security analysis, combining approaches for
vulnerability detection, and so on, 
%use of the YAG-Suite, etc., 
and will be available at both undergraduate or masters level. We will also seize opportunities to direct PhD and EngD student research towards areas that are of interest to \TheProject{}. The proposed work is a close fit to the research interests of several academic staff at University of St Andrews, and we have already started to explore how e.g. formal reasoning for refactoring of security can be further integrated into research on modeling semantics and soundness via dependant types and logic. We will seek opportunities to give open seminars and run courses for industry, e.g. on 
%energy and timing analysis, type systems for energy and time contracts. 
security verification, type systems for security contracts. Finally, we will assist \UODshort{} in the preparation of MOOCs for focused online courses on aspects of the \TheProject{} project and the organisation of summer schools. \SA has considerable experience at organising summer schools for master students as part of its (present and past) Erasmus Mundus Joint Masters programmes.

\horizontalline

\subsection*{Draft Exploitation Plan for \UODshort{}}

\begin{wrapfigure}{R}{3cm}
\vspace{-1.4cm}
\hfill \includeimage[scale=0.25]{logos/UOD-logo.png}
\vspace{-0.9cm}
\end{wrapfigure}

\UODshort{} will actively work on promoting and exploiting the tools and techniques produced by the \TheProject{} project. School of Science and Engineering at the University of Dundee has a long experience in Data Science and Big Data Analytics, being the first school in the UK that had an MSc programme in Data Science. Over the course of the previous projects, \UOD has established an active collaboration with big data industry, collaborating with Teradata  on Proteomic research along with open source solutions such as Cassandra and Neo4j. In addition there are strong links to the games industry in Scotland with companies such as Ninja Kiwi and Outplay and also Find my Past which hosts over 4 billion searchable records of census, directory and historical record information. These links will be used to promote the results of the \TheProject{}. \UOD also has a long-standing collaboration with the Health Informatics Centre (also a part of the University of Dundee and tightly linked with National Health Service Scotland), which is recognised in Scotland as a leader in health data linkage. This is the area where ensuring security of access and protecting privacy of the data is absolutely crucial, and opens another avenue for testing and exploitation of the \TheProject{} technologies, both on a repository of anonymised sensitive eHealth data that HIC owns and which covers approximately 20\% of the Scottish population, and, where feasible, on real data in co-operation with local company Waracle who specialise in assisting the NHS in data collection. \UODshort{} is also one of the main hubs in Scotland for Human-Computer Interaction and it operates a User Centre, which is used to test and evaluate research tools and technologies on various classes of users. This gives us a potentially large base of users to test e.g. authentication and continuous identification mechanisms developed over the course of the project. There are also numerous other projects currently running at the University of Dundee, especially in the area of application of machine learning to analyse medical images, where the output of \TheProject{} can be fed to further exploitation, providing additional assurance in security and privacy of the data. Finally, we will also look into creation of spin-off companies based on the technologies produced over the course of the project, seeing the \TheProject{} technologies as an essential layer of security for big data analytics in future.

\textbf{Teaching.} There is a huge potential of incorporating the results of the \TheProject{} project into the teaching curriculum at \UOD. There are many undergraduate and postgraduate modules that teach data analytics, and we also have dedicated MSc programs on Data Engineering, Data Science and Health Data Science for Applied Precision Medicine which can be expanded with the research output and technologies developed in \TheProject{}. Furthermore, we will look into developing new MSc courses on Security of Big Data Analytics and new interdisciplinary courses on applications of Big Data technologies into other areas, such as Medicine Life Sciences and Citizen Science and Observatories and we will investigate feeding the results of \TheProject{} into these new programs. \TheProject{} also gives opportunities for new research at the PhD level, investigating particular topics that the project deals with.
%
A Massive Open Online Course (MOOC)
is a recent (since 2008) innovation in education and learning. MOOC’s provide access, often to hundreds or thousands of people across the EU and the World, to focused online courses with learning goals and paths that present the opportunity to learn together at a flexible pace. UOD already has a strong collaboration with FutureLearn, which is the leading global MOOC 2.0 provider (an inclusive, open, sustainable and collaborative approach)  and can bring in vast experience from the Grow Observatory, WeObserve and the "Data Science in the Games Industry" MOOC for Scotland's DataLab.  The FutureLearn MOOC is free and open and brings a highly scalable platform into the \TheProject{} 
ecosystem, capable of sustaining more than 100,000 learners at one time. The FutureLearn platform will provide metrics and data as evidence of numbers of participants, progress through courses, data submission and completion and interpret data for \TheProject{} reports and deliverables.


\horizontalline

\subsection*{Draft Exploitation Plan for \UCMshort{}}

\begin{wrapfigure}{R}{5.5cm}
\vspace{-1cm}
\hfill \includeimage[scale=0.2]{logos/UC3M-logo.png}
\vspace{-0.9cm}
\end{wrapfigure}

\UCMshort{} will exploit the results from the project in two main areas:
the definition and tool support of coding standards and
the C++ language extension for contract based programming.
The new security centric coding guideline and the supporting toolset will
open new market opportunities that will be investigated.
Additionally, the contract based programming insights for the project
will enrich the feedback that \UCMshort{} will provide in the 
ISO C++ standards committee and its study group on contracts which
is currently targeting the ISO/IEC 14882:2023 standard.

In addition, the participation in the project will strengthen the ability of
\UCMshort{} to open new co operations in the area of cybersecurity for big data
analytics. On one hand, the support of coding guidelines may be of major
interest in the financial sector, where they have had several co operations with
BBVA (one of the major banks in Spain).  On the other hand, a better support for
contract based programming also allow to reinforce \UCMshort's current position
in the aerospace industry where it currently cooperates with major players in
the Spanish market, including GMV or SENR.  \UCMshort{} will be
able to improve its offerings with less effort devoted to security increasing
its competitiveness.


\horizontalline


\subsubsection{Knowledge Management and Protection}
\label{knowledgeprotection}
\vspace{-12pt}

\eucommentary{Outline the strategy for knowledge management and protection. Include measures to provide open access (free on-line access, such as the 'green' or 'gold' model) to peer-reviewed scientific publications which might result from the project}

Before the project starts, all project partners will agree on explicit rules concerning IP ownership, access rights to any
Background and Results for the execution of the project and the
protection of intellectual property rights (IPRs) and confidential
information as part of the Consortium Agreement.
As part of the Consortium Agreement, in order to ensure a smooth
execution of the project, the project partners will agree to grant each other
royalty-free Access Rights to their Background and Results for the
execution of the project. The Consortium Agreement will define further
details concerning the Access Rights after the duration of the project 
with respect to Background and Results.

\paragraph{Dissemination and Communication:}
While fully taking into account issues of potential exploitation and IPR ownership by project partners
as governed by the Consortium Agreement,
the project aims to provide good general access to its research results.
Balancing access with cost, the project will therefore generally adopt a ``green'' model to open access for publications,
but has included funding to support targeted ``gold'' open access for key publications.
The academic partners all maintain suitable institutional repositories, which will allow public access to research papers produced in the
course of the project, perhaps with some moratorium.  Some publishers (e.g. the ACM) also provide links that allow
free access to their publications from authors' home pages, and this will be exploited wherever possible.
% y, where publisher charges are not excessive, the project will consider ``gold'' open access to key project publications.
%
Furthermore, and perhaps of most significance, the project website will provide free,
open and publicly searchable access to all the public deliverables, to technical reports, data and results, to software tools
and libraries, to white papers and also to all
the other non-confidential documents that are generated in the course of the project.  

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Key Performance Indicators for Dissemination and Communication:}
\begin{itemize}
\item KPI 1: 5 publications per year per academic partner, at least 1 conference paper and 1 journal paper.
\item KPI 2: 30\% of accepted papers should be journal papers.
\item KPI 3: 2 project press releases.
\item KPI 4: At least 1 public project workshop per year.
\item KPI 5: At least 100 followers on social media channels by Year 2; 200 followers by Year 3.
\item KPI 6: At least 150 new visitors to the project website per project year.
\item KPI 7: Project brochure produced by Month 18.
\end{itemize}
\end{mdframed}


% The material released in this way will represent the vast bulk of the scientific
% and technical output of the project.

%\subsubsection*{Intellectual Property Rights}

%\pagebreak
\paragraph{IP Ownership.}

The project results will be owned by the project partner carrying out the work leading to such results. If any results are created jointly by at least two project partners and it is not possible to distinguish between the contributions of each of the project partners, such results, including inventions and all related patent applications and patents, will be jointly owned by the contributing project partners. In order to further the competitiveness of the EU market, and to enhance exploitation of the Consortium Results, each contributing party shall have full own freedom of action to exploit the joint IP as it wishes, and further the goals of the consortium. To promote this effort the contributing party will have full own consideration regarding their use of such joint results and will be able to exploit the joint IP without the need to account in any way to the other joint contributor(s). Further details concerning jointly owned results, joint inventions and joint patent applications will be addressed in the Consortium Agreement.

\paragraph{Transfer of Results.}

As Results are owned by the project partner carrying out the work leading
to such Results, each project partner shall have the right to transfer
Results to their affiliated companies/organisations without prior notification to the
other project partners, while always protecting and assuring the Access
Rights of the other project partners.  Such use of Results will encourage
competitiveness of the EU market by creating broader uses of the Results
and opening up the markets for the Consortium's Results in all markets.

\paragraph{Open Source and Standards.}

A central aim of this consortium is to provide benefit to the European community.  Some of the project partners may be either using Open Source code in their deliverables or contributing their deliverables to the Open
Source communities. Some of the partners may also be contributing to Standards (open standards or other). Details concerning open source code use and standard contributions will be
addressed in the Consortium Agreement.
The base technologies being developed by the academic partners within the duration of the project will be published under Open Source Licenses except where specified under Intellectual Property Management, allowing the broader community to benefit from the outcome of this project.
The major validated project results  will be contributed to the key international standardisation bodies  such as ISO, ITU and others where the consortium members take part in. 


\paragraph{Data Management Plan}
\vjcomment{This needs to be fixed.}

The primary research data that will be produced by the project will be the performance results that are reported in
various research publications.  This data will predominantly be scientific,
without confidentiality restrictions, and it will
therefore be made available through the project website, in line with the agreements that will be
made in the Consortium Agreement.  As far as possible, 
this data will be recorded in a human-readable form, such as plain ASCII text
or XML.  Where this is not possible, converters will be provided to make the data accessible to other researchers
in a human-readable form. 
The research data on the project website will be associated with the relevant research publications. 
We have budgeted for adequate disk and processing capacity to allow for the expected access to this data.
The project website will be maintained after the end of the project, but to ensure long-term continuity
and value, data will also be transferred to the \SAshort{} institutional data repository.

\draftpage
\subsubsection{Communication Activities}
\label{sect:comm-activities}

\eucommentary{Describe the proposed communication measures for promoting the project and its findings during the period of the grant. Measures should be proportionate to the scale of the project, with clear objectives. They should be tailored to the needs of various audiences, including groups beyond the project's own community. Where relevant, include measures for public/societal engagement on issues related to the project.}

\vjcomment{Fix this too...Add the Data Science, Security, AI and Healthcare related events}
As described in the draft dissemination plan above, and in the description of~\ref{wp:dissem} (Page~\pageref{wp:dissem}),
the \TheProject{} project aims to communicate itself and its findings intensively to various communities.
Research publications and presentations will aim to target various groups of academic and industrial researchers: parallel programmers, big data researchers, and programming language
designers, and will add scientific weight and credibility to our findings.  Press releases and news articles will be used to communicate project results and major
project life events (start, finish, key milestones) to both a technical and general audience.
We will also take advantage of opportunities as they arise for radio/TV interviews, public seminars and general articles
in both the technical and non-technical press.
The project website will be used to provide open access to project results, public deliverables,
software tools, technical reports, white papers, (video) tutorials, podcasts etc., and will serve
as a key resource for those wishing to use the project results, whether they are acting as an academic researcher, scientific, commercial or independent
software developer, public sector worker,  educator or private individual.
By making research results public in this way, we especially aim to engage with the software developer
community, who may not normally have access to academic papers and reports.
We will disseminate information about our tools and standards directly to customers via MOOCs, aiming to
increase engagement with an already motivated group of developers/users.
We will run open technical workshops that will showcase our work to interested parties.
These will generally be co-located with major networking events, such as the annual HiPEAC
conference and the HiPEAC spring/autumn gatherings.
We will also engage with relevant industrial/developer conferences, grass-roots meetings, workshops etc.,
producing poster and demonstrations as necessary to communicate with the broader developer community
and especially with project managers and decision makers.
Finally, we will actively participate in standardisation activities through e.g. the ISO C++ standard committee aiming to influence development and awareness of the \TheProject{} results.

%\clearpage

% ---------------------------------------------------------------------------
%  Section 3: Implementation
% ---------------------------------------------------------------------------


\clearpage
\section{Implementation}

\subsection{Work Plan --- Work Packages, Deliverables and Milestones}
\label{sect:workplan}


\begin{figure}[tp]
\begin{center}
\vspace{-5mm}
\begin{tabular}{ll}
\hspace{-0.75in}
\includeimage[scale=0.5]{PertChart.pdf}
\vspace{-10mm}
\end{tabular}
\caption{Overview of the \TheProject{} Workpackage Structure and Dependencies (PERT chart)}
\label{fig:wps}
\end{center}
\end{figure}

\subsubsection*{Overall Structure of the Work Plan}

The work plan is broken down into 6 technical work packages as shown
in \textbf{Figure~\ref{fig:wps}}: WP2 deals with fundamental techniques for vulnerabilities detection and self healing, WP3 deals with application of these techniques in multi-layer AI-based big data analysis applications, WP4 deals with formal specification and proving of the security and privacy properties, WP5 deals with intelligent identity management, WP6 deals with the \TheProject{} methodology for development of secure and privacy preserving applications and WP7 deals with use cases and evaluation of the project tools, technologies and methodology. In addition, WP1 deals with the management of the project and WP8 deals with dissemination, exploitation, community building and promotion of the project.

\input{deliverables}

\bigskip\bigskip
\addtocounter{subsubsection}{1}
\addcontentsline{toc}{subsubsection}{\protect\numberline{\thesubsubsection}Work
Package List}
\fbox{\begin{minipage}{\textwidth}\begin{center}{\Large\bf
        Work package list} % (full duration of project)}
  \end{center}
  \end{minipage}}

\bigskip\bigskip

\begin{tabular}{|p{1.2cm}|p{9cm}|p{0.8cm}|p{1.35cm}|p{1cm}|p{0.9cm}|p{0.9cm}|}
\hline
{\bf Work \mbox{package} No} & {\bf Work package title} &
{\bf Lead \mbox{partic.} no.} &
{\bf Lead short name} &
{\bf Person months} & {\bf Start month} & {\bf End month} \\\hline 

\newcounter{wp}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{USTAN}{28}{1}{36}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{IBM}{88}{1}{34}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{SCCH}{67}{1}{34}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{USTAN}{57.5}{1}{34}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{COGNI}{50}{1}{34}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{UCM}{73}{1}{36}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{SOPRA}{98}{1}{36}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{UOD}{50.5}{1}{36}

{\textbf{Total}} & & & &
\textbf{\large 512}&
&
\\\hline
\end{tabular}

\landscape

\subsubsection*{Work Plan Timing: GANTT Chart showing Task Dependencies and Information Flows for Technical Tasks}


%\vspace{-0.7in}
\centerline{\hbox to \columnwidth{\hss%
    \includeimage[scale=0.85]{Gantt.pdf}
\hss}}
\label{fig:gantt}
\vspace{-1in} % Fool LaTeX into avoiding unnecessary page break
\endlandscape

\newpage
%\bigskip\bigskip\bigskip

%% Set up the milestone numbers.
\input{milestones}

\fbox{\begin{minipage}{\textwidth}\begin{center}\Large\bf List of Milestones
  \end{center}
  \end{minipage}}
\label{sect:milestones}

\bigskip

\newcounter{ms}
\renewcommand{\thems}{MS\arabic{ms}}
\begin{minipage}{\textwidth}
\begin{center}
 \begin{tabular*}{\textwidth}{|p{1cm}|p{10.3cm}|p{1.2cm}|p{0.6cm}|p{2.7cm}|}  \hline
 \textbf{MS} & \textbf{Milestone name} & \textbf{Related WPs} & \textbf{Est. date} & \textbf{Means of
   verification} \\ % (success criteria below)} \\ % (deliverables shown here + success criteria below) \\
\hline
\ref{mil:req1} & Initial Requirements Analysis & WP7 & M3 & \ref{del:req1} \\
   \hline
 \ref{mil:dmp} & Initial Data Management Plan & WP8 & M6 & \ref{del:data-mgt-plan} \\
   \hline
\ref{mil:auth1} & Identity Management Mechanisms for Private Clouds & WP5 & M9 & \ref{del:auth1} \\
   \hline
   \ref{mil:vul1} & Initial Vulnerabilities Identification and Self-Healing Framework & WP2 & M10 & \ref{del:vul1} \\
   \hline
   \ref{mil:bigdata1} & Formally-Verifiable Security and Privacy of Big Data Analytics for Private Clouds & WP3, WP4 & M11 & \ref{del:bigdata1}, \ref{del:formal1} \\
   \hline
   \ref{mil:meteval1} & Initial \TheProject{} Methodology; Application and Evaluation on Initial Uses Cases for Private Clouds & WP6, WP7 & M12 & \ref{del:met1}, \ref{del:cs1}, \ref{del:eval1} \\
   \hline
   %\ref{mil:met1} & \TheProject{} Methodology for AI-Based Big Data Analytics on Private Clouds & WP6 & M12 & \ref{del:met1}, \ref{del:cs1} \\
   %\hline
   %\ref{mil:eval1} & Use Cases and Evaluation of \TheProject{} Technology for Private Clouds & WP7 & M13 & \ref{del:eval1} \\
   %\hline
   \ref{mil:req2} & Final Requirements Analysis & WP7 & M15 & \ref{del:req2} \\
   \hline
   \ref{mil:auth2} & Identity Management Mechanisms for Public Clouds & WP5 & M22 & \ref{del:auth2} \\
   \hline
   \ref{mil:mooc} & First run of Code Security MOOC & WP8 & M22 & \ref{del:mooc} \\
   \hline
   \ref{mil:vulbigdata2} & Refined Vulnerabilities Identification and Self-Healing Framework, Formally-Verifiable Security and Privacy of Big Data Analytics for Public Clouds & WP2 -- WP4 & M24 & \ref{del:vul2}, \ref{del:bigdata2}, \ref{del:formal2} \\
   \hline
   \ref{mil:meteval2} & Refined \TheProject{} Methodology; Application and Evaluation on Refined Use Cases for Public Clouds & WP6, WP7 & M25 & \ref{del:met2}, \ref{del:cs2}, \ref{del:eval2} \\
   \hline
%   \ref{mil:met2} & \TheProject{} Methodology for AI-Based Big Data Analytics for Public Clouds & WP6 & M25 & \ref{del:met2}, \ref{del:cs2} \\
 %  \hline
  % \ref{mil:eval2} & Use Cases and Evaluation of \TheProject{} Technology for Public Clouds & WP7 & M26 & \ref{del:eval2} \\
   %\hline
   \ref{mil:authvulbigdata3} & Final Versions of \TheProject{} Technologies for Public Clouds & WP2 -- WP5 & M34 & \ref{del:vul2}, \ref{del:bigdata3}, \ref{del:formal3}, \ref{del:auth3} \\
   \hline
   \ref{mil:meteval3} & \TheProject{} Methodology, Use Cases and Evaluation for Hybrid Clouds; \TheProject{} Roadmap; End of Project & WP6, WP7 & M36 & \ref{del:met3}, \ref{del:cs3}, \ref{del:eval3} \\
   \hline
%   \ref{mil:authvulbigdata3} & User Authentication Mechanisms for Hybrid Clouds & WP5 & M34 & \ref{del:auth3} \\
  % \hline
%   \ref{mil:vulbigdatameteval3} & \TheProject{} Technologies and Methodology for Hybrid Clouds; Use Cases and Evaluation on Public Clouds; End of the Project & WP2 -- WP4, WP6, WP7 & M36 & \ref{del:vul3}, \ref{del:bigdata3}, \ref{del:formal3}, \ref{del:met3}, \ref{del:cs3}, \ref{del:eval3} \\
  % \hline
   
\end{tabular*}
\end{center}
\end{minipage}

\setcounter{ms}{0}
\vspace{20pt}
\begin{center}
\begin{tabular*}{\textwidth}{|p{1.2cm}|p{13.3cm}|p{2.2cm}|}\hline
\textbf{MS No.} & \textbf{Success Criteria} & \textbf{Contributes to
  Objective(s)} \\
  \hline
\ref{mil:req1} & Initial Requirements and Dependencies for Tools and Use Cases Identified & \textbf{1 -- 8} \\
  \hline
 \ref{mil:dmp} & Gathering of likely data outputs from project & \textbf{ 8} \\
  \hline
\ref{mil:auth1} & Initial Identity Management Mechanisms for AI-Based Big Data Processing on Private Clouds Developed & \textbf{5} \\
\hline
\ref{mil:vul1} & Initial Static, Dynamic and Runtime Analysis and Self-Healing Methods Developed & \textbf{2, 3} \\
\hline
\ref{mil:bigdata1} & Vulnerabilities Detection and Self Healing Methods Applied and Verified for AI-Based Big Data Analytics on Private Clouds & \textbf{1 -- 3} \\
\hline
\ref{mil:meteval1} & Initial \TheProject{} Methodology Developed, Applied and Evaluated on the Initial Versions of Use Cases on Private Clouds & \textbf{4, 6, 7, 8} \\

%\ref{mil:met1} & \TheProject{} Methodology for Developing Secure AI-Based Data Analytics Code for Private Clouds Developed & \textbf{4, 6} \\
%\hline
%\ref{mil:eval1} & \TheProject{} Methodology Applied and Evaluated for Initial Versions of Use Cases for Private Clouds & \textbf{7, 8} \\
%\hline
\ref{mil:req2} & Final Requirements and Dependencies for Tools and Use Cases Identified & \textbf{1 -- 8} \\
  \hline
\ref{mil:auth2} & Refined Identity Management Mechanisms for AI-Based Big Data Processing on Public Clouds Developed & \textbf{5} \\
\hline
\ref{mil:mooc} & MOOC developed bu UOD and approved by Futurelearn & \textbf{5, 8} \\
\hline
\ref{mil:vulbigdata2} & Different Analysis and Self-Healing Methods Combined, Applied and Verified for AI-Based Big Data Analysis on Public Clouds & \textbf{1 -- 3} \\
\hline
\ref{mil:meteval2} & Refined \TheProject{} Methodology Developed, Applied and Evaluated on Refined Versions of the Use Cases for Public Clouds & \textbf{4, 6, 7, 8} \\
\hline
%\ref{mil:met2} & \TheProject{} Methodology for Developing Secure AI-Based Data Analytics Code for Public Clouds Developed & \textbf{4, 6} \\
%\hline
%\ref{mil:eval2} & \TheProject{} Methodology Applied and Evaluated for Refined Versions of Use Cases for Public Clouds & \textbf{7, 8} \\
%\hline
\ref{mil:authvulbigdata3} & Final \TheProject{} Technologies for AI-Based Big Data Processing on Hybrid Clouds Developed & \textbf{1 -- 5} \\
\hline
\ref{mil:meteval3} & Final \TheProject{} Methodology for AI-Based Big Data Analytics on Hybrid Clouds Developed, Applied and Evaluated on the Final Versions of Use Cases on Hybrid Clouds; The Project Roadmap Produced; & \textbf{4, 6, 7, 8} \\
\hline

%\ref{mil:auth3} & Final Authentication Mechanisms for AI-Based Big Data Processing on Hybrid Clouds Developed & \textbf{5} \\
%\hline
%\ref{mil:vulbigdatameteval3} & Final Technologies and \TheProject{} Methodology for AI-Based Big Data Analytics on Hybrid Clouds Developed, Applied the Final Versions of Use Cases and Evaluated; The Project Roadmap Produced; & \textbf{1 -- 8} \\
%\hline
  
\end{tabular*}
\end{center}

%\landscape
\newpage
\fbox{\begin{minipage}{\textwidth}\begin{center}\Large\bf List of Deliverables
  \end{center}
  \end{minipage}}
\label{sect:deliverables}

\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{|p{0.8cm}|p{9.65cm}|p{0.8cm}|p{1.15cm}|p{1.2cm}|p{0.8cm}|p{0.8cm}|}  \hline
\textbf{Del. no.}              & \textbf{Deliverable name}        & \textbf{WP no.} & \textbf{Lead}
& \textbf{Type}              & \textbf{Dis. level}   & \textbf{Del. date}
\\ \hline
% Year 1
\ref{mgt:mailinglists}           & Internal and public mailing lists
                                                                  & WP1 &\coordshort{} & OTHER & CO &  1 \\
\hline \ref{mgt:swrepository} & Internal software repository & WP1 & \coordshort{} & OTHER & CO & 1 \\
\hline \ref{del:req1} & Report on Initial Requirements for \TheProject{} Techniques & WP7 & \SOPRAshort{} & R & PU & 3 \\
\hline \ref{del:pressrelease1} & Press Release Announcing Start of \TheProject{} & \ref{wp:dissem} & \SAshort{} & DEC & PU & 3 \\
\hline \ref{del:website1} & Initial Project Website / Presentation & \ref{wp:dissem} & \UODshort{} & DEC & PU & 3 \\
\hline \ref{del:data-mgt-plan} & Data Management Plan & WP8 & \coordshort{} & R & CO & 6 \\
\hline \ref{del:auth1} & Report on Initial Identity Management Mechanisms for Data Analytics on Private Clouds & WP5 & \COGNIshort{} & R & PU & 9 \\
\hline \ref{del:vul1} & Software on Initial Techniques for Vulnerabilities Detection and Self Healing & WP2 & \YAGshort{} & OTHER & CO & 10 \\
\hline \ref{del:bigdata1} & Report on Vulnerabilities Detection and Self-Healing for AI-Based Big Data Analytics on Private Clouds & WP3 & \UODshort{} & R & PU & 11 \\
\hline \ref{del:formal1} & Report on Formal Mechanisms for Verifying Security Properties on Private Clouds & WP4 & \SCCHshort{} & R & PU & 11 \\
\hline \ref{mgt:periodic-rep-1} & Project Periodic Report (first year) & WP1 & \coordshort{} & R & CO & 12 \\
\hline \ref{del:dissemplan1} & First Interim Report on Dissemination and Exploitation & \ref{wp:dissem} & \UODshort{} & R & PU & 12 \\
% Year 2
\hline \ref{del:met1} & Report on \TheProject{} Methodology for Developing Secure Big Data Analytics on Private Clouds & WP6 & \SAshort{} & R & PU & 13 \\
\hline \ref{del:cs1} & Initial \TheProject{} Coding Standards & WP6 & \UCMshort{} & R & PU & 13 \\
\hline \ref{del:eval1} & Report on Initial Implementation and Evaluation of Use Cases & WP7 & \FRQshort{} & R & PU & 14 \\
\hline \ref{del:req2} & Report on Final Requirements for \TheProject{} Techniques & WP7 & \SOPRAshort{} & R & PU & 15 \\
\hline \ref{del:auth2} & Software on Refined Identity Management Mechanisms for Data Analytics on Public Clouds & WP5 & \COGNIshort{} & OTHER & PU & 22 \\
\hline \ref{del:mooc} & Report on first run of Code Security MOOC & WP8 & \UODshort{} & R & CO & 24 \\
\hline \ref{mgt:periodic-rep-2} & Project Periodic Report (second year) & WP1 & \coordshort{} & R & CO & 24 \\
\hline \ref{del:vul2} & Report on Refined Techniques for Vulnerabilities Detection and Self Healing & WP2 & \IBMshort{} & R & PU & 24 \\
\hline \ref{del:bigdata2} & Software on Vulnerabilities Detection and Self-Healing for AI-Based Big Data Analytics on Public Clouds & WP3 & \SCCHshort{} & OTHER & CO & 24 \\
\hline \ref{del:formal2} & Report on Formal Mechanisms for Verifying Security Properties on Public Clouds & WP4 & \SA{} & R & PU & 24 \\
\hline \ref{del:dissemplan2} & Second Interim Report on Dissemination and Exploitation & \ref{wp:dissem} & \UODshort{} & R & PU & 24 \\
% Year 3
\hline \ref{del:met2} & Report on \TheProject{} Methodology for Developing Secure Big Data Analytics on Public Clouds & WP6 & \SAshort{} & R & PU & 25 \\
\hline \ref{del:cs2} & Refined \TheProject{} Coding Standards & WP6 & \UCMshort{} & R & PU & 25 \\
\hline \ref{del:eval2} & Report on Refined Implementation and Evaluation of Use Cases & WP7 & \SOPRAshort{} & R & PU & 26 \\
\hline \ref{del:vul3} & Report on Final Techniques for Vulnerabilities Detection and Self Healing & WP2 & \IBMshort{} & R & PU & 34 \\
\hline \ref{del:bigdata3} & Report on Vulnerabilities Detection and Self-Healing for AI-Based Big Data Analytics on Hybrid Clouds & WP3 & \SCCHshort{} & R & PU & 34 \\
\hline \ref{del:formal3} & Software on Formal Mechanisms for Verifying Security Properties on Hybrid Clouds & WP4 & \SAshort{} & OTHER & PU & 34 \\
\hline \ref{del:auth3} & Report on Final Identity Management Mechanisms for Data Analytics on Hybrid Clouds & WP5 & \COGNIshort{} & R & PU & 34 \\
\hline \ref{mgt:periodic-rep-3} & Project Periodic Report (third year) & WP1 & \coordshort{} & R & CO & 36 \\
\hline \ref{del:met3} & Software on \TheProject{} Methodology for Developing Secure Big Data Analytics on Hybrid Clouds & WP6 & \YAGshort{} & OTHER & CO & 36 \\
\hline \ref{del:cs3} & Final \TheProject{} Coding Standards & WP6 & \UCMshort{} & R & PU & 36 \\
\hline \ref{del:eval3} & Report on Final Implementation and Evaluation of Use Cases and Project Roadmap & WP7 & \FRQshort{} & R & PU & 36 \\
\hline \ref{del:pressrelease2} & Final Press Release Describing the \TheProject{} Results & \ref{wp:dissem} & \SAshort{} & DEC & PU & 36 \\
\hline \ref{del:website2} & Final Project Website / Presentation & \ref{wp:dissem} & \UODshort{} & DEC & PU & 36 \\
\hline \ref{del:dissemplan3} & Final Report on Dissemination and Exploitation & \ref{wp:dissem} & \UODshort{} & R & PU &  36 \\

\hline
\end{tabular}
\end{center}
\end{minipage}


%\comment{JB}{Should we assume that there will be only one review in M18 rather than a yearly one?}
%\comment{CB}{Yes, I think so. In H2020 there are two periods. But the PO might want yearly reviews, negotiated on funding stage.}

\input{WPs/WPs}
%\input{WPs/WP3_SCCH}


% \TODO{Milestones need to be discussed and then described here.}

\bigskip\bigskip\bigskip
%\draftpage
\pagebreak
\fbox{\begin{minipage}{\textwidth}

\begin{center}\Large\bf
Critical Risks for Implementation
\label{sect:risks}
\end{center}
\end{minipage}}

\vspace{0.2cm}
Steps have already been taken to reduce the level of risk within the overall implementation plan.  The table below
identifies the main residual risks that are foreseen.  This register will be maintained
and updated as necessary during the project in order to minimise risk and so to maximise its successful completion.

%\bigskip

%\begin{tabular}{| p{3.2cm} | p{1.8cm} | p{1.5cm} | p{10.3cm}  |}  \hline
%\begin{longtable}{| p{3.2cm} | p{1.8cm} | p{1.5cm} | p{10.3cm}  |}  \hline
\begin{longtable}{| p{3.5cm} | p{1.5cm} | p{11.8cm}  |}  \hline
\textbf{Description of risk} & \textbf{WPs\newline involved} & \textbf{Proposed Risk-mitigation measures} \\ \hline
\multicolumn{3}{l}{\ }
\\\hline

Disagreement over task requirements/implementation.
\par
(\textbf{Likelihood: Low}
\par
\textbf{Severity: Medium})
& ALL &
A conflict resolution mechanism has been defined that can be used to resolve
problems as they arise (see Page~\pageref{conflict-resolution}).
Most of the partners have worked together
successfully in previous projects, there will be regular technical
and management meetings, and the Project Coordinator
is actively involved in all but one work package.  These measures will help ensure
good working relationships between project partners.

\\\hline
Non-performance of a partner.
\par
(\textbf{Likelihood: Low}
\par
\textbf{Severity: High}
)
& ALL & 
Where skills overlap, 
effort will be redeployed to other partners;
otherwise the tasks may be scaled back, if possible; or,
if necessary, new partners with required competencies will be
incorporated into the project.

\\\hline
COVID-19 pandemic negatively impacts the progress of the project.
\par
(\textbf{Likelihood: Low}
\par
\textbf{Severity: Medium}
)
& ALL & 
The ongoing COVID-19 pandemics might impose severe restrictions on physical meetings and dissemination activities (such as project workshops). The project partners are, however, well used to online collaboration, being members of various EU project consortiums that have switched to online meetings during the ongoing pandemic, and are thus able to continue to do so without impacting the progress of the project. Project workshops will, should this be necessary, also switch to virtual delivery.

\\\hline
Brexit and the free flow of personal data.
 \par
(\textbf{Likelihood: Low}
\par
\textbf{Severity: Medium}
)
& ALL & 

Should the UK leave the EU %European Union (“the EU”) 
without an adequacy agreement with the European Commission whereby the Commission do not recognise the Data Protection Act 2018 to provide the same feel of protection for the personal data of EU citizens as provided for by the General Data Protection Regulation, then the transfer of personal data from partner institutions in member states or the European Economic Area (EEA) may experience some level of disruption.
The UK Government have noted that they intend to recognise the GDPR, and to allow for the free transfer of personal data from the UK to the EU and the EEA. \SAshort{} will work with partners to implement EC Standard Contractual Clauses to allow for the free flow of personal data from the EU to the UK, for the purposes of this project, should the need arise.

\\\hline
Failure to achieve key project objectives.
\par
(\textbf{Likelihood: Low}
\par
\textbf{Severity: High}
)
& ALL & 
In order to make the major progress that is required,
\TheProject{} incorporates leading-edge
research and complex software development, whose absolute success cannot be predicted. 
Taking into account the excellent technical and research track records of each of
the partners, it appears entirely feasible to achieve not only our general
technical research objectives, but also all the specific objectives
of the work packages.  In the event that it
proves impossible to achieve some specific objective within the
scope of the project, we will firstly attempt to reallocate
resources to ensure that all objectives are obtained, then
to prioritise the most critical objectives and finally, if absolutely necessary, we will scale down our technical objectives, by
relaxing or deleting some part of those objectives as required
to achieve success. 

\\\hline
Inability to achieve interoperability between techniques for detecting security vulnerabilities
(\textbf{Likelihood: Medium}
\par
{\textbf{Severity: Low}})
& WP2 &
It might prove infeasible to provide automatic linking of all three methods for detecting security vulnerabilities (static analysis, dynamic analysis and runtime monitoring) in a tool-chain. In this case, we will not aim for full integration of the overall system and the \TheProject{} methodology will be extended with guidelines of how the interoperability can be achieved, i.e.~how do the outputs of one phase of detection need to be adapted to serve as inputs to the next one. 

\\\hline
Inability to adapt the generic vulnerability detection techniques to AI-based distributed big data analytics.
(\textbf{Likelihood: Low})
\par
{\textbf{Severity: High}}
& WP2, WP3 &
Adapting vulnerability detection techniques to AI-based distributed big data analytics requires us to identify, model and develop mechanisms to detect vulnerable code sequences such applications. It may happen that \TheProject{} technologies won't be able to automatically detect some vulnerabilities because of technological limitations, combinatory explosion or because of the complexity of some vulnerabilities. In that case we will revise the ambition to support semi automated vulnerability detection instead of fully automated. Additional training of the machine learning algorithms, code mining and supervised self healing, will then be deployed.

\\\hline
Self-Healing techniques cannot repair the identified vulnerabilities.
(\textbf{Likelihood: Medium}
\par
{\textbf{Severity: Medium}})
& WP2, WP3 &
While implementing both code refactoring and runtime adaptation as means for self-healing will ensure that we are able to repair a wide range of vulnerabilities, it is possible (and likely) that we will identify certain vulnerabilities that will require so substantial re-engineering of the code that it will be impossible to achieve it with either of these two methods. In such cases, we will aim to capture such code patterns and feed them into the extensions of security coding standards that will be developed, as well as to identify the code practices that lead to such threats and address them in more abstract way in the \TheProject{} methodology. In an unlikely event that this turn out to be the case for most of the security and privacy vulnerabilities, we will redeploy the effort from self-healing into developing coding standards and project methodology.

\\\hline
Security and privacy vulnerabilities arise from the internals of the libraries for storage and processing of big data.
\par
({\textbf{Likelihood: Low}}
\par
{\textbf{Severity: Medium}})
& WP3 &
\TheProject{} aims to identify vulnerabilities arising from the interaction between different layers (data storage, data processing etc.) of a typical big-data analytics application, as well as from the use of libraries at each layer in the data analysis code. It is, however, possible that majority of vulnerabilities will come from the \emph{internals} of these libraries, which we do not aim to extensively analyse (due to complexity of their code). This is, however,  not likely as we will use tried-and-tested libraries. Should this still turn out to be the case, we will redeploy the effort into analysing internals of the considered libraries instead of their composition. This would likely result in us being able to detect and repair fewer vulnerabilities, but it would still allow us to make significant improvement in security compared to the state-of-the-art.

%\\\hline
%Inability of the generic tools for vulnerability detection and self-healing to detect/repair vulnerabilities in big-data analytics applications
%\par
%({\textbf{Likelihood: Low}}
%\par
%{\textbf{Severity: High}})
%& WP2, WP3 &
%It is possible that generic techniques for discovering and repairing vulnerabilities in WP2 will not be able to detect majority of vulnerabilities 
%WP2 will develop generic techniques for discovering and repairing security and privacy vulnerabilities in distributed applications, focusing on analysing the source code of such applications and profiling their execution. WP3 will apply these techniques for the problem of big-data analytics. It is possible that the techniques from WP2 will not be able to detect majority of vulnerabilities when applied to the specific problem that WP3 considers, due to sheer scale of the computations and unpredictability of the interactions between storage and processing agents. This is unlikely to happen, as the techniques in WP2 will be designed with the scaling in mind and will be adapted to the non-determinism that parallel computations exhibit. In the case that it still happens, we will redeploy effort from other work packages (specifically WP2 and WP3) to further adaptation of the core techniques. 
%
\\\hline
Inability to ensure security due to lack of data
\par
({\textbf{Likelihood: Low }}
\par
{\textbf{Severity: Medium }})
& WP2 &
Security verification requires that the data that are necessary to detect or anticipate a safety-critical situation at run-time are available. This may not always be the case, in particular, as the project cannot consider all security preservation methods. If this should turn out to be the case, the mitigation measure is to investigate stronger security requirements that can be assessed on the grounds of available data, which may result in an overly cautious system, but guarantee security.
\\\hline
Unacceptable complexity
\par
({\textbf{Likelihood: Medium }}
\par
{\textbf{Severity: Medium }})
& WP2, WP4 &
While the verification may prove that a system satisfies the required security contracts, the measures required to assess, recognise and mitigate critical situations may turn out to be too complex. In this case, the mitigation measure is to investigate, if the requirements can be weakened to lower complexity.

%\\\hline
%Inability to formalise security requirements in a form viable for automated reasoning
%(\textbf{Likelihood: Low})
%\par
%{\textbf{Severity: Medium}}
%& WP4 &
%XX

\\\hline
Continuous user identification not accurate 
(\textbf{Likelihood: Low})
\par
{\textbf{Severity: Medium}}
& WP5 &
Based on our method to triangulate raw data metrics from multiple resources (video, eye gaze, physiological sensors), we will minimize this threat.

\\\hline
Identity and authentication prototypes not acceptable for user evaluation 
(\textbf{Likelihood: Low})
\par
{\textbf{Severity: Medium}}
& WP5 &
User-centered design approach, involving key stakeholders in the design should prevent this, as issues would be caught early. Will involve UX/IT/technical staff.

%\\\hline
%Something related to coding standards
%(\textbf{Likelihood: Low})
%\par
%{\textbf{Severity: Medium}}
%& WP4 &
%XX

\\\hline
Inability to design a coherent design patterns and the \TheProject{} methodology.% due to differences in systems and libraries. at different levels of abstraction.
(\textbf{Likelihood: Low})
\par
{\textbf{Severity: Medium}}
& WP6 &
There is a large number of systems, libraries and frameworks for different layers of data analytics, including many different distributed databases, processing frameworks and machine-learning libraries. There is no clear standard in any of these layers, which makes developing universal design patterns and methodology for combining them challenging. It is necessary, due to the length of the project and resources committed, to restrict ourselves to a relatively small subset of the systems/libraries. Should it prove difficult to develop patterns and methodology for the systems/libraries considered, we will further restrict ourselves to very particular systems (based on MS Azure that is used in most parts of the use cases), sacrificing the generality 

\\\hline
Coding guidelines cannot be
fully automated.
(\textbf{Likelihood: Low})
\par
{\textbf{Severity: Medium}}
& WP6 &
Several partners have experience in analysing and
refactoring source code, which gives high confidence that both the
identification and the refactoring will be successful.
We will independently implement the coding guidelines violation detection
and the corresponding refactorings and later make them interoperable.
In case that for some specific rule the  interoperation cannot be integrated, still both the identification
and the refactoring will be available.

\\\hline
Adapting the \TheProject{} technologies to the considered use cases presents unforeseen difficulties.
(\textbf{Likelihood: Low})
\par
{\textbf{Severity: Medium}}
& WP7 &

In the unlikely event that the use cases does not match the systems under development \SOPRAshort{} and \FRQshort{} will take actions to adapt the use cases to assist the research. The use cases presented is part of the core business and \SOPRAshort{} and \FRQshort{} have several installation with these types of applications. In event of total miss-match \SOPRAshort{} and \FRQshort{ }will offer alternative use cases for approval by the consortium to assist with the success of the project.

\\\hline
Legal security of Machine Learning algorithms and applications 
(\textbf{Likelihood: Medium})
\par
{\textbf{Severity: High}}
& WP2, WP7 &

Questions about liability, certification and data protection that are relevant for both the application and the manufacturer side will be addressed to tackle this risk. An evaluation of the approval and certification of algorithms is planed. This is intended on the one hand to check technical reliability, but also to guarantee compliance with certain ethical and moral guidelines ("Ethics by Design"). Supervisory bodies and control options are intended to promote the development of trustworthy Machine Learning, thereby ensuring consumer safety and increasing trust in artificial intelligence.

\\\hline

\end{longtable}
%\newpage

\subsection{Management Culture and Procedures (Figure~\ref{fig:management})}
\label{sect:mgt}

\eucommentary{Describe the organisational structure and the decision-making ( including a list of milestones (table 3.2a)).\\
Explain why the organisational structure and decision-making mechanisms are appropriate to the complexity and scale of the project.\\
Describe, where relevant, how effective innovation management will be addressed in the management structure and work plan.\\
Describe any critical risks, relating to project implementation, that the stated project's objectives may not be achieved. Detail any risk mitigation measures. Please provide a table with critical risks identified and mitigating actions (table 3.2b).}

Responsibility for the overall management and technical
direction of the project will rest with the \emph{Project Coordinator}
(Dr Juliana Bowles, \SA{}) who will be the primary point of
contact with the European Commission. Responsibility for
individual work packages will rest with the \emph{Work Package Team
Leader (WTL)} identified below, who will report to the Project Coordinator.
Where a work package is split across more than one
institution, the day-to-day management of each task will be handled
locally, with the task manager reporting to the WTL.   
% Disputes
% will be resolved at the lowest possible level by an independent
% adjudicator (for disputes between WTLs this will be the Project
% Coordinator, unless he is involved in the dispute).
%
In order to ensure good integration of the project and sound
overall management, the Project Coordinator will convene
annual technical workshops containing representatives from
the entire project team.   These workshops will be open to
invited external researchers/industrialists, including members
of the \emph{Project Advisory Board}, and will usually be
accompanied by a physical meeting of the \emph{Project Steering Committee}. In
addition, the Project Coordinator will convene management
meetings involving the relevant partners and members of the
Project Advisory Board, as necessary and appropriate. These meetings will be
conducted either using video-conferencing, through a
teleconference, or in person, as appropriate and with due consideration to
cost, urgency and effectiveness.  Technical teams
working on a work package that is spread across sites will
coordinate through email, video-conferencing, telephone and
scheduled meetings.  Finally, the research teams will maintain
regular contact with the Project Coordinator and each other
through regular email reports and telephone conversations.
Progress will be carefully monitored with progress reports and
monitoring documents open to inspection by the EU project
monitoring officer. In the event of a serious and urgent matter
involving all partners, the Project Coordinator may also
convene an Extraordinary meeting of the Steering Committee.
%
All project documentation (whether managerial, legal or
technical) will be maintained through a centralised electronic
repository, accessible to all consortium members on an open
basis, and incorporating audit trails concisely recording
reasons for changes etc.  We propose to use either GIT or SVN,
which provide suitable low-cost,  low-overhead solutions that all partners are
familiar with.  Our technical reports will form the basis for
the public deliverables that appear on the project web site.

\begin{figure}[ht!]
\vspace{-0.28in}
\begin{center}
\centerline{\hspace{0.6in} \hbox to \columnwidth{\hss\includeimage[height=4.7in]{Management}\hss}}
\end{center}
\vspace{-1.4in}
\caption{Management Structure}
\label{fig:management}
\end{figure}

%\begin{figure}[t!]
%\begin{wrapfigure}{l}{0.7\textwidth}
%\vspace{-0.75in}
%\hspace{-1in}
%\begin{center}
%\centerline{\hspace{1in} 
%\hbox to \columnwidth{\hss\includeimage[height=5.7in,trim={0 0 0 2.5cm},clip]{Management}\hss}
%}
%\end{center}
%\vspace{-1.8in}
%\caption{Management Structure}
%\label{fig:management}
%\end{figure}
%\end{wrapfigure}

%\TODO{Update this to reflect the consortium.}

\subsubsection*{Technical Steering Committee}
\vspace{-5pt}

The \emph{Technical Steering Committee} comprises the WTLs, plus the
Project Coordinator (who will act as chair).  Its purpose is to ensure
the effective running of the project on a day-to-day basis, and to
coordinate work across work packages.  In particular the Technical Steering
Committee will be
responsible for the implementation of the directives of the Project Steering
Committee, for the
guidance and monitoring of the technical WPs, for coordination among
WPs, for  the timely preparation, approval, and forwarding to the Commission
of the deliverables produced by the WPs, and for the resolution of conflicts
amongst WPs.  It will meet on a regular basis, usually through a monthly
teleconference.  Meetings may also be convened on request by any member.
% but also in person where necessary.  
Each member of the Technical Steering Committee has one vote,
which may be made by proxy, or in absentia, if necessary.  
Decisions are taken by consensus, if possible, otherwise by majority vote, 
with the
Project Coordinator retaining the casting vote.

\subsubsection*{Project Steering Committee}
\vspace{-5pt}

The \emph{Project Steering Committee} comprises one representative from each partner
(usually the PI), and is chaired by the Project Coordinator.  
The purpose of this committee is to decide
the general technical direction of the project.  It will also
take major decisions on project finances, addition of partners, removal of non-performing
partners, IPR issues, reallocation of workload etc.  It will meet in person
at least once per year, supplemented by more regular teleconference
meetings as required. Extraordinary meetings may also be convened on request by any partner.
Each representative has one vote, which
may be made by proxy if necessary.  Decisions are taken by
consensus, if possible, otherwise by majority vote, with the
Project Coordinator retaining the casting vote.

\subsubsection*{Project Advisory Board}
\vspace{-5pt}

The Project Advisory Board comprises a small group of invited
academics and industrialists who will provide input to the
project on general technical trends and directions, and advise
the steering committee where required.  The initial composition
of the Advisory Board will be determined at the outset of the
project, but we expect to include academic experts from cybersecurity, data science, formal methods, code analysis and artificial intelligence, as well as from the cloud computing, healthcare, banking and air traffic management domains. The Coordinator is authorised to 
execute with each member of the EEAB a non-disclosure agreement, which 
terms shall be not less stringent than those stipulated in this 
Consortium Agreement, no later than 30 calendar days after their nomination 
or before any confidential information will be exchanged, whichever date is earlier. 
We have invited senior representatives from industry and academia including: Jonathan Cameron (Deputy director, Digital Health and Care, Scottish Government), Horacio Gonzalez-Velez (Head of the Cloud Competency Centre at National College of Ireland), Kenneth White (Head of Security at MongoDB), Scott Hirleman (Community Outreach at Datastax), Eric Dupuis (Director of Security of Orange Cyber Defence France).
%Aarhus University, SAP Institute for Digital Government, Ericsson, TypeSafe, British Telecom,
%the oil\&gas industries, the Cloud Competency Centre (Dublin),
%	and Scottish Enterprise.

\subsubsection*{Work Package Team Leaders}
\vspace{-6pt}

Work package team leaders (WTLs) are responsible for tracking progress within their work package,
developing metrics for each deliverable at the outset of each
task, ensuring that results are properly reviewed against these
metrics, and consequently providing feedback to the Project Coordinator on the achievement of goals. 
%
WTLs have been chosen on the basis of managerial experience, technical expertise and
commitment to the work package programmes. % , as shown below.

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{|l|l|l|}\hline
\textbf{WP} & \textbf{WTL} \\ \hline
WP1 &  Juliana Bowles (\coordshort{}) \\ \hline
WP2 & Michael Vinov (\IBMshort{}) \\ \hline
WP3 & Vladimir Janjic (\UODshort{}) \\ \hline
WP4 & Juliana Bowles (\coordshort{}) \\ \hline
\end{tabular}
\quad\quad&\quad\quad
\begin{tabular}{|l|l|l|}\hline
\textbf{WP} & \textbf{WTL} \\ \hline
WP5 & Marios Belk (\COGNIshort{}) \\ \hline
WP6 & Jose Daniel Garcia Sanchez (\UCMshort{}) \\ \hline
WP7 & Andreas Vermeulen (\SOPRAshort{}) \\ \hline
WP8 & Andrew Cobley (\UODshort{}) \\ \hline
\end{tabular}
\end{tabular}
\end{center}



%\pagebreak
\subsubsection*{Principal Investigators}
\vspace{-6pt}

One principal investigator (PI) will be nominated by each partner.
The PI is responsible for properly managing the budget allocated to the partner and for performing
all the tasks that are carried out by that partner, reporting to the appropriate WTLs where necessary.  PIs
also act as line managers for the researchers/developers employed on the project by the partner.
PIs will usually also act as WTLs for the main WPs that are carried out at that site, and may be allocated their own
technical tasks. They will normally be the partner's representative on the steering committee.
They have been chosen for their technical expertise and experience of line management and budget handling.

\newcounter{partic}

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{|l|l|l|}\hline
& \textbf{Partner} & \textbf{PI} \\ \hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Juliana Bowles \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Vladimir Janjic \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Michael Vinov \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Andreas Vermeulen \\\hline
\end{tabular}
\quad\quad&\quad\quad
\begin{tabular}{|l|l|l|}\hline
& \textbf{Partner} & \textbf{PI} \\ \hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Michael Rossbory \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Marios Belk \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Jose Daniel Garcia Sanchez \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Eduard Gringinger \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Herv\'e Le Goff \\\hline

\end{tabular}
\end{tabular}
\end{center}

\vspace{12pt}
\subsubsection*{Project Coordinator}
\vspace{-6pt}

The Project Coordinator is \emph{Dr Juliana Bowles}.  Her role
is to act as the primary point of contact with the European
Commission, to receive feedback on research results from each
work package, to ensure the project maintains effective
progress towards the project objectives based on these results,
to produce any required  project management reports, to ensure
that deliverables are produced according to the planned
schedule and delivered to the Commission and project reviewers
as required, and to resolve disputes between project partners
as and when these arise.  She will convene regular management
and technical meetings, monitor progress on each work package,
collate deliverables, and maintain good contact with each site,
in addition to producing the annual management reports, and
ensuring that each site produces the required financial (audit)
certificates.  She will also be responsible for ensuring that
the Consortium Agreement (including IPR issues, voting rules and the conflict resolution procedures)
and any other legal documents are properly prepared and managed. This will be
done through \SAshort{} \emph{Research and Enterprise
Services}, who have significant expertise in preparing such
agreements.

\vspace{12pt}
\subsubsection*{Project Administrator}
\vspace{-6pt}

The Project Coordinator will be supported in her management
duties by a part-time \emph{Project Administrator} (to be appointed
from staff already employed by \SA{}) and located at \SA{}.  The Administrator
must possess both strong organisational skills and a
sufficient level of technical expertise in order to communicate
management requirements to the Partners, but will not be
involved in the management of day-to-day RTD activities.

\vspace{12pt}
\subsubsection*{Consortium agreement}
\vspace{-6pt}

% The relationship between all partners will be fixed in a Consortium Agreement based on the following principles:

% In order to have a management system applicable through all phases of the
% project, a reasonable approach is to have straight, clear and direct
% management and organization protocol at all levels. This is particularly
% relevant given the challenging financial and industrial policy
% constraints. Therefore, in order to have clearly assigned
% responsibilities, to avoid any friction and to progress as per the
% project plan, the responsibilities and authorities of the project manager
% and the team members will be unambiguous.

The partners will be bound by a formal consortium agreement that is
planned to be signed prior to the beginning of the project of the project,
and in which their roles, responsibilities and mutual obligations will be
defined both for the project life and, where relevant, beyond.  This will
formalise key issues including conflict resolution, IPR procedures, governance structure
etc.  %It will be based on the model consortium agreement issued by the European Commission.
The Digital Europe version of the DESCA, including the European Commission's
inputs will constitute the basis for such consortium agreement.

%\pagebreak
% \vspace{-6pt}
\subsubsection*{Conflict Resolution}
\label{conflict-resolution}
If conflicts arise during the execution of the project, they will be resolved according to the following principles:
% 
% \begin{itemize}
% \item
They will first be addressed within the relevant WP through discussion chaired by the WTL.
% \item
If this fails, the issue will be presented by the WTL either to the Technical Steering Committee
or to the Project Steering Committee, depending on the nature of the problem (technical or business/strategic).
% \item
The relevant board will attempt to resolve the issue through the usual voting procedure.
% \end{itemize}
%
Technical issues between WPs will also be addressed by the Technical Steering Committee.
%  As noted above, the TSC of the project consists of the WP leaders (chaired by the technical coordinator), and the GA consists of representatives of each partner (chaired by the management coordinator).
Any conflicts that cannot be resolved through the principles above will
be handled according to the dispute resolution provision set forth in the
Consortium Agreement.

% \subsubsection*{Management Costs}

% We have budgeted for the Project Administrator at 25\% effort
% over the lifetime of the project (i.e. 9 person-months) at
% \SAshort{}, plus small-scale management effort as required by each support to support the
% project through the preparation of reports for review meetings and other project-level
% management tasks.
% We have also budgeted for the cost of running annual Advisory Committee meetings, including travel
% support for unfunded Advisory Committee members, at \euros{} XX p.a. for a total cost of \euros{} XX.
% We have budgeted an additional
% \euros{}~4,800 for travel by the Project Coordinator (estimated
% as an additional two trips per annum).
% In order to minimise costs and time expenditure, as far as
% possible, all project management activities will be carried out using
% low-cost means such as email, Skype, telephone or
% video-conferencing, and any managerial travel will normally be
% combined with technical or research meetings.  Each site with expenditure
% in excess of \euros{}~325,000 also requires specific costs
% to cover the preparation of the required financial certificates, which will
% generally involve subcontracting an external financial auditor.

\draftpage
\subsection{Consortium as a Whole}
\eucommentary{\begin{itemize}
\item
Describe the consortium. How will it match the project's objectives? How do the members complement one another (and cover the value chain, where appropriate)? In what way does each of them contribute to the project? How will they be able to work effectively together?
\item
If applicable, describe the industrial/commercial involvement in the project to ensure exploitation of the results and explain why this is consistent with and will help to achieve the specific measures which are proposed for exploitation of the results of the project (see section 2.3).
\item
Other countries: If one or more of the participants requesting EU funding is based in a country that is not automatically eligible for such funding (entities from Member States of the EU, from Associated Countries and from one of the countries in the exhaustive list included in General Annex A of the work programme are automatically eligible for EU funding), explain why the participation of the entity in question is essential to carrying out the project
\end{itemize}
}

\begin{figure}[t]
\begin{center}
\includeimage[scale=0.5,angle=0]{ElysianConsortium.pdf}
\end{center}
\vspace{-0.3in}
\caption{Areas of Partner Expertise}
\label{fig:consortium}
\end{figure}

\textbf{Figure~\ref{fig:consortium}} shows the areas of expertise that
are relevant to this project and the consortium partners that
possess that expertise.  All partners span multiple areas,
providing technical depth within the consortium, and avoiding
knowledge gaps.  Within the areas, each partner possesses
complementary expertise, but with enough knowledge overlap to
ensure tight cohesion of the consortium. The consortium
comprises both academic and industrial expertise.
%  from XX
% highly-respected partners.
The consortium links the world-leading technical expertise of
the participating groups on security, big data analytics, formal modelling, AI, privacy preserving etc.
A combination of academic and industrial partners will ensure wide dissemination of the project results and ensure the best possible uptake of the project results.



\paragraph*{Integration of the Consortium.}
%\vspace{-6pt}

Several of the partners already have close working relationships through
recent and ongoing research projects (e.g. \textbf{Serums}).
The teams share common technical interests and several are active members in e.g. the
HiPEAC network of excellence (\SAshort{}).
Work Packages have been designed to foster close collaboration
between teams at different organisations.
%, with multiple groups involved in all of
%the technical and evaluation work packages. 
The tasks in 
each work package have been allocated on the basis of
technical expertise and ability. All tasks have been designed to involve multi-site
collaboration and/or the exchange of information, which is
intended to promote healthy interaction between the partners.
Finally, in order to ensure good integration between the partners,
we propose to run at least one technical workshop each year, and
have also requested funds to allow researchers from each group
to visit other groups on a regular basis.  We anticipate that
all of the \TheProject{} researchers will participate in these
technical workshops and exchanges.  We also intend to publish a significant
number of research papers and technical reports deriving from our joint research,
and to collaborate on joint tool production. 
We thus foresee a necessary and close level of integration between
the \TheProject{} partners.

\paragraph{Industrial/Commercial Involvement.}

\TheProject{} directly involves three large multinational companies (\IBMshort{}, \SOPRAshort{} and \FRQshort{}) and
two SMEs (\COGNIshort{} and \YAGshort{}).
These organisations have included draft exploitation plans that will  use the results of the project
directly as part of their ongoing business strategies and commercial development.
% follows an industrially-inspired agenda and addresses a key and topical challenge in
The \TheProject{} project further engages directly with industry through its
dissemination, user community and outreach activities 
% a dedicated  workpackage (WP7) 
% that are aimed at promoting
will promote the
\TheProject{} tools, technologies and above all \emph{mindset} and \emph{methodology} to a wider user base,
especially through industry-focused events. % that should attract C/C++ programmers and Simulink/SCADE users.
The objective is to ensure widespread uptake of the project results in a broad base of potential industrial
software developers targeting a variety of commercially important domains. % including telecommunications, 3D modelling and the automotive sector.
This will be assisted by including major industry participants on its Advisory Committee,
by actively engaging with the C, C++communities, and by engaging with the ISO C++ Standard Committee and ITU FG-DPM.
% and with the upcoming C++17 design.
% and by actively engaging with new coding standards for parallel and data-intensive applications, as well as C++.
% Finally, members of the consortium are also active members of the ISO C++ Standards Committee. This will
% ensure that the techniques, approach and methodology developed in the \TheProject{} will
% have a broad exposure through the C++ community, and through the evolution of the C++ language
% design itself.

% \khcomment{More needed here.}

\paragraph{Project Management Expertise.}

Members of the team have been heavily involved in running
various national and international research projects.  The
Project Coordinator, Dr Juliana Bowles, has obtained several
research grants and awards from national and international
bodies. She currently coordinates the Horizon 2020 research project SERUMS (\euros 4.37M, SU-TDS 826278, 2019-2021).
She was the principal investigator on various grants from the UK's Engineering and Physical Science Research Council (EPSRC), Royal Society, Royal Academy of Engineering, Scottish Funding Council, and is a Co-Investigator on one of the research initiatives in the Health Data Research (HDR)-UK Scotland grant with over \pounds 6M. She has worked and held invited  research positions in several European countries over many years. 
As described in the partner descriptions below,
%% Should add activities for INRIA, CodePlay etc.
most partners have been extensively involved in previous and
ongoing EU projects at both technical and managerial
levels, and have dedicated experienced senior staff contributing to the \TheProject{} project.
%XX
%
This experience will be called on as needed to resolve any
managerial problems that may arise. % during the course of the project.


%\bigskip
%\bigskip
\subsection{Resources to be Committed}

\eucommentary{Please provide the following:
\begin{itemize}
\item
a table showing number of person/months required (table 3.4a)
\item
a table showing 'other direct costs' (table 3.4b) for participants where those costs exceed 15\% of the personnel costs (according to the budget table in section 3 of the administrative proposal forms)
\end{itemize}}

\input{resources}

%\pagebreak
\subsubsection{Management Level Description of Resources and Budget}
\vspace{-6pt}

\TODO{This needs to be updated in line with the rest of the
project.}

The project will employ 512 Person-Months (PMs) of effort over three
years, comprising one or more full-time or part-time researchers at each site
plus one part-time project administrator at \SAshort{}, % and one part-time web designer at \INRIAshort,  Not in PMs?
20\% of the Project Coordinator and 10\% of the WTLs. The researchers will be supported by
the necessary dedicated computing equipment,
% the
% usual basic research equipment (workstations and/or laptop
% computers, network facilities, printers, dedicated file
% servers, etc.) funded from the project overheads, by
% special-purpose heterogeneous hardware necessary to carry out
% the research, 
by funding to enable the necessary travel to scientific and technical conferences, trade shows, 
project meetings and other project-related events, and by the funding that is needed to establish/enhance existing
industrial and academic contacts and to establish a user
community for the \TheProject{} tools and technologies.
%
The quoted budget includes all relevant national social and
other legitimate employment costs as permitted under the rules
governing EU Horizon 2020 ICT projects, including costs of
healthcare, social security and pensions provision, in line with national norms for each site.
%
Sufficient travel funding is also needed to support good
collaboration between the groups, including attendance at the
annual technical project meetings, plus individual visits between
sites. We have budgeted approximately \euros{}~8,000 per
site per year (varied in line with previous
costs at each site)  to cover, for example:
% \begin{itemize}
% \item 
attending two project workshops at \euros{}~600 each;
% \item
attending the annual Project Review Meeting at \euros{}~600;
% \item
one 1-week inter-site visit at \euros{}~750 each;
% \item
attending two conferences per year within the EU at \euros{}~1,000 each;
% \item
attending one conference per year outside the EU at \euros{}~1,500;
%\item
three conference fees per year at \euros{}~650 each.
% \end{itemize}
%
\noindent
In addition, \SAshort{} has budgeted \euros{}~1,000 per year to cover attendance at
IFIP Working Group meetings, visits to industrial concerns for dissemination purposes,
attendance at developer conferences, demonstrations etc. to promote the project,
and \euros~1,000 per year to support travel that is related to the management of the
project.
Wherever possible, travel for different purposes will be
combined into a single trip. We have also budgeted \euros~1,000
per year at \SAshort{} to support attendance by the Project Advisory Board members at
the annual Advisory Board Meetings, where this cannot be met from other sources.

\TODO{Add any specialist equipment.  We might add a serious
  multi-core/multi-GPU machine.}

%\pagebreak
\subsubsection{Additional Partner Costs}
\vspace{-6pt}

\paragraph{Testbed systems and development servers.}
\UODshort{} has budgeted \euros{13,500} for Azure cloud infrastructure for development and testing of the project technologies. This covers approximately 4 4-core machines and storage for the whole duration of the project. The infrastructure will be purchased on pay-as-you-go basis, allowing us to scale the volume of resources up and down, as needed in different phases. All of the project partners will have access to this infrastructure, allowing for simultaneous use of resources and collaboration. In addition to that, \UODshort{} has budgeted \euros{20,000} for the costs of developing and deploying MOOCs. This is in line with the final cost of the MOOCs that were developed by \UODshort{} in the previous EU projects and includes recording of all the material and organisation and promotion of the courses. \FRQshort{} has planned \euros{4,000} for cloud machines, user accounts for the air traffic management use case. \YAGshort has budgeted \euros{5,500} for a private SAAS server hosting for the use of the  YAG-Suite in the project. %A more powerful server is required because vulnerability detection can be calculation intensive.
%\SAshort{} has budgeted \euros{} XX for a central
%server to support the various software and document repositories that are needed by the project, to run the project website and to provide access
%to the shared research data that will be generated by the project.

\paragraph{Open Access Publication Fees.}
Each academic partner has budgeted approximately \euros{5,000} to support gold open access publication for key project publications (representing about 10-20\% of the expected project output). The budget will be pooled if not used by a specific partner, and used to support further gold open access publication by other partners or other dissemination activities, as necessary to maximise the overall success of the project. In addition \FRQshort{} has budgeted \euros{1,500} for one open access publication. There will be no charge for green open access publication, which will be used for the remainder of the project publications.

\paragraph{Audit Fees.} Where external auditing is necessary, each partner has budgeted approximately \euros{4,000} to cover the necessary costs of auditing the project.

%\vspace{-12pt}
\label{bibliography}
\addcontentsline{toc}{section}{References}

\bibliographystyle{unsrt}
\bibliography{bibliography}


%% Write macro to split Sections 1-3
\Split{1-3}

% ---------------------------------------------------------------------------
%  Section 4: Members of the Consortium
% ---------------------------------------------------------------------------

\newpage

\eucommentary{Page limits do not apply.}

\section{Members of the Consortium}

\eucommentary{Please provide, for each participant, the following (if available):\\
\begin{itemize}
\item
a description of the legal entity and its main tasks, with an explanation of how its profile matches the tasks in the proposal;
\item
a curriculum vitae or description of the profile of the persons, including their gender, who will be primarily responsible for carrying out the proposed research and/or innovation activities;
\item
a list of up to 5 relevant publications, and/or products, services (including widely-used datasets or software), or other achievements relevant to the call content;
\item
a list of up to 5 relevant previous projects or activities, connected to the subject of this proposal;
\item
a description of any significant infrastructure and/or any major items of technical equipment, relevant to the proposed work;
\item
[any other supporting documents specified in the work programme for this call.]
\end{itemize}}

\subsection{Participants}
\Participant{SA}{(\url{http://www.st-andrews.ac.uk})}

\begin{wrapfigure}{R}{2cm}
\vspace{-3.95cm}
\hfill \includeimage{logos/st-andrews-logo.jpg}
\vspace{-1cm}
\end{wrapfigure}

\label{sec:participantUSTAN}

%===============================================================================
The \SAlong{} is the third-oldest in the English-speaking world (founded 1413).
The School of Computer Science was likewise one of the earliest Computer Science departments in the world (founded 1972).
It has established an excellent reputation for its pioneering research in e.g.,
parallel computing, software engineering, programming language design,
software architectures, theoretical computer science and
distributed/mobile systems.  This research expertise has been
recognised through the award of numerous research grants and
awards from the UK and the European Commission. The School of Computer Science is a member of SICSA, the pan-Scottish pooling agreement for Informatics and Computer Science, which groups the 14 Scottish universities.

\vspace{10pt}
\subsubsection*{Role in Elysian}
\textbf{The \SAlong{} will coordinate the \TheProject{} project, lead work package \ref{wp:management} on project management and \ref{wp:securityContracts} on Formal Verification of Security and Privacy for Distributed Data Analytics. It will also contribute with its expertise on refactoring, formal methods, parallel programming, pattern-based design and development to WP2, WP3, WP5 and WP6. It will also contribute to dissemination in
\ref{wp:dissem}.}


\vspace{10pt}
\subsubsection*{Key Personnel Involved in the Project}
\paragraph{Dr Juliana Bowles (n\'ee K\"uster Filipe)} (female) 
%was awarded her PhD at the Technical University Braunschweig, Germany, in 2000. After working as a postdoctoral researcher at the Laboratory for Foundations of Computer Science (LFCS), University of Edinburgh, she then took a lectureship position at the University of Birmingham in 2004. In 2007, Dr Bowles moved to the University of St Andrews in Scotland, where she 
is  a Senior Lecturer in the School of Computer Science, leader of the Health Informatics group, and Director of Impact for Computer Science. 
%She leads the Health Informatics and Formal Methods Groups. 
Jointly with Dr Stephen Brown, Maynooth University, Ireland, and Professor Didier Galmiche, Universit\'e de Lorraine, Nancy, France, she proposed an Erasmus+ Joint Masters
programme on Advanced Systems Dependability (DEPEND) which was awarded funding from the EACEA in July 2017. Bowles is the programme director at St Andrews since it started in September 2018, and its predecessor DESEM since 2012.
With over 20 years experience in teaching and research in formal methods, over 75 book chapters, journal papers and other refereed publications, her research interests lie in the development of automated techniques for dependable computer modelling and formal verification with a more recent focus on healthcare applications. She is particularly dedicated to using computational methods to maximise safety and medication effectiveness for patients undergoing multiple treatments. She was the PI in St Andrews on a recently completed \pounds 1.2M EPSRC grant on \emph{Automated Conflict Resolution in Clinical Pathways}, is a Co-Investigator on one of the research initiatives \emph{Precision Therapeutics} in the Health Data Research (HDR)-UK Scotland grant with over \pounds 6M, has held several consecutive grants funded by the Royal Academy of Engineering and the Scottish Funding Council for work on \emph{Global Health}, and is currently the PI and coordinator of the \euros{} 4.37M EU H2020 project SERUMS. 
As Director of Impact for Computer Science at St Andrews, she has recently contributed to a SICSA response to the AI Strategy consultation for the Scottish Government. %documents to 
She is a member of the Silver Digital Applications and Systems Group that advises the Health and Care Division of the Scottish Government.

%\url{http://www.}

\paragraph{Dr Christopher Brown} (male) is a lecturer in the School of Computer Science and has a background in refactoring and program transformation. Dr Brown is the PI on the \euro{}5.4M EU H2020 \emph{TeamPlay} project, leading WP1 on contracts and formalisation for contracts of non-functional properties (including time, energy and security),
and is the PI on a newly funded EPSRC project, \emph{Energise}. He has undertaken work on automated refactoring for parallelism in Haskell, Erlang and C++, including developing ideas of program shaping and pattern discovery. He received his PhD from the University of Kent in 2008, with a thesis on Tool Support for Refactoring Haskell Programs. He worked on the \euro{}3.2M EU SCIence project, where he built domain-specific parallel skeletons for use in both Computer Algebra and Functional Programming. He was workpackage team leader on the \euro{}4.2M EU FP7 ParaPhrase project, which investigated the application of pattern-based refactoring in Erlang and C++. He was also workpackage team leader on the \euro{}3.6M EU Horizon 2020 RePhrase project, which was applying these refactoring techniques as part of a new software engineering approach to parallel programming using C++. He has previously led a technical team of six developers on the £537K Scottish Enterprise ParaFormance project, which was investigating commercial applications of refactoring for parallel programming, and which forms a key part of our longer-term exploitation plan. This approach is both effective and practical: using the ParaFormance tooling, we have successfully analysed million-line C++ programs, narrowing over 1,000 potential sites of potential parallelism to 29 effective sites. This task would take weeks or months of human effort, but our tool achieves it in less than 60 seconds. The ParaFormance tooling Dr Brown has developed will be exploited heavily in the \TheProject{} project. 

\paragraph{Dr Marco Caminati} (male) is a postdoctoral research fellow in the School of Computer Science. 
His research explores new ways of combining logical tools to model and solve computational problems.
To this end, he uses a variety of foundations and logical formalisms (e.g., first order logic, set theory, higher order logic) and of software tools (SAT and SMT solvers, proof assistants as Isabelle and Mizar).
Besides contributing new and extensive verifications of (correct by construct) software and of mathematical results, he has worked on integrating different logical formalisms in order to be able to use them together according to the specific aspect of the computational problem being faced.
Overall his research aims at improving theorem proving techniques. For example, his novel usage of the Mizar soft type system has inspired its developers to introducing new automation techniques in this particular proof assistant.

\paragraph{Dr Adam Barwell} (male) is a Rpostdoctoral research fellow in the School of Computer Science, with an interest in formal methods, static analysis, and program transformation. To date, he has published 11 papers. Dr Barwell received his PhD on ``Pattern Discovery for Parallelism in Functional Languages'' in 2018 from the University of St Andrews. This work resulted in two novel approaches for automatically detecting recursion schemes, i.e.\ formally-defined patterns of recursion, in source code via static analyses. Discovering such patterns can guide or enable the safe transformation of code; e.g., for the introduction of parallelism, or for software rejuvenation via the transformation of data from one representation to another.
%
As part of his PhD, he investigated how refactoring can facilitate the parallelisation of meta-heuristic algorithms, and later how a more functional approach to the definition of meta-heuristics can improve their design and the understanding of their behaviour. 
%
Following completion of his PhD, Dr Barwell worked on the EU H2020 RePhrase project, developing refactorings for the introduction of parallel stencil, divide and conquer, and map-reduce patterns to C++ programs via the generic pattern library, GrPPI. 
%This work was reported in the deliverable on ``Software for the Final Refactoring Tool''.
%
Dr Barwell is currently a PDRF on the EU H2020 TeamPlay project. 

\subsubsection*{Relevant publications}
\begin{itemize}
\item 
V. Janjic, J.K.F. Bowles, A. Vermeulen, A. Silvina, M. Belk, C. Fidas, A. Pitsillides, M. Kumar, M. Rossborry, M. Vinov, T. Given-Wilson, A. Legay, E. Blackledge, R. Arredouani, G. Stylianou \& W. Huang.
\emph{The SERUMS tool-chain: ensuring security and privacy of medical data in smart patient-centric healthcare systems}.  IEEE BigData. 9-12 December 2019, Los Angeles, USA. pp. 2726-2735, February 2020.
\item J.K.F. Bowles and M.B. Caminati.
\emph{Correct Composition in the Presence of Behavioural Conflicts and Dephasing}.  \emph{Science of Computer Programming}, 185:102323, January 2020. 
\item 
J.K.F. Bowles, M. B. Caminati, S. Cha and J. Mendoza.
\emph{A Framework for Automated Conflict Detection and Resolution in Medical Guidelines},  \emph{Science of Computer Programming}, 182:42-63, August 2019. 
%\item Modelling Concurrent Interactions. J. K\"uster-Filipe. Theoretical Computer Science 351(2), 203- 220, 2006. Elsevier B.V.
\item 
C. Brown, V. Janjic, A.D. Barwell, J.D. Garcia, K. MacKenzie.
\emph{Refactoring GrPPI: generic refactoring for generic parallelism in C++},
\emph{International Journal of Parallel Programming}, 1-23, July 2020.
\item 
C. Brown, V. Janjic, M. Gohli, J. McCall.
\emph{Programming Heterogeneous Parallel Machines Using Refactoring and Monte–Carlo Tree Search},
\emph{International Journal of Parallel Programming}. 48:583–602. July 2020. 
%\item
%V. Janjic, C. Brown, K. MacKenzie, K. Hammond, M. Danelutto, M. Aldinucci, J. Daniel Garcia
%\emph{RPL: A Domain-Specific Language for Designing and Implementing Parallel C++ Applications},
%\emph{24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing}
%Euro-Micro. 2016.
\end{itemize}

%pagebreak
\subsubsection*{Relevant Research Projects}

\begin{itemize}
\item Securing Medical Data in Smart Patient-Centric Healthcare Systems (SERUMS, SU-TDS 826278, 2019-2021), Coordinator and leader of work packages WP1 (Management), WP6 (Integration) and WP8 (dissemination). \url{https://www.serums-h2020.org}
\item Time, Energy and Security Analysis for Multi/Many-core heterogenerous PLAtforms (TeamPlay, ICT-779882, 2018-2020), \url{https://www.teamplay-h2020.eu}
\item Parallel Pattern Discovery and Program Shaping for Heterogeneous Manycores (Discovery, UK EPSRC EP/P020631, 2017-2020).
\item Automated Conflict Resolution in Clinical Pathways
(UK EPSRC EP/M014290/1, 2015-2018).
% \item
% Automatic Prediction of Resource Bounds for Embedded Systems (EmBounded, IST-2004-510255, 2005-2008, \url{http://www.embounded.org});
%\item Parallel Patterns for Adaptive Heterogeneous Multicore Systems (ParaPhrase, ICT-288570, 2011-2014, \url{paraphrase-ict.eu}).
\item Refactoring Parallel Heterogeneous Resource-Aware Applications - a Software Engineering Approach (RePhrase, ICT-644235, 2015-2018).


\end{itemize}

\Participant{UOD}{\url{http://www.dundee.ac.ul}}
\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/UOD-logo.png}
\vspace{-1cm}
\end{wrapfigure}

The University of Dundee is a leading Scottish institution and one of the world’s top universities. With 18,000 students from 145 countries, it is truly a global University. Research at Dundee delivers impact by harnessing expertise across disciplines to tackle some of the most challenging problems the world faces today. It promotes the sustainable use of global resources, shapes the future through innovative design and improve social, cultural and physical well-being. The University takes pride in delivering research with significant impact on global innovation and international policy. With 44\% of staff devoted solely to research, it continues to advance the lives of people across the world. Research in Dundee benefits from a wealth of experience from around the world, as international academics and external partners work closely together.
The University of Dundee was the first university in the UK that had an MSc program for Big Data Analytics. It continues to be at the forefront of Big Data Science in the UK, collaborating with national and international companies such as Teradata, Microsoft,Yahoo and local companies such as Outplay, Ninja Kiwi and Waracle along national bodies such as DataLab to deliver research and training into Data Science. \UODshort{} also has an established collaboration with various institutions in the area of Health Informatics, including the Health Informatics Centre (HIC) of the University of Dundee, which is recognised as a leader in health data linkage. It was the first centre in Scotland to offer a Safe Haven, which is now Nationally Accredited and ISO27001 certified. Operating under tight data governance controls, the Safe Haven allows secure collaborative research using sensitive eHealth data. HIC also maintains a clinical data repository of eHealth data covering approximately 20\% of the Scottish population, with core datasets providing a continuous record extending back 30 years.


%\ref{wp:management}wp:securityBigData}
\subsubsection*{Role in Elysian}
\textbf{\UODshort{} will lead \ref{wp:securityBigData} on Security and Privacy of Distributed Machine Learning
and \ref{wp:dissem} on Dissemination, Exploitation, Community Building and Communication activities. For \ref{wp:dissem} they will lead the delivery of an education programme through MOOCs. It will also contribute with its expertise on parallel programming, pattern-based application design and development, big data analytics and machine learning to \ref{wp:vulnerability} and \ref{wp:methodology}.}  

\vspace{10pt}
\subsubsection*{Key Personnel Involved in the Project}

\paragraph{Dr Vladimir Janjic} (male) is a lecturer in Data Science at the Division of Computing in the School of Science and Engineering at the University of Dundee. He received his PhD from the University of St Andrews in 2011. He has relevant expertise in distributed systems, parallel pattern-based programming, frameworks for big data analytics, machine learning, managing complexity and systems integration. His PhD work was undertaken in the context of the EU FP6 \textbf{SCIEnce} project, where he worked on mechanisms to ensure adaptivity on heterogeneous environments, which included building the SCALES simulator for work distribution policies on heterogeneous environments, and the implementation of dynamic scheduling mechanisms. Whilst at the University of St Andrews, he was also involved in several other EU projects, including H2020 RePhrase project, where he investigated software engineering techniques for developing pattern-based parallel applications, including application of machine learning techniques in managing system properties. He has published more than 25 research papers so far. He was one of the main authors and original contributors of the \textbf{SERUMS} EU project that investigates security and privacy of medical data in modern healthcare systems, and he is currently a principal investigator of the project for the University of Dundee.

\paragraph{Andrew Cobley} (male) is a senior lecturer at the school of Science and Engineering at the University of Dundee.    He is the program director for the MSc programs in Data Science and Data Engineering at the University.   Amongst his research interest is the NoSQL movement and applying these technologies to internet and games applications. He is currently working with the Horizon 2020 \textbf{WeObserve} project, which aims to improve our knowledge of how to build and operate citizen observatories. He has recently finished the highly successful Horizon 2020 \textbf{GROW} project investigating over seeing the technology work package on how to process and store large quantity of  soil moisture data  that is generated from Citizen Science.  He has supervised PHd students working with the Angus Lamond Lab in the University investigating Big Data approaches using Hadoop to analyze the large data that is generated during the study of Proteomics by the Lab.  Some of this work was carried out in co-operation with Teradata and Microsoft.


He is the creator of the FutureLearn MOOC “Data Science in the Games Industry” for the Scottish DataLab, this program has run on more than four occasions attracting attention from around the world. In addition, he is a writer for the on-line computing magazine “The Register” specializing in Data Science, Devops and database technologies at scale. 

\pagebreak

\subsubsection*{Relevant publications}
\begin{itemize}
\item V.~Janjic, J.~Bowles, A.~Vermeulen, A.~Silvina, M.~Belk, C.~Fidas, A.~Pitsillides, M.~Kumar, M.~Rossbory, M.~Vinov, T.~Given-Wilson, A.~Legay, E.~Blackledge, R.~Arredouani, G.~Stylianou, W.~Huang; \emph{The Serums Tool-Chain: Ensuring Security and Privacy of Medical Data in Smart Patient-Centric Healthcare Systems}; Proc. Big Data 2019, p. 2726-2735, IEEE.

\item C.~Brown, V.~Janjic, A.~Barwell, J.~D.~Garcia, K.~MacKenzie, \emph{Refactoring GrPPI: Generic Refactoring for Generic Parallelism in C++}, International Journal of Parallel Programming (IJPP), 2020.


\item T.~Yu, W.~Zhao, P.~Liu, V.~Janjic, X.~Yan, S.~Wang, H.~Fu, G.~Yang, J.~Thomson, \emph{Large-Scale Automatic K-Means Clustering for Heterogeneous Many-Core Supercomputer}, IEEE Transactions on Parallel and Distributed Systems; Vol.~31, No.~5, p.~997-1008; IEEE, 2020.

%\item 
%Masó , J., Cobley, A., Tsiakos, V., Tousert, N., Theodoropoulos, T., Jirka, S., Schade, S., Matheus, A., Tamascelli, S., Klan, F., \& Padiya, T. (2020). \emph{OGC Citizen Science Interoperability Experiment Engineering Report. Open Geospatial Consortium.} http://docs.opengeospatial.org/per/19-083.html
%\item
%Hemment, Drew  ; Woods, Mel  ; Ajates Gonzalez, Raquel  ; Cobley, Andrew ; Xaver, Angelika .  \emph{Enhancing collective intelligence through citizen science : The case of the GROW citizens’ observatory.} Collective Intelligence 2019 Proceedings. Association for Computing Machinery (ACM), 2019. pp. 1-4
%\item Cobley, Andrew  ; Woods, Mel  ; Hemment, Drew ; Giesen, Rianne.  \emph{The GROW Project, Interoperability between In Situ Sensors, Citizen Science and Crowd Sourced Data.} 39th EARSeL Symposium: Book of Abstracts. editor / Barbara Riedler ; Stefan Lang ; Stefanie Lettner ; Dirk Tiede. 2019. pp. 24-25
\item
A.~Cobley; \emph{NoSQL: Injection Vaccination for a New Generation}. The Register. 2015.

%\item Hemment, Drew  ; Ajates Gonzalez, Raquel  ; Woods, Melanie  ; Long, Deborah ; Giesen, Rianne  ; Cobley, Andrew. \emph{GROW Citizens’ Observatory Framework : A reference and guidelines for community platforms to generate, share and utilise information to address community, science, policy and innovation challenges }. European Commission, 2017. 55 p.
\item C.~Hillman, Y.~ Ahmad, M.~Whitehorn, A.~Cobley; \emph{Near Real-Time Processing of Proteomics Data Using Hadoop}; In: Big Data 2014 ; Vol. 2, No. 1. pp. 44-49; 2014.

%\item Boulon, Severine  ; Ahmad, Yasmeen  ; Trinkle-Mulcahy, Laura ; Verheggen, Celine  ; Cobley, Andy  ; Gregor, Peter ; Bertrand, Edouard  ; Whitehorn, Mark  ; Lamond, Angus I.  \emph{Establishment of a protein frequency library and its application in the reliable identification of specific protein interaction partners}. In: Molecular \& Cellular Proteomics. 2010 ; Vol. 9, No. 5. pp. 861-879.
\end{itemize}
\subsubsection*{Relevant Research Projects}

\begin{itemize}
\item Securing Medical Data in Smart Patient-Centric Healthcare Systems (SERUMS, SU-TDS 826278, 2019-2021),  \url{https://www.serums-h2020.org/}

\item Refactoring Parallel Heterogeneous Resource-Aware Applications - a Software Engineering Approach (RePhrase, ICT-644235, 2015-2018).

\item Parallel Pattern Discovery and Program Shaping for Heterogeneous Manycores (Discovery, UK EPSRC EP/P020631, 2017-2020).

\item GROW Observatory (GROW, SC5-17-2015, funded as 690199, 2016-2019), . \url{https://growobservatory.org/}


\item An Ecosystem of Citizen Observatories for Environmental Monitoring (WeObserve, SC5-19-2017, Grant no 776740, 2017-2020), . \url{https:www.weobserve.eu//}
\end{itemize}

\Participant{IBM}{(\url{http://www.research.ibm.com/labs/haifa/)}}
\begin{wrapfigure}{R}{4cm}
\vspace{-2.5cm}
\hfill \includeimage[width=4cm]{logos/ibm.jpg}
\vspace{-1cm}
\end{wrapfigure}

\vspace{0.5cm}

For more than sixty years, IBM Research, as the world's largest IT research organisation has been the innovation engine of the IBM corporation. Since the beginning of 2000, IBM has spent \$75 billion in R\&D, enabling IBM to deliver key innovations and maintain U.S. patent leadership for the 21st consecutive
year in 2013.
IBM also participates in and contributes to the work of standards consortia, alliances, and formal national and international standards organisations. 
The IBM Haifa Research Lab (HRL) is IBM's largest research laboratory outside of the United States, 
employing almost 500 researchers, the majority of whom hold doctorate and master degrees in computer science, electrical engineering, mathematics, and related fields. Since its founding in 1972, HRL has conducted world class research vital to IBM's success. R\&D projects are being executed today in areas such as Cognitive computing, Healthcare and Life Sciences, Verification Technologies, Telco, Machine Learning, Cloud Computing, Multimedia, Active Management, Information Retrieval, Programming Environments and Information and Cyber Security. The Quality and Security Department of the IBM Research Haifa includes researchers in the fields of cyber security and privacy. As a multi-disciplinary research area, researchers come from different research domains including verification, data analytic, operating systems and run-time systems, languages and compilers, network systems, and protocols and cloud technologies.
HRL has a long history of successful participation in EU projects (a partial list is included below). 
%A partial list includes the following: SMESEC (H2020), REPHRASE (H2020), SHARCS (H2020), PINCETTE (FP7, coordinator), RESERVOIR (FP7, coordinator), CloudWave (FP7, coordinator), SHADOWS (FP6, coordinator), CASPAR (FP6), HiPEAC (FP6 + FP7), SARC (FP6), ACOTES (FP7), MilePost (FP7), HYPERGENES (FP7), HERMES (FP7), SAPIR(FP6, coordinator), PROSYD (FP6, coordinator), Modelplex (FP6, coordinator).
%
The Quality and Security Department develops advanced tools and technologies spanning the entire spectrum of Functional Verification, Code Analysis and Cyber Security.

As a global leader in IT security, IBM offers the strategies, capabilities, and technologies necessary to help organizations in the private and public sectors preemptively protect the organisation from threats and address the complexities and growing costs of security risk management and compliance. IBM is helping to solve essential security challenges including:
\begin{itemize}
\item Better secure data and protect privacy
\item Control network access and help assure resilience
\item Defend mobile and social workplace
\item Manage third-party security compliance
\item Address new complexity of cloud and virtualization
\item Build a risk-aware culture
\end{itemize} 	

To facilitate a comprehensive offering, IBM is continuously investing in emerging technologies in the area of security intelligence and has a wide variety of security products and services. IBM is recognized in the industry as a leader in IT cyber security.
In recent years, IBM has acquired several cyber security start-ups in Israel increasing its R\&D presence in the region. IBM has recently also announced the establishment of a Cyber Center of Excellence (CCoE) in Beer-Sheva, Israel. The IBM Haifa Research Lab, as a well recognized IBM research facility and the largest one outside of the US, is collaborating with European research facilities to support the buildup of the CCoE as well as the acquired cyber security start-ups.

\vspace{10pt}
\subsubsection*{Role in Elysian}
\textbf{IBM will lead  \ref{wp:vulnerability}  on Vulnerability Detection and Self-Healing. It will also contribute to the development of privacy preserving ML models in WP3, formal verification of security in WP4, use cases evaluation and provides the Data Fabrication Platform to produce synthetic but realistic data for the project use cases in WP7. It will also contribute to dissemination in
\ref{wp:dissem}.}

\vspace{10pt}
\subsubsection*{Key Personnel Involved in the Project}

\paragraph{Michael Vinov} (male) is the manager of the Security and Data Quality group at the IBM Haifa Research Lab. He has been at IBM for more than 20 years where he has held a variety of development and management positions. He has a rich experience in computer architectures, hardware and software verification, performance optimization, cyber security and leading international projects inside IBM. He has a degree in Computer Engineering from Moscow Institute of Radio Technology and Electronics (MIREA), and M.S. degree in Electrical Engineering from The Technion -- Israel Institute of Technology.

\paragraph{Omer Boehm} (male) is a software engineer and researcher in the Security and Data Quality group at the IBM Haifa Research Lab. He has rich experience in compilers, binary file analysis, code optimization, sampling tools and rule-based test data generation. He has been at IBM for 15 years where he has participated in various projects that included the full life cycle of complex software systems. Among the main projects are:
\begin{itemize}
\item Data Fabrication Platform (DFP) – state-of-the-art rule-based test data generation platform.
\item Anti ROP – load time x86 binary instruction-level randomization tool to mitigate ROP attacks.
\item Feedback Directed Program Restructuring (FDPR) – IBM AIX/Linux tool used to optimize mission-critical enterprise software.
\end{itemize}
Omer has a degree in Computer Science from the Technion -- Israel Institute of Technology and an M.Sc. degree in Computer Science from The University of Haifa, Israel. He is currently completing his PhD study in the area of Machine Learning and Data Analysis at the University of Haifa, Israel.

\subsubsection*{Relevant Publications}

\begin{itemize}

\item
\emph{Create high-quality test data while minimizing the risks of using sensitive production data.} IBM InfoSphere Optim Test Data Fabrication, IBM, 2017, https://www.ibm.com/il-en/marketplace/infosphere-optim-test-data-fabrication.
%\item
%\emph{Test Data Fabrication.} Security and Data Fabrication, IBM Research, 2011, https://www.research.ibm.com/haifa/dept/vst/eqt\_tdf.shtml.
\item
Y. Richter, Y. Naveh, D. L. Gresh, and D. P. Connors (2007), \emph{Optimatch: Applying Constraint Programming to Workforce Management of Highly-skilled Employees}, International Journal of Services Operations and Informatics (IJSOI), Vol 3, No. 3/4, pp. 258 - 270.
%\item
%A. Goldsteen, S. Garion, S. Nadler, N. Razinkov, Y. Moatti and P. Ta-Shma, \emph{Brief Announcement: A Consent Management Solution for Enterprises}, International Symposium on Cyber Security Cryptography and Machine Learning (CSCML 2017).
\item
M. Abadi, S. Keidar\-Barner, D. Pidan, T. Veksler, \emph{Verifying Parallel Code After Refactoring Using Equivalence Checking}, Int. J. Parallel Program, volume 47, pages 59--73, (2019)
\item
H. Chockler, D. Pidan, S. Ruah, \emph{Improving Representative Computation in ExpliSAT}, 9th International Haifa Verification Conference, Haifa, Israel, November 5-7, 2013
\item
S. Barner, C. Eisner, Z. Glazberg, D. Kroening, I. Rabinovitz, \emph{ExpliSAT: Guiding SAT-Based Software Verification with Explicit States}, Second International Haifa Verification Conference, 2006, Haifa, Israel, October 23-26, 2006.
\end{itemize}

\subsubsection*{Relevant Research Projects}
\begin{itemize}
\item Securing Medical Data in Smart Patient-Centric Healthcare Systems (SERUMS, SU-TDS 826278, 2019-2021),  \url{https://www.serums-h2020.org/}
\item Validating Changes and Upgrades in Embedded Software (PINCETTE, ICT-257647).
\item
CloudWave: Agile Service Engineering for the Future Internet (CloudWave, ICT-610802).
\item
Refactoring Parallel Heterogeneous Resource-Aware Applications --- a Software Engineering Approach (RePhrase, 
ICT-644235, 2015-2018).
\item
Secure Hardware-Software Architectures for Robust Computing Systems (SHARCS, ICT-322014, 2015-2017).
%\url{http://sharcs-project.eu}

\end{itemize}


% ============================

\Participant{SOPRA}{(\url{https://www.soprasteria.co.uk/)}}


\begin{wrapfigure}{R}{6cm}
\vspace{-2cm}
\hfill \includeimage[width=6cm]{logos/Sopra-Steria-logo2.png}
\vspace{-1cm}
\end{wrapfigure}

\ 

%\vspace{24pt}

Sopra Steria, a European leader in digital transformation, provides one of the most comprehensive portfolios of end-to-end service offerings on the market: consulting, systems integration, software development, infrastructure management and business process services. Sopra Steria is trusted by leading private and public-sector organisations to deliver successful transformation programmes that address their most complex and critical business challenges. Combining high quality and performance services, added value and innovation, Sopra Steria enables its clients to make the best use of digital technology. With nearly 42,000 employees in more than 20 countries, Sopra Steria generated revenue of \euros 3.8 billion in 2017.

 %Sopra-Steria, since 1968, supports the primary business areas of consulting services, systems integration, integration of ERP, implementation of applications, as well as providing technical support to users and application maintenance and outsourcing services and operation of professional processes.

%Has experience in:

%\begin{itemize}
 %   \item Cyber Security via: (\url{https://www.soprasteria.com/services/cybersecurity})
 %   \item Artificial Intelligence via:\\ (\url{https://www.soprasteria.com/services/technology-services/artificial-intelligence})
 %   \item Internet of Things (IoT) via:\\ (\url{https://www.soprasteria.com/services/technology-services/internet-of-things})
%\end{itemize}

%Supports following industries:
%\begin{itemize}
 %   \item Aerospace
  %  \item Defense and Security
  %  \item Energy Utilities
  %  \item Financial Services
  %  \item Insurance and Social
  %  \item Government
  %  \item Retail
  %  \item Telecommunication, Media and Entertainment
  %  \item Transport
%\end{itemize}

\vspace{10pt}
\subsubsection*{Role in Elysian}
%\vspace{10pt}
\textbf{Sopra Steria will lead work package \ref{wp:usecases} on Requirements, Use Cases and Evaluation and will contribute to work packages WP2-6.}

%\vspace{10pt}

\vspace{10pt}
\subsubsection*{Key Personnel Involved in the Project}
\paragraph{Andr\'eas Fran\c{c}ois Vermeulen} (male) %Head of Analytics, Digital Consultancy Services within the United Kingdom.
%Andr\'eas 
has been in the information technology industry for forty plus years. He is the head of analytics and digital consulting delivery for over hundred consultants in the UK. Andr\'eas received his bachelor degree at the North West University at Potchefstroom, South Africa, and his Master of Business Administration at the University of Manchester, Master of Business Intelligence and Data Science at the University of Dundee, and is busy undertaking a Doctor of Philosophy degree at the University of St Andrews. Andr\'eas is a General Data Protection Regulation (GDPR) certified professional. He has designed and implemented data lakes, data warehouses and information systems for several industrial business solution on an international level using agile development methodology. 
%Author of three international books on big data, data science and machine learning, including:


\paragraph{Ian Kayne} (male) is the head of Cybersecurity Consulting within Sopra Steria, and has
over twenty years of experience in cybersecurity and cloud ecosystems, leading global initiatives for FTSE100 and Fortune500 organizations as well as the government and public sector. Ian is an expert at penetration testing and reverse engineering services. Ian has published books and lectures on topics such as secure coding and reverse engineering at MSc level in cybersecurity.


\paragraph{Rakhee Porter}  (female) is the Cyber resilience Lead within Sopra Steria, and has
more than 10 years of experience delivering risk-based resilience programs to secure ecosystems. She is an expert in business resilience services including Technology Resilience, Operational Resilience, Crisis Management and Business Continuity.

\subsubsection*{Relevant Publications}

\begin{itemize}
\item V.~Janjic, J.~Bowles, A.~Vermeulen, A.~Silvina, M.~Belk, C.~Fidas, A.~Pitsillides, M.~Kumar, M.~Rossbory, M.~Vinov, T.~Given-Wilson, A.~Legay, E.~Blackledge, R.~Arredouani, G.~Stylianou, W.~Huang. \emph{The Serums Tool-Chain: Ensuring Security and Privacy of Medical Data in Smart Patient-Centric Healthcare Systems}. Proc. Big Data 2019, p. 2726-2735, IEEE.
\item A. Vermeulen. \emph{Industrial Machine Learning: Using Artificial Intelligence as a Transformational Disruptor}, APress, 2019.
\item A. Vermeulen. \emph{Practical Data Science: A Guide to Building the Technology Stack for Turning Data Lakes into Business Assets}, APress, 2018.
\item S. Shaw, A. Vermeulen, A. Gupta and D. Kjerrumgaard.  \emph{Hive: A Guide to Hadoop’s Data Warehouse System}, APress, 2016.

\end{itemize}

\subsubsection*{Relevant Research Projects}
\begin{itemize}
%\item SERUMS (\url{https://www.serums-h2020.org/})
\item Securing Medical Data in Smart Patient-Centric Healthcare Systems (SERUMS, SU-TDS 826278, 2019-2021),  \url{https://www.serums-h2020.org/}
\end{itemize}


\Participant{SCCH}{(http://www.scch.at)}

\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/SCCH.jpg}
\vspace{-1cm}
\end{wrapfigure}

\SCCH{} (\url{www.scch.at}) is an Austrian research and technology organization founded in 1999 by mathematics and informatics institutes of the Johannes Kepler University, Linz, and is partially funded by the national programme COMET (Competence Centers for Excellent Technologies). Its primary focus is on research in the fields of data and software science as well as control, optimization, and machine learning. During recent years, SCCH has focused on the research area: ``Privacy Secured Transferable AI'' under the framework of international and national projects including EU H2020 project: ``Securing Medical Data in Smart Patient-Centric Healthcare Systems (SERUMS)'' and national projects: ``Privacy Preserving Machine Learning for Industrial Applications (PRIMAL)'', ``Security and Safety for Shared Artificial Intelligence (S3AI)'', and ``Privacy Secured Explainable and Transferable AI for Healthcare Systems (PETAI)''. Also relevant for this project, SCCH has important experience with formal modelling and verification as per the deep fundamentals developed in the project ``Behavioural Theory and Logics for Distributed Adaptive Systems (BlogDAS)'' as well as extensive industrial experience in software analysis and knowledge-extraction from source code. 

SCCH strives toward leadership in the fields of digitalization and artificial intelligence (AI) and especially supports its (industrial) partners on their transition from contemporary (software) systems to AI-based cyber-physical systems (CPS). In projects with partner companies state-of-the-art research results are applied to practical industrial projects to increase and maintain their competitiveness. One of the focus areas is Data Analysis Systems (DAS), specialising on the advancement and application of methods for the analysis and modelling of complex and massive sensor data in its (industrial) application context. Particular application domains include
\begin{inparaenum}[a)]
\item modelling, prognosis, forecast and control of systems
\item industrial fault detection, diagnosis and prognosis
\item the discovery of knowledge and structure in industrial processes.
\end{inparaenum}
SCCH has a long tradition of company partnerships in the manufacturing industry, like KEBA AG, Siemens AG, STIWA Holding, RUBBLE MASTER HMH, Fronius International GmbH, ENGEL AUSTRIA GmbH, TRUMPF Maschinen Austria GmbH and voestalpine Stahl GmbH, also in the energy domain and increasingly also in the healthcare sector, where, e.g., a COMET project (ARProSys) together with B. Braun from Hungary was carried out for research on the implementation of adaptive and robust software support for industrial processes in the area of electronic medical devices. 

The participating investigators are researchers and project managers with expertise in the relevant areas and are active in the research community by publishing and reviewing for top-rated journals and conferences.

\vspace{10pt}
\subsubsection*{Role in Elysian}
\textbf{\SCCH{} 
%will lead WP3 on Security and Privacy of Distributed Machine Learning, with the special focus on privacy-preserving machine learning and knowledge sharing in distributed AI. SCCH 
will contribute mostly to WP3 in the development of advanced machine learning and information theory 
%based tools 
for evaluating and reducing privacy leakage in distributed AI, and to %
WP4 on the specification of security contracts, tool support for the extraction of the required formal models from source code, novel techniques for the verification of security properties on distributed systems and theoretical foundations for attack models and security repairability of proof obligations. \SCCH{} will further contribute to WP2 by working in the identification of vulnerabilities and self-healing, 
%that arise from unexpected iteration of data processing %agents during computation 
%s well as in the development of related adaptation mechanisms for self-healing and 
WP5 on privacy-preserving biometrics and authentication, and to dissemination in WP8.  }


%\vspace{10pt}

\vspace{10pt}
\subsubsection*{Key Personnel Involved in the Project}

\paragraph{DI (FH) Michael Roßbory} (male) is senior research project manager in the Data Analysis Systems group at \SCCHshort{}. He studied Software Engineering at the University of Applied Sciences in Hagenberg with focus on programming languages, compiler, professional software development and project management. In 2005 he received his degree for his diploma thesis about the development of access control software system. During his study he worked for the research and development department of Daimler AG in Ulm in the area of automotive applications in 2004. He started a second Master study for Informatics at JKU Linz. During his work at \SCCHshort{} since 2005 he gained expertise in high performance and parallel computing during his work in several EU projects, where he also worked as project manager. In various other projects he was engaged as software developer in embedded, distributed and automotive environments and as data analyst (e.g. development of a deep learning framework). He also recently worked as a reviewer for journals in the area of parallelization (e.g. IJPP).

\paragraph{Apl. Prof. Dr.-Ing. habil. Mohit Kumar} (male) is a key researcher in the Data Analysis Systems group at \SCCHshort{}. He served as Research Group Leader from 2008 to 2016 at the Center for Life Science Automation and Institute of Automation of University of Rostock. Since 2016, he is the professor for Computational Intelligence in Automation at the University of Rostock. In 2016, he was awarded the Thousands Talent Foreign Expert award of China. He served as Adjunct Research and Development Technical Director at Binhai Industrial Technology Research Institute of Zhejiang University, China from 2017 to 2018. He is also a Guest Lecturer at the Universität Göttingen, Germany. His research area is Artificial Intelligence in Automation. He has published more than 45 journal articles in top-rated journals in  his area of research. 
%He has an h-index of 17 and around 1200 citations. More information about his research is available at www.fuzzycomputing.com.

\paragraph{Dr Flavio Ferrarotti} (male) is a key researcher in the Rigorous Methods in Software Engineering team. Since he joined \SCCHshort{} in 2014, he has primarily worked in the fundamental research projects BLogDAS and HOLS. BLogDAS is particularly relevant for the work on formal specification and validation planned for the current project. In BLogDAS, Dr. Ferrarotti has co-developed a sophisticated behavioural theory and logics for distributed adaptive systems. HOLS is a bilateral cooperation project between Austria and Flanders (Belgium). In HOLS, Dr Ferrarotti took the role of main applicant and PI, establishing himself as independent researcher on the expressive power of higher-order logics as foundation for modern data models and data manipulation languages. Dr. Ferrarotti holds a Ph.D. in Information Systems from Massey University, New Zealand. He has worked in database theory, finite model theory and rigorous methods. Before joining \SCCHshort{}, he worked as postdoctoral researcher at Yahoo! Research Latin America (two years) and as lecturer and researcher at Victoria University of Wellington in New Zealand (four years). He has a strong record of accomplishment in research with over 50 peer-reviewed publications in well-regarded international journals and conferences. He has received three best-paper awards, given invited talks, acted as reviewed of international research proposals, Masters and Ph.D. theses, chaired conference program committees, edited conference proceedings and special issues of international journals.



\subsubsection*{Relevant Publications}

\begin{itemize}
\item M. Kumar, M. Rossbory, B. A. Moser, B. Freudenthaler, ``An Optimal $(\epsilon,\delta)-$Differentially Private Learning of Distributed Deep Fuzzy Models,'' \emph{Information Sciences}, \url{https://doi.org/10.1016/j.ins.2020.07.044}{}. 
\item M. Kumar and B. Freudenthaler, ``Fuzzy Membership Functional Analysis for Nonparametric Deep Models of Image Features,'' \emph{IEEE Transactions on Fuzzy Systems}, \url{https://doi.org/10.1109/TFUZZ.2019.2950636}{}.
\item F. Ferrarotti, M. Moser and J. Pichler: Stepwise abstraction of high-level system specifications from source code. Journal of Computer Languages. In press, journal pre-proof \url{https://doi.org/10.1016/j.cola.2020.100996} (2020).
\item F. Ferrarotti, S. González and K.-D. Schewe: BSP abstract state machines capture bulk synchronous parallel computations. Sci. Comput. Program. 184 (2019)
\item F. Ferrarotti, Klaus-Dieter Schewe, Loredana Tec and Qing Wang: A unifying logic for non-deterministic, parallel and concurrent abstract state machines. Ann. Math. Artif. Intell. 83(3-4): 321-349 (2018)

\end{itemize}

\subsubsection*{Relevant Research Projects}

\begin{itemize}
\item Securing Medical Data in Smart Patient-Centric Healthcare Systems (SERUMS, SU-TDS 826278, 2019-2021),  \url{https://www.serums-h2020.org/}

%\item SERUMS (H2020, SU-TDS-02-2018), contribution on privacy-preserving distributed data analytics for securing medical data in smart patient-centric healthcare 

%\item PRIMAL (FFG IKT der Zukunft 2018) on privacy-preserving machine learning on industrial applications

\item S3AI: Austrian COMET Module on fundamental research on deep collaborative learning, since Jan 2020, Coordinator. 

\item RePhrase (H2020, ICT-644235) - Refactoring Parallel Heterogeneous Resource-Aware Applications. SCCH's role: reinforcement learning based dynamic scheduling and industrial use cases.

%\item ParaPhrase (IST-2011-288570) - Parallel Patterns for Adaptive Heterogeneous Multicore Systems. SCCH's role: use cases in the area of machine learning

%\item ADVANCE (IST-2010-248828) - Asynchronous and Dynamic Virtualisation through performance ANalysis to support Concurrency Engineering, \url{http://www.project-advance.eu});

\item ALOHA (H2020 Grant Agreement 780788) - Software framework for run-time-Adaptive and secure deep Learning On Heterogeneous Architectures. SCCH's role: develop deep transfer learning based methods for surveillance applications

\item TRESSPASS: robusT Risk basEd Screening and alert System for PASSengers and luggage (H2020 SEC-2016-2017-2, 787120, 2018-2021). SCCH's role: develop deep learning based methods for security applications

%\item GROW (H2020 Grant Agreement No.690199) Citizen Observatory that has ground-truthed Sentinel-1 to improve the accuracy of predictions on extreme events, such as flood, drought and wildfire. \url{https://growobservatory.org};

%\item WeObserve (H2020 Grant Agreement No.776740) An Ecosystem of Citizen Observatories for Environmental Monitoring. Umbrella for H2020 projects GROW Observatory, Landsense, Groundtruth 2.0 and Scent \url{https://growobservatory.org};

%\item BLogDAS (FWF: [P26452-N15]) Behavioural Theory and Logics for Distributed Adaptive Systems. Supported by the Austrian Science Fund.

\end{itemize}




% ============================
\Participant{COGNI}{(\url{https://cognitiveux.com/})}

\begin{wrapfigure}{R}{6,2cm}
\vspace{-3cm}
\hfill \includeimage[width=5cm]{logos/COGNI-logo.png}
\vspace{-1cm}
\end{wrapfigure}
\vspace{10pt}

Cognitive UX LTD is an IT company aiming to develop AI-driven and human-centered cybersecurity and privacy systems. The Cognitive UX key team members have complementary expertise spanning the spectrum of business, academia, technology and research. 

The company’s mission, as proven by the strong research and development track of the company, is to apply state-of-the-art and innovative developments in the domain of Cybersecurity for building and deploying products and services. The core team members have strong experience in implementing EU and National research projects (H2020, FP7, AAL, RIF), including awards of excellence (H2020 Seal of Excellence).

\subsubsection*{Role in Elysian}
\textbf{\COGNIshort{} will lead WP5 on Intelligent Identity Management and Trust, providing its expertise in usable security, with focus on identity management and user authentication. It will also contribute with its expertise on intelligent user interfaces, application design and development, data analytics and machine learning to WP2 and WP6, as well as with its expertise on user-centered design methods and human-computer interaction to WP7. It will also contribute to dissemination and WP8.}

\subsubsection*{Key Personnel Involved in the Project}

\paragraph{Marios Belk} (male, PhD, MSc, BSc) is CEO and co-founder of Cognitive UX LTD. He completed a Ph.D. degree in Computer Science, with focus on usable security and human-centered computing (2015) at the Department of Computer Science, University of Cyprus. His Ph.D. work was ranked in the finalists of the ERCIM 2016 Cor Baayen Young Researcher Award Competition. Recently he was appointed as a Marie Curie research fellow at IDMind Systems Engineering in Lisbon, Portugal. His research work has been published in accredited scientific journals and conferences, such as ACM CHI, ACM IUI. His publications include a best paper award at the SouthCHI 2013 conference and a best paper nomination award and honorable mention at the UMAP 2014 conference. He has professional experience for over ten years as a work package leader, technical manager and researcher in six EU projects (H2020 Serums, H2020 GrowMeUp, AAL Success, AAL CogniWin, FP7 Miraculous-Life, Marie Sklodowska-Curie SocialRobot) and three National projects.

\paragraph{Argyris Constantinides} (male, MSc, BSc) is CTO and co-founder of Cognitive UX LTD, and a Ph.D. candidate at the Department of Computer Science, University of Cyprus. He holds a M.Sc. in Web Science and Big Data Analytics from the University College London (2015), and a B.Sc. in Computer Science from the University of Cyprus (2014). His research interests revolve around the areas of Human-Computer Interaction, Usable Security, and Intelligent User Interfaces and he has published scientific papers on accredited conferences (ACM IUI, ACM UMAP, ACM MobileHCI, IEEE/WIC/ACM Web Intelligence). Constantinides has professional experience as a researcher and software engineer in EU research projects (H2020 SERUMS, EU-AAL MEMENTO, EU-AAL SUCCESS) and National research projects. In the past, he has worked as a software engineer at Playtech BGT Sports, London, UK (2015-2017).

\paragraph{Christos Fidas} (male, PhD, MSc, BSc) is Scientific Advisor of Cognitive UX LTD and an Assistant Professor at the University of Patras, Greece. He has a long-standing professional experience (since 2000) as an academic and entrepreneur in applying User-Centered Design methods in various application domains. As an academic, he has published more than 70 papers in accredited scientific journals and conferences. He has also professional experience in project management and human resource management in more than ten EU-funded and National research projects (H2020 Serums, FP7 CR-PLAY). As an entrepreneur, he was the co-founder, CEO and CTO of Instance LTD software development company (2002-2010), and patent owner that was acquired by an angel investor of the National Bank of Greece.

\subsubsection*{Relevant Publications}

\begin{itemize}
\item A. Constantinides, M. Belk, C. Fidas and A. Pitsillides. \emph{An eye gaze-driven metric for estimating the strength of graphical passwords based on image hotspots}. ACM Intelligent User Interfaces (IUI 2020), ACM, 33-37, 2020. (16\% acceptance rate)
\item A. Constantinides, M. Belk, C. Fidas and A. Pitsillides. \emph{On the accuracy of eye gaze-driven classifiers for predicting image content familiarity in graphical passwords}. ACM User Modeling, Adaptation and Personalization (UMAP 2019), ACM, 201-205, 2019.
\item C. Katsini, C. Fidas, G. Raptis, M. Belk, G. Samaras and N. Avouris. 
\emph{Influences of human cognition and visual behavior on password security during picture password composition}. ACM SIGCHI Human Factors in Computing Systems (CHI 2018), ACM Press, paper 87, 2018.
\item C. Katsini, C. Fidas, G. Raptis, M. Belk, G. Samaras and N. Avouris. \emph{Eye gaze-driven prediction of cognitive differences during graphical password composition}. ACM SIGCHI Intelligent User Interfaces (IUI 2017), ACM Press, 147-152, 2018.
\item M. Belk, C. Fidas, P. Germanakos and G. Samaras. The interplay between humans, technology and user authentication: a cognitive processing perspective. \emph{Computers in Human Behavior} 76:184-200, 2017.
(Impact Factor: 4.252)


\end{itemize}

% =====================
\Participant{UCM}{ }

\begin{wrapfigure}{R}{6,2cm}
\vspace{-3cm}
\hfill \includeimage[width=7cm]{logos/UC3M-logo.png}
\vspace{-1cm}
\end{wrapfigure}
\vspace{10pt}

Founded in 1989, University Carlos III of Madrid (UC3M) is a public university characterized by its strong
international focus, the quality of its faculty, excellence in research, and commitment to society. Despite being
a young university, its achievements have placed it among the top universities in Spain. UC3M was one of
the top five Spanish universities selected in 2009 for the Spanish Campus of International Excellence program.
UC3M
is listed in the QS World University ranking in position 311 universities in the world, in position 201-300 for
Computer Science, and in position 35 in the top 50 universities under 50 years. 

The Computer Architecture, Communication and Systems Group (ARCOS) is part of the School of Engineering. It was formed in 2001 and has been able to attract as faculty members researchers from different
universities from Spain and other countries as well as from the industry. ARCOS focuses on research in
parallel and distributed computing systems, and real-time embedded systems. ARCOS coordinated the NESUS COST Action (IC 1304). The ARCOS group coordinated the  FP7 REPARA project, and the H2020 ASPIDE project. Besides that, ARCOS is also a member of international networks as the HIPEAC (European Network of High Performance and Embedded Architecture and Compilation) and ETP4HPC (European
Technology Platform for High Performance Computing) and actively particpates in the ISO C++ standardization effort through ISO/IEC JTC1/SC22/WG21 (C++ standards committee) representing the Spanish National
Body (UNE).

ARCOS cooperates with well-known international research centers such as Argonne National Laboratory, CERN, IMEC, European Space Agency, University of St. Andrews, IBM Research, University of Pisa, University of Torino, and Univeristy of Linkoping. Besides, UC3M has a good cooperation history with several members in this proposal.

\subsubsection*{Role in Elysian}
\textbf{\UCM will lead WP6 on the Refactoring-based Methodology with a particular focus on new coding standards, compliance and continuous deployment. It will also contribute specifically to WP4 for the work on security contracts, formalisation of refactorings and analysis of risk assessment, and WP3 for the work on identifying and repairing vulnerabilities. It will also contribute to WP2, WP7 and WP8. }



\subsubsection*{Key Personnel involved in the Project}

\paragraph{Prof. J. Daniel GARCIA} (male) is a Senior Member of the IEEE and a member of the ACM. He has worked in industry for major companies in Spain and Germany including Telefonica, British Telecom, ING Bank and SIEMENS, having the opportunity to participate in large scale international projects.
He has published more than 80 journal and conference papers and has edited several special issues for the Journal of Supercomputing, International Journal of Parallel Programming, and Concurrency and Computation: Practice and Experience. He has participated in 14 technology transfer contracts and 22 publicly funded research projects, as well as many others before joining academia. He was the Project Coordinator of the FP7 project REPARA, and he also led UC3M participation in H2020 project RePhrase.
He has been visiting researcher at University of Modena, Italy, and Visiting Faculty at University of
Texas, A\&M. He was General Chair of IEEE ISPA 2012 and ICA3PP 2010 and has served in PC of
more that 50 other related conferences. Since 2008 he is the Spanish Head of delegation in the ISO
C++ Standards Committee where he actively participated in the development of the current C++11, C++14, C++17 and C++20 standards. He participates with special intensity in the C++ Evolution Working Group and the special Group on Concurrency and Parallelism. He is an
Associate Professor of Computer Architecture at UC3M since 2006.

\paragraph{Prof. Jesus CARRETERO} (male) is a Professor of Computer Architecture at University Carlos III of Madrid since 2002, where he has been involved in a wide range of real-time, security, and high-performance data management related research, ranging from computer systems (distributed and parallel storage solutions and file systems) to communications systems (security, energy, and programming techniques). Prof. Jesus Carretero has coordinated two EU projects related security issues: EID@CLOUD for federated EU identification and MADE for secure delivery of electronic documents.  He has also coordinated the NESUS COSt Action IC1305, and he is the chair for the ADMIRE project, that has been just granted by EuroHPC. He has published in excess of 150 journal and conference papers, has edited conference proceedings (such as SBAD-PAC 2019, CCGRID 2017, ICA3PP 2016 and EuroMPI 2013 ), and special issues of IoT journals, has supervised 20 PhD students in several European universities, and has co-authored 19 book chapters. He has taught in several universities (UPM, UC3M, Northwestern University) and he is Senior Member of the IEEE, member of the IEEE TPDS Committee, and member of the ACM. He is currently coordinating the Informatics area in the Spanish Research Agency. 

\paragraph{Prof. Javier GARCIA} (male) is an Associate Professor of the University Carlos III of Madrid since 2018. He received the MS degree in Computer Science in 2007 at the University Carlos III of Madrid. He also received a PhD in Computer Science from University Carlos III in 2010. He has cooperated in several
projects with researchers from various high performance research institutions such as HLRS (funded by HPC-Europe program), DKRZ, and Argonne National Laboratory. 
He is the coordinator of the H2020 ASPIDE project, a FET-HPC project focused on Exascale computing. He has been involved in other research projects funded by the European Union (such as FP7 REPARA or H2020 Rephrase). He has participated in many conference organization committees, including EuroMPI, ICA3PP, IUCC 2016 and CCGRID 2017.
He currently counts with 80 publications in journal and conference papers.


\subsubsection*{Relevant Publications}

\begin{itemize}

\item
C. Brown, V. Janjic, A.D. Barwell, J.D. Garcia, and K. MacKenzie. 
Refactoring GrPPI: Generic Refactoring for Generic Parallelism in C++. 
\emph{International Journal of Parallel Programming} 48:603–625, 2020. Springer. 
%ISSN: 1573-7640. 
%DOI: 10.1007/s10766-016-0425-6.


\item 
J. López-Gómez, D. Rio-Astorga, M.F. Dolz, J. Fernández, and J.D. García. 
Detecting semantic violations of lock-free data structures through C++ contracts.
\emph{Journal of Supercomputing} 76:5057–5078, 2020. 
%ISSN: 0920-8542. 
%DOI: 10.1007/s11227-019-02827-4.

\item

D. Rio-Astorga, M. Dolz, J.D. Garcia, M. Torquati and M. Danelutto. 
Finding Parallel Patterns through Static Analysis in C++ Applications. 
\emph{International Journal of High Performance Computing Applications} 32(6):779–788, 2018. 
%SAGE Publications, Ltd. Reino Unido. 
%ISSN: 1094-3420. 
%DOI:10.1177/1094342017695639.

\item
M.F. Dolz, D. Rio-Astorga, J. Fernández, J.D, Garcia and J. Carretero.
Towards Automatic Parallelization of Stream Processing Applications.
\emph{IEEE Access} 6:39944–39961, 2018. 
%ISSN: 2169-3536. 
%DOI: 10.1109/ACCESS.2018.2855064

\item
G. Dos Reis, J.D. Garcia, J. Lakos, A. Meredith, N. Myers and B. Stroustrup. 
P0542R5: Support for contract based programming in C++. 
ISO/IEC JTC1/SC22/WG21 (C++ standards committee), 2018.


\end{itemize}

\subsubsection*{Relevant Research Projects}

\begin{itemize}

\item
ASPIDE: exAScale ProgramIng models for extreme Data procEssing,
(FET-HPC-2017, 2018-2021),
%Funded by European Commission.
Coordinated by University Carlos III of Madrid.
%6/2018 -- 06/2021.

\item
BigHPC: Towards the unification of HPC and Big Data pardigms.
Funded by Spanish Ministry of Science and Innovation.
Coordinated by University Carlos III of Madrid.
01/2017 -- 12/2019.

\item eID@Cloud: Integrating the eIdentification in European
cloud platforms according to the eIDAS Regulation.
Funded by European Commission.
(CEF-2016: 2016-EU-IA-0064, 2017-2018),
Coordinated by University Carlos III of Madrid.
%05/2017 -- 05/2018.

\item RePhrase: Refactoring Parallel Heterogeneous Resource-Aware Applications -- a Software Engineering Approach.
H2020 ICT-09-2014.
%Funded by European Commission.
Coordinated by University of St. Andrews.
01/2015 -- 12/2017.


\item REPARA -- Reengineering and Enabling Performance And poweR of Applications,
(FP7-ICT-609666, 2013-2016),
%Funded by European Commission.
Coordinated by University Carlos III of Madrid.
%09/2013 -- 08/2016.

\end{itemize}

% ============================
\Participant{FRQ}{(\url{https://www.frequentis.com/})}

\begin{wrapfigure}{R}{6,2cm}
\vspace{-3cm}
\hfill \includeimage[width=7cm]{logos/FRQ_logo.png}
\vspace{-1cm}
\end{wrapfigure}
\vspace{10pt}

Established in 1947 Frequentis has grown to an international supplier of communication and information systems with an export quota of more than 95\%. The core market is focused on safety critical control room solutions in various domains. Over 2000 employees are working in the corporate headquarters in Vienna, and subsidiaries in more than 50 countries worldwide. Frequentis is a member of SESAR Joint Undertaking and an active participant in numerous researches, regulatory, industry and standardization communities.

\subsubsection*{Role in Elysian}
\textbf{\FRQshort{} will mostly contribute to WP7. It has key expertise in international project and technology know-how as an industrial partner in various safety critical domains. Particularly the knowledge as a system integrator in the Air Traffic Management (ATM) domain, which is one of the considered use cases, will help to integrate the different components developed by the partners in the project to have successful demonstration and validation in the end. The international knowledge and experience of 
\FRQshort{} will assure that the consortium possesses the scientific and technological expertise required for Elysian.}

%\vspace{10pt}
%\textbf{Frequentis leads work package WPXX}
\vspace{10pt}

\textbf{Key Personnel involved in the Project}

\paragraph{Eduard Gringinger} (male) (PhD, MSc, MSocEcSc, BSc), is senior lead scientist and project manager with more than 10 years of experience in the research areas of safety critical data and services with a business focus on the air traffic management domain. He has work experience in relation with information management, data and service modelling and is member in various EUROCAE, ICAO/AMO working groups. He received a PhD, MSc, and a MSocEcSc with distinctions from Vienna University of Technology.

\paragraph{Christoph Fabinaek} (male) (PhD, MSc, MBA), is senior lead scientist. Since 2006, he worked in various roles in safety critical domains at Frequentis. He was consultant for 3 years at AI Informatics and Siemens and is now is chairman of OwnYourData. He received his PhD from the Vienna University of Technology in mathematics (also working at the Innovative Computing Laboratory/University of Tennessee) and an MBA from the Danube University Krems (1 semester at the Weatherhead School of Management in Cleveland, Ohio).

\subsubsection*{Relevant Publications}

\begin{itemize}
\item C.G. Schuetz, B. Neumayr, M. Schrefl, E. Gringinger and S. Wilson, \emph{Semantics-based summarisation of ATM information: Managing information overload in pilot briefings using semantic data containers}. The Aeronautical Journal (2019), 1-27. %\url{doi:10.1017/aer.2019.74}
\item E. Gringinger, R.M. Keller, A. Vennesland, C.G. Schuetz, and B. Neumayr. \emph{A Comparative Study of Two Complex Ontologies in Air Traffic Management}, Proc. of the AIAA/IEEE 38th Digital Avionics Systems Conference (DASC), IEEE, September 2019. %\url{doi.org/10.1109/DASC43569.2019.9081790}
\item E. Gringinger, C. Fabianek, C. Schütz, B. Neumayr, M. Schrefl, A. Vennesland, S. Wilson, \emph{The Semantic Container Approach: Techniques for ontology-based data description and discovery in a decentralized SWIM knowledge base}, Proc. of the SESAR Innovation Days 2018 (SID 2018), December 2018. %, Salzburg, Austria. %\url{https://www.sesarju.eu/sites/default/files/documents/sid/2018/papers/SIDs\_2018\_paper\_78.pdf}
%\item E. Gringinger, C. Fabianek, C. Schütz, J. Stöbich, B. Neumayr, M. Schrefl, "A Proof-of-Concept Implementation of a Semantic Container Management System for Air Traffic Management", Proceedings of the EDAW 2018 Posters and Demonstrations Session co-located with 21st International Conference on Knowledge Engineering and Knowledge Management (EKAW 2018), November 12-16, 2018, ISSN 1613-0073, pp. 69-72, Nancy, France. \url{https://dblp.org/rec/bib/conf/ekaw/GringingerFSNS18}
\item E. Gringinger, C. Schuetz, B. Neumayr, M. Schrefl and S. Wilson, \emph{Towards a value-added information layer for SWIM: The semantic container approach}, 2018 Integrated Communications, Navigation, Surveillance Conference (ICNS), Herndon, VA, 2018, pp. 3G1-1-3G1-14. 
%\url{https://doi.org/10.1109/ICNSURV.2018.8384870}
\item C. Flachberger, E. Gringinger, "Decision Support for Networked Crisis \& Disaster Management - A Comparison with the Air Traffic Management Domain", Information Systems for Crisis Response and Management (ISCRAM), Rio de Janeiro, Brasil, May 22-25, 2016. %\url{https://episecc.eu/sites/default/files/1332\_ChristianFlachberger\%2BEduardGringinger2016.pdf}
\end{itemize}


\subsubsection*{Relevant Research Projects, Patents, and Products}

\begin{itemize}

\item SINAPSE (H2020-SESAR-2019-2, Grant agreement ID: 892002) - Software defined networking architecture augmented with Artificial Intelligence to improve aeronautical communications performance, security and efficiency, \url{https://cordis.europa.eu/project/id/892002};

\item SATIE (H2020-SU-INFRA-2018, Grant agreement ID: 832969) - Security of Air Transport Infrastructure of Europe, \url{https://cordis.europa.eu/project/id/832969};

%\item SAPIENT (H2020-SESAR-2015-1, Grant agreement ID: 699328) - Satellite and terrestrial architectures improving performance, security and safety in Air Traffic Management (ATM), \url{https://cordis.europa.eu/project/id/699328};

\item PJ19 CI (H2020-SESAR-2015-2, Grant agreement ID: 731765) - Sesar2020 Pj19-03 - Air Traffic Management Systems and Services, \url{https://www.eurocontrol.int/articles/content-integration-sesar-2020-project-pj19-ci};

\item BEST (H2020-Sesar-03-2015, Grant agreement ID: 699298) - Achieving the BEnefits of SWIM by making smart use of Semantic Technologies, \url{http://www.project-best.eu/};

%\item Engage (H2020-SESAR-2016-2, Grant agreement ID: 783287) - SESAR Knowledge Transfer Network, \url{https://www.engagektn.com/};

%\item SEMCON (ICT of the Future Call 2018 Data Market: 869781) - Semantic Containers for Data Mobility, \url{https://www.ownyourdata.eu/en/semcon/};

%\item Patent for prioritization information (EP3118839A1) could be relevant for Digital Fortress, \url{https://patents.google.com/patent/EP3118839A1/en};

\item MosaiX SWIM, the Frequentis ATM informaiton integration platform, \url{https://www.frequentis.com/sites/default/files/support/2018-02/MosaiX\%20SWIM.pdf}; 

\end{itemize}

% ============================
\pagebreak
\Participant{YAG}{(\url{https://www.yagaan.com/en})}


\begin{wrapfigure}{R}{5cm}
\vspace{-2.8cm}
\hfill \includeimage[width=5cm]{logos/YAG-logo.png}
\vspace{-0.8cm}
\end{wrapfigure}
\label{sec:participantYAG}

\vspace*{0.5cm}

%===============================================================================
\YAGlong{} is a french cybersecurity product editor dedicated at supporting cyber risks management in software applications. The company was established in January 2017 under the status of Simplified Joint Stock Company with a capital of 85,000 Eur. The company's product, named \textit{YAG-Suite}, equips security and privacy by design development processes as well as source code audit service provision. 

The YAG-Suite is a smart tool based on static analysis and machine learning which supports identification of the vulnerabilities that matter in the source code of software applications. It assists the users to effectively detect security and sensitive data protection breaches in the target application, to understand their causes via detailed diagnostics, to identify the necessary and sufficient remediation actions as well as feeding managerial dashboards.The YAG-Suite supports the two phases vulnerability investigation processes:
\begin{itemize}
\item To equip code reviews and "security and privacy by design" software development processes, the YAG-Suite offers automated scanning capabilities and false positives reduction. Deployed on the developer's workstation, in continuous integration software factory or in SAAS mode, the YAG-Suite comprehensively scans the source code of the project, without requiring that the project be "buildable", and without the need to document the code. It significantly reduces the false positives rate raised by statis analysis with supervised machine learning,
\item To facilitate the manual investigations driven by code auditors, the YAG-Suite provides unprecedented "code mining" capabilities. These advanced functionalities provide assistance in discovering and understanding an unknown application, in identifying the processed sensitive data and their protection mechanisms, in searching for specific properties of the application (not limited to security properties) and high interest zones (smells code, critical functionalities, business data), thanks to the crawling of relevant information in the source code.
\end{itemize}

\vspace{10pt}
\subsubsection*{Role in Elysian}
\textbf{\YAGshort{} will make its key contributions in  WP2 on Vulnerability Detection and Self-healing given its expertise in the use of static analysis methods for security. It will also contribute to WP4, in particular leading a metric-based assessment of security requirements task, WP3,and  WP6-WP8.
 }

\vspace{10pt}

\textbf{Key Personnel involved in the project}

\paragraph{Hervé Le Goff} (male) (MSc) is Co-founder and CEO of \YAGlong{}. He has been working at the french ministry of defence for more than 25 years in the field of information systems, cybersecurity, procurement and management. He also gained experience in multinational relations during a three years post at an international organisation HQ in Brussels.

\vspace{10pt}

\paragraph{Dr Antoine Floc'h} (male)  is Co-founder and CTO of \YAGlong{} with more than 13 years of experience in the research and innovation area of compiling, SAST, operational research and source code scanning. He is the author and technical lead of \YAGlong{} technology.

\subsubsection*{Relevant publications}
\begin{itemize}
\item K. Martin, C. Wolinski, K. Kuchcinski, A. Floch and F. Charot. Constraint Programming Approach to Reconfigurable Processor Extension Generation and Application Compilation. ACM Transactions on Reconfigurable Technology and Systems - TRETS. 5 (2):1-38, 2012. 10.1145/2209285.2209289.
\item C. Wolinski, K. Kuchcinski, K. Martin, A. Floch, E. Raffin and F. Charot. Graph Constraints in Embedded System Design, Workshop on Combinatorial Optimization for Embedded System Design, Bologna, Italy, 2010.
%Wolinski, C., Kuchcinski, K., Martin, K., Floch, A., Raffin, E., & Charot, F. (Accepted/In press). Graph constraints in embedded system design. Paper presented at 

\item K. Martin, C. Wolinski, K. Kuchcinski, A. Floch and F. Charot. Constraint-Driven Identification of Application Specific Instructions in the DURASE System. 
In: Bertels K., Dimopoulos N., Silvano C., Wong S. (eds) Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS 2009). LNCS 5657, pp 194-203, 2009. Springer, Berlin, Heidelberg.
%194-203. 10.1007/978-3-642-03138-0-21. 
\end{itemize}




\subsubsection*{Relevant Technology Projects}

\begin{itemize}
\item YAG-Suite (2016-2020): Feasibility, design and development of a new technology for vulnerability detection in the source code of Java and PHP applications, diagnostics and remediation support. The technology detects and identify vulnerabilities in the source code with SAST and bring automation capabilities to qualify the warnings raising from SAST with machine learning, in a supervised training approach. The project led the company to be labelled ”Jeune Entreprise Innovante” (Young Innovative Company).
\end{itemize}

% ============================







\subsection{Third parties involved in the project (including use of third party resources)}

No third parties are involved in the project.

% ---------------------------------------------------------------------------
%  Section 5: Ethics and Security
% ---------------------------------------------------------------------------

\newpage

\section{Ethics and Security}

\subsection{Ethics}

The proposal raises no specific ethical concerns. Nonetheless, some of our technologies developed in \texbf{WP5} on Intelligent Identity Management and Trust (e.g. identity management components, intelligent user interface) will be designed and developed following a User-Centered Design approach with the active participation and evaluation of users. Hence, for these technologies, there is provision for user studies in which we will invite human participants to evaluate the technologies used. These are typical empirical studies, which are planned to involve adult volunteers for the collection of experimental data within specific use case scenarios. 
These users will be monitored through logging their usage with the supported tools and devices. It is expected that users will be recorded on video, as well as their eye gaze behavior and physiological signals will be monitored and analyzed in order to establish efficiency, effectiveness and quality of the developed technologies. We also plan to collect the participants' opinions through semi-structured interviews, surveys and focus groups that will be based on accredited questionnaires for requirements' elicitation as well as for measuring the users’ perceived usability, security and trust with regards to the developed technologies.

In this context, we will comply with research ethics' regulations and guidelines such as the Code of Professional Conduct of the User Experience Professionals Association (UXPA - https://uxpa.org/uxpa-code-of-professional-conduct), which is the most established set of guidelines for conducting usability studies. This foresees in §6 the rules that should be followed with respect to privacy, confidentiality and anonymity in any usability study. We will also comply with the relevant regulation, principles and legislation of the European Commission and particularly those involving the participation of adults in user studies (https://ec.europa.eu/programmes/horizon2020/en/h2020-section/ethics). The participants will be informed, and their consent will be required for participation. 
Participants can decide to leave the study at any time.
All data will be confidential, secure, anonymous, and the identification of individuals will not be possible.

%\textbf{Ethics Form, question 2 HUMANS (YES)} - need to indicate a page in the portal where this gets discussed/described (here).

%Does your research involve human participants?\\

%\textbf{Ethics Form, question 4 PERSONAL DATA (YES)} - need to indicate a page in the portal where this gets discussed/described (here).\\

%Does your research involve personal data collection and/or processing? \\


%\textbf{Ethics Form, question 6 Third Countries}
%Do you plan to import any material - including personal data - from non-EU countries into the EU? (NO)??\\

\subsection{Security}

Please indicate if your proposal involves:

\begin{itemize}
\item
activities or results raising security issues: NO
\item
'EU-classified information' as background or results: NO
\end{itemize}

Although \TheProject{} deals with use cases from the domains where privacy of the data is of paramount importance, such as banking and healthcare, the use cases will be developed and evaluated on synthetic but realistic data that will be produced using the data fabrication technology from \IBMshort{}. Therefore, over the course of development and evaluation, the \TheProject{} technologies will not use real data and, therefore, will not present any security and/or privacy concerns.    
%% Write macro to split Sections 4-5
\Split{4-5}

%% Finalise batch file
\immediate\write\BatchFile{exit}% 
\immediate\closeout\BatchFile% 

\newpage

\label{bibliography}
\addcontentsline{toc}{section}{References}

%\bibliographystyle{abbrv}
%\bibliography{bibliography_ustan}
%\bibliography{bibliography_scch}

\end{document}
