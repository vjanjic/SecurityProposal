     % !TeX encoding = UTF-8
\documentclass[a4paper,11pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{IEEEtrantools}
\newcommand{\project}[1]{\textbf{#1}\xspace}
\newcommand{\SECURITY}{\project{Elysian}}
\newcommand{\TheProject}{\SECURITY}

%\ifdefined\final
%\else
%\newcommand{\final}{}
%\fi

\input{preamble}
\input{participants}

\begin{document}
\pagenumbering{arabic} % for pageslts

\begin{titlepage}

\begin{center}
{\Huge \textsc{\TheProject}}
\end{center}

\begin{tabular}{lp{5in}r} %something strange about the spacing...
\textbf{Title of Proposal:}&\hspace*{-7cm}\textbf{Intelligent Security and Privacy for AI-Based Big Data Analytics } & \\[4ex] 
\textbf{Date of preparation:} &\hspace*{-3cm} \textbf{\today} & \comment{}{$
$Revision: 0.0$ $}\\[4ex]
\textbf{List of participants} && \\[1ex]

https://www.overleaf.com/project/5e5e45121e493b000149fe20
\end{tabular}

%% Participants Table
\newcounter{p}
\begin{center}
\begin{tabular}{|l|p{5in}|l|l|}\hline
\textbf{Participant no} & \textbf{Participant organisation name} & \textbf{Country}\\ \hline 
1 (Coordinator) & {\sc \longparticipant{1}} \hfill (\shortparticipant{1}) & \country{1}  \\ \hline
\forloop{p}{2}{\value{p} < \theparticipant}{%
\thep & {\sc \longparticipant{\thep}} \hfill  (\shortparticipant{\thep}) & \country{\thep}  \\ \hline}%
\theparticipant & {\sc \longparticipant{\theparticipant}} \hfill  (\shortparticipant{\theparticipant})& \country{\theparticipant}  \\ \hline
\end{tabular}\end{center}https://www.overleaf.com/project/5e5e45121e493b000149fe20

\tableofcontents

\end{titlepage}

% \input{snags}
\newpage


\pagenumbering{roman}

% ---------------------------------------------------------------------------    
%  Section 1: Excellence
% ---------------------------------------------------------------------------

\pagebreak

ToDo List:

\begin{itemize}

\item Section 1.3.1 : Add challenges for security and privacy of big data analytics (basically, a few sentences to motivate why we do the work we propose)

\begin{itemize}
\item USTAN for formal modelling and refactoring
\item COGNI for authentication
\end{itemize}

\item Section 1.3.3 : Add the relevant technologies that will be developed over the course of the project, together with current and future TRLs
\begin{itemize}
\item USTAN for formal methods and refactoring
\item SCCH for privacy-preserving AI
\item YAG for source code analysis
\end{itemize}

\item Section 1.3.4: Add how this project goes beyond what is done in the previous ones (EVERYONE)

\item Section 1.4 : Add the related work and advancement beyond state-of-the-art:
\begin{itemize}
\item YAG for static code analysis in 1.4.1 (maybe add a picture of the tool in action too?)
\item USTAN for formal methods in 1.4.4, including security contracts
\item COGNI for authentication in 1.4.6
\end{itemize}

	
\item Section 1.4.12 : Write Innovation Potential (UOD)

\item Section 2.1 : Finish writing up the Impacts (UOD)

\item Section 2.1 : Write up Improving Innovation Capacity, Societal Impact and Barriers (UOD)

\item Section 2.3.1 : Add security, ML, refactoring and formal methods conferences and journals (EVERYONE)

\item Section 2.3.1 : Add membership to the standardisation committees and general scientific and technical community (EVERYONE)

\item Section 2.3.2: Write up Draft Exploitation Plans:
\begin{itemize}
\item SCCH Exploitation Plan
\item DEM Exploitation Plan
\item FRQ Exploitation Plan
\item COGNI Exploitation Plan
\end{itemize}

\item Section 2.3.2 : Write a joint exploitation plan (EVERYONE)

\item Section 3: Write up WP5, make tasks consistent with the Gantt chart (COGNI)

\item Section 3: Write up WP6, make tasks consistent with the Gantt chart (UC3M)

\item Section 3: Write up WP7, make tasks consistent with the Gantt chart (SOPRA)

\item Section 3: Write up critical risks table (EVERYONE to provide 1-2 critical risks and mitigations related to their technologies)
\begin{itemize}
\item Think about political critical risks (Brexit, COVID, closures of universities/companies, currency exchange rate)
\item Make sure to specify that in the case of disagreement, EU law and court of Brussels will be in charge
\item Consider risks about tooling not being available
\end{itemize}




\item Section 3.2 : Think about people to add to the Advisory Board and contact some of them (EVERYONE)

\item Section 3.3: Draw a table of expertise (UOD)

\item Section 3.4: Write the table for staff effort



\item Section 4.1 Write up USTAN description (USTAN)

\item Section 4.1: Write up UC3M description (UC3M)

\item Section 4.1: Write up UOD description (UOD)

\item Section 4.1: Write up DEM description (DEM)

\item Section 5: Think about security and ethics considerations (EVERYONE)
\end{itemize}

DONE:

\begin{itemize}
\item Section 1.3.1 : Add challenges for security and privacy of big data analytics (basically, a few sentences to motivate why we do the work we propose)
\begin{itemize}
\item UC3M for security standards
\end{itemize}

\item Section 1.4 : Add the related work and advancement beyond state-of-the-art:
\begin{itemize}
\item SCCH for runtime analysis in 1.4.2. 
\end{itemize}
\end{itemize}


\begin{itemize}
\item Section 3: Write up WP3, make tasks consistent with the Gantt chart (UOD)

\item Section 3: Make changes to the list of milestones and deliverables (UOD):
\begin{itemize}
\item Merge Mi6 and Mi7
\item Move D2.3, D3.3 and D4.3 to Mi12
\item Make some deliverables software instead of reports
\item Assign D7.2 and D7.3 to Sopra
\item Give leadership to some WP2 deliverables to YAG
\item Give leadership to some WP4 deliverables to SCCH
\item Add the Data Management Plan as the deliverable (M6) 
\item MOOC deliverable and milestoenes at M22 
\item Add milestones for Data Management Plan
\end{itemize}

\item Section 2.3.2: Write up Draft Exploitation Plans:
\begin{itemize}
\item UOD Exploitation Plan
\end{itemize}

\item Section 3.2: Write up tables of WTLs and PIs (UOD) -- EVERYONE TO CHECK

\item Section 1.3.1 : Add challenges for security and privacy of big data analytics (basically, a few sentences to motivate why we do the work we propose)

\begin{itemize}
\item IBM for symbolic execution
\end{itemize}

\item Section 1.3.3 : Add the relevant technologies that will be developed over the course of the project, together with current and future TRLs
\begin{itemize}
\item IBM for ExpliSat
\end{itemize}

\end{itemize}




\pagenumbering{arabic}
\setcounter{page}{2}

%%\subsection{Contributions from the Partners}

%%\subsubsection{University of St Andrews - Security Contracts and Verification}
%%\begin{itemize}
%%    \item Formal specification of security contracts for pieces of code
%%    \item Refactoring to introduce secure code in the applications
%%    \item Formal verification of properties of the code with regard to security contracts
%%    \item Automated verification and reasoning (model checkers, constraint solvers and theorem provers, sometimes used in combination), logics including distributed temporal logics 
%%\end{itemize}

%%\subsubsection{Dundee - Security for Big Data}
%%\begin{itemize}
%%    \item Identifying and addressing security risks in large-scale distributed databases, both open- and closed-source ones.
%%    \item Identifying and analysing security risks in distributed parallel processing.
%%    \item Security for machine-learning based big data analysis
%%    \item Security contracts for distributed parallel code
%%    \item Dissemination
%%\end{itemize}

%%\subsubsection{IBM - Vulnerability Detection and Data Fabrication}
%%\begin{itemize}
%%\item  Vulnerability Detection
   
%%We propose to extend the ExpliSAT symbolic execution technology to discover known security vulnerability patterns in C/C++ code and combine it with the tailored fuzzing techniques in specific areas for the purpose of assisting the symbolic execution engine to consistently make progress and thus overcome a known "path explosion" problem of symbolic interpretation technologies.
 
%%\item Data Fabrication for ML
 
%%We have recently started to explore a new and very challenging direction of synthetic data fabrication for improving robustness of ML models and AI-based applications. It includes  use cases like data enrichment (missing or not sufficient training data), poisoned data (malicious data that is used as part of training data set to cause machine learning model malfunction) and evasion attack (malicious data that causes malfunction of a trained model).
%%Most of the recent research work in academia in this field is done based on "unstructured data" like images, video or text. We believe that for real industry use cases fubrication of structured data to test and improve robustness of ML models is more relevant. Our idea is to combine our rule-based (CSP-based) data fabrication approach with machine learning techniques to fabricate synthetic data for ML use cases.
 
%%\item Data Fabrication Platform
 
%%It is actually our current DFP product that is used in SERUMS. It can be mentioned and used in the new proposal in combination with any of the two "new" direction listed above.
%%\end{itemize}

%%\subsubsection{SCCH - Privacy-Preserving AI}
%%\begin{itemize}
%%    \item Informational Privacy
    
%%    To constrain the information leakage from a data set, we motivate an information theoretic approach to  privacy where privacy is quantified by the mutual information between sensitive private information and the released public data.
%%    \item Optimal Privacy Secured Data Release Mechanism 
    
%%    A data release mechanism aims to provide useful data available while simultaneously limiting any reveled sensitive information. The data perturbation approach uses a random noise adding mechanism to preserve privacy, however, results in distortion of useful data and thus utility of any subsequent machine learning and data analytic algorithm is adversely affected. We introduce a novel information theoretic approach for studying privacy-utility trade-off suitable for different data types (including high-dimensional data, signals, and images) and for the cases with unknown statistical distributions. 
    
%%    \item Privacy Secured Knowledge Sharing
    
%%   While sharing of knowledge extracted from a relatively large set of labelled data owned by an organization with another organization owning a few or no labelled data, it is intended that 
%%    \begin{itemize}
%%        \item privacy of data is preserved;
%%        \item transferability of knowledge from source to target domain is evaluated for the design and analysis of transfer learning algorithms;
%%        \item privacy-transferability trade-off is optimized. 
%%    \end{itemize} 
%%    We aim at the development of techniques and tools for the study and optimization of privacy and transferability aspects of machine learning based AI systems. 
%%\end{itemize}
%%\subsubsection{Cognitive UX - Intelligent User Authentication-as-a-Service}
%%\begin{itemize}
%%\item AI-driven and eye gaze-driven behavioral authentication (Topic C and D)

%%We can bring in multi-factor authentication solutions and more specifically AI-driven behavioral authentication by adding another layer of security in the authentication process to verify the end-users based on their interaction behavior, and/or their eye gaze behavior. For doing so, we currently utilize and can also bring in the project a range of eye-tracking and wearable technologies.

%%\item Authentication-as-a-Service (Topic C and D)
%%We are interested to conduct research and extend our product (Cognitive Authentication), which offers an integrated user authentication solution that allows service providers to set their own password policies, authentication types, get insights from end-users' interaction data, etc.

%%\item User experience and usability evaluations and activities
%%\item System integration and testing
%%\item Dissemination activities on Usable Security, HCI, Authentication, Eye-tracking, and Intelligent User Interfaces
%%\end{itemize}

%%\subsubsection{Sopra-Steria Limited}

%%As a commercial partner we want to start investigation into the following areas:
%%\begin{itemize}
%%    \item Finance (open banking for CMA9) as data source for a universal banking solution that supports a secure banking interaction with appropriate trust and privacy as expected. Want to test existing and future code for security vulnerabilities.
    
%%    \item Space-based Open Internet capabilities with low Earth orbit satellites using an open meshed network for communication. Investigate how Delay/Disruption Tolerant Networking (DTN) and Solar System Internet (SSI) will impact our communication security and enable citizen services like social care and healthcare. Want to include security by design into the code and indirectly the deployed systems.
    
%%    \item Distributed healthcare (IoT) to support remote healthcare and intervention via open network technology. Test and repair the IoT code for vulnerability at the edge of the network.
    
%%    \item Sovereign Identity Clearance House using a Self-sovereign identity (SSI) token for essential communication as a citizen with the citizen's digital twin.
%%\end{itemize}

%%We propose three business areas to research the develop of automated tools for validating the security and privacy of data processing code, underlying systems' code and exposed online services used to provision these services to citizens. We want to introduce a practical solution for zero-trust architecture of the system while preserving the ever-increasing use of open and common internet-based communication channels between businesses and citizens without losing the trust and security that each citizen wants.

%%Sopra-Steria Limited will supply an insight into the real-world case studies that we could use to prove the research is achieving the proposed outcomes.

%%We also hold various advance technical capability that can be used to support the general project to ensure data engineering, machine learning and data science skills are available for use by the projects research life-cycle.

%%\subsubsection{University Carlos III of Madrid}

%%\begin{itemize}
%%\item C++ standardization process with 12 years of membership in the ISO C++ standards committee.
%%\item Integration in the C++ programming language of contracts in the form of preconditions, post conditions, invariants, \ldots
%%\item Identification and avoidance of software vulnerabilities (security and safety) derived from programming language features 
%%\item Patterns for parallel programming.
%%\end{itemize}

%%\pagebreak


\section{Excellence}

\begin{figure}[tp]
  \begin{center}
  \vspace{-5mm}
  \includeimage[scale=0.92]{DigitalFortress-Vision-v2.png}
%  \vspace{-3cm}
  \caption{\TheProject{} Vision}
  \label{fig:vision}
  \end{center}
  \end{figure}

"The world's most valuable resource is no longer oil, but data" is the now world-famous quote from The Economist\footnote{\url{https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data}} that highlights the importance of big data analytics in the modern world. Advances in the techniques for analysing the vast amount of data available today from a variety of sources are having %tremendous effects
significant impact in a number of areas. Businesses use data to provide more personalised services to customers and improve their operation, and researchers use 
%analytics techniques 
data exploration and analysis for scientific discovery made possible only due to %from
%to tackle 
the ever-growing and detailed datasets being collected. 
%coming from sensors and simulations. 
It is hence not surprising 
%,Due to this, 
that data analysts and statisticians are amongst the most desirable professions in 2019\footnote{\url{https://www.cnbc.com/2019/01/07/these-are-the-best-jobs-to-have-in-2019-according-to-us-news--world-report.html}}. Modern data analytics is necessarily distributed in nature and, for the most part, based on advanced AI techniques such as deep learning.  This 
introduces
%presents 
completely new challenges for establishing the security and privacy %properties 
of the data itself and of the analytics techniques used for processing it. A highly-distributed world of AI-based big data analytics 
is, however, also much more vulnerable and exposed to
%there are many more 
potential targets for cyber attacks then code running on single machines. Vulnerabilities arise %coming 
%both 
from the way big data is stored (usually in distributed filesystems or databases) and is processed (with multiple distributed agents interacting and exchanging potentially sensitive information). Security mechanisms that protect sensitive data in such settings are still fragile, and not advanced enough to provide sufficient trust in the whole data analytics process. 
\emph{The main goal of the \TheProject{} project is to significantly increase the trust in modern data analytics systems by assisting the end-users in developing secure and privacy-preserving distributed data analysis code, combining tools for identifying and repairing security vulnerabilities with rigorous formal methods for the verification of security and data privacy properties of the code.}



%The emergence of big data, supported by distributed machine-learning based data analytics tools and techniques, presents new and unforeseen challenges for data security and privacy. Security vulnerabilities can come both from the individual components of the distributed data-analytics systems, as well as from the undeterministic way in which these components may interact. The Digital Fortress project aims to develop a novel methodology and the associated tool-chain for implementing secure distributed data processing applications. We will tackle the problem both from theoretical and practical aspect, implementing novel formally-verifiable security contracts that will specify security properties of the distributed code, and supporting this contracts with a novel code refactoring tools and dynamic vulnerability detection techniques. The focus on the project will be on machine-learning based analytics techniques, addressing the issues that come from both storage and processing of the data and learning models. As an additional layer of security, we will implement novel integrated behaviour-based user authentication schemes.


\subsection{Aims and Objectives}
\label{sect:objectives}

\eucommentary{\emph{Describe the specific objectives for the project1, which should be clear, measurable, realistic and achievable within the duration of the project. Objectives should be consistent with the expected exploitation and impact of the project (see section 2).}}


The specific \emph{aims} of the \TheProject{} project are:

\begin{description}
\item[Aim 1:] To produce novel rigorous methods for \emph{formal specification and verification} of 
%security properties of
  distributed AI-based data analytics systems, addressing security and privacy of both data storage and data processing;

\item[Aim 2:] To %build on the 
enhance existing and develop new efficient techniques for \emph{identifying generic code patterns} that
  represent known security vulnerabilities and information leakages in distributed AI-based systems, and to develop novel techniques for  
  \emph{repairing the identified vulnerabilities and leakages};

\item[Aim 3:] To build an additional security layer on top of the existing end-user applications that will be based on novel, intelligent and secure \emph{authentication methods};

\item[Aim 4:] To integrate the \TheProject{} tools and techniques into a coherent \emph{self-healing methodology} for establishing
  security and privacy properties of the code and repairing any identified vulnerabilities and leakages, targeting both 
  existing distributed AI-based data analytics code and new provably-secure code;

\item[Aim 5:]  To demonstrate the applicability of the \TheProject{} tools and
 methodology in building secure real-world application from the medical, aerospace and
 banking domains and to promote their long-term uptake by building a sustainable user community,
 covering both experts in security and normal application developers;

\end{description}

The corresponding concrete \emph{objectives} are: 
\begin{description}

%\item[Objective 1:] To develop a novel concept of machine-readable \emph{security contracts}
%  that will be used to specify precisely security properties for parts of the C++ code, and to develop methods
%  for their formal specification and verification in the new and existing C/C++ code. \emph{This fulfils part of \textbf{Aim 1} and \textbf{Aim 4}}.
%  \comment{CB: It's not clear to me what machine-readable actually means, as, arguably, any arbitrary stream of bytes in a file is machine readable :D I think this needs clarified. There's a worry here that this is the same as what we have done on TeamPlay, where we also developed proofs, contracts and certificates for security, energy and time. What is the novelty here? There should probably be some tie-in, perhaps, to the work done on TeamPlay, CSL, for instance, or the proof system developed there. I think this needs to tie in with the refactoring tooling. One novel approach here is the refactorings... perhaps new refactorings that transform the application in such a way that it can meet a user-defined security specification or contract?  }
%  \comment{JB: If we are treating security contracts as a notion at several levels of abstraction and we can go between them, this would potentially be quite different from TeamPlay? It should not just be at the code level... }
 
\item[Objective 1:] To develop a novel concept of \emph{secure} code patterns for distributed AI-based data analytics and to support these
patterns with strictly-defined and formally verifiable multi-level 
\emph{security contracts}, raising the level of abstraction
for developing secure distributed applications and providing formal guarantees about their security properties. 
\emph{This fulfils part of \textbf{Aim 1}}. 
%  \item[Objective 1:] To develop a novel techniques for verification of security properties of the application code,
%  specified in a formal way by machine-readable \emph{security contracts}, giving strict guarantees about
%  the safety of run-time behaviour of undeterministic large-scale distributed data analytics; \emph{This fulfils part of
%    \textbf{Aim 1}}.

\item[Objective 2:] To develop novel techniques based on a combination of static analysis, dynamic symbolic execution and run-time monitoring for identifying security and privacy vulnerabilities both in storage of big data used for training of machine-learning models and in pattern-based distributed data analytics based on AI, guiding the verification of security properties in large-scale distributed settings. \emph{This fulfils parts of \textbf{Aim 1}, \textbf{Aim 2} and \textbf{Aim 4}}.
  
\item[Objective 3:] To develop novel \emph{self-healing} techniques based on software refactoring and machine-learning based run-time adaptation for semi-automatic repairing of the identified security vulnerabilities in the code, together with the novel privacy-preserving mechanisms for data analytics and knowledge-sharing across parties, providing rigorous handling of information-leakage using information-theoretic mathematical analysis. \emph{This fulfils part of \textbf{Aim 2} and \textbf{Aim 4}}.

\item[Objective 4:] To build on the existing and develop new security \emph{coding standards} for C++, providing guidance to developing new secure distributed data analytics applications. \emph{This fulfils parts of \textbf{Aim 1}, \textbf{Aim 2} and \textbf{Aim 4}}.

\item[Objective 5:] To augment the existing security layer of distributed data analytics application with AI-driven behavioural authentication models, implementing Authentication-as-a-Service (AAAS) technology. \emph{This fulfils parts of \textbf{Aim 3} and \textbf{Aim 4}}.

\item[Objective 6:] To develop a novel methodology, based on security contracts, vulnerability detection/mitigation,
  privacy preserving and secure authentication, and supported by semi-automated code refactoring techniques for
  increasing security of the existing and developing secure new distributed data-analytics algorithms. \emph{This fulfils
    part of \textbf{Aim 4}}.

\item[Objective 7:] To demonstrate that the \TheProject{} tools and techniques allow for improved security and reduced
  information leakage in distributed AI-based data analytics code, using a variety of real-world security-sensitive
  use cases from healthcare, banking and aerospace application domains. \emph{This fulfils part of \textbf{Aim 5}};

\item[Objective 8:] To build a sustainable user community involving a variety of stakeholders that will ensure the long-term
  uptake, development and commercial success of the tools and techniques developed over the course of the \TheProject{}
  project. \emph{This fulfils part of \textbf{Aim 5}}.

\end{description}

%\subsubsection{Provisional list of workpackages:}
%\begin{itemize}
%\item WP2: Specification and Verification of Security Contracts for Distributed Data Analytics (USTAN)
%\item WP3: Security and Privacy of Big Data Storage and AI-based Processing (UOD/SCCH)
%\item WP4: Vulnerability Detection (IBM)
%\item WP5: AI-Driven Behavioural Authentication (COGNITIVE UX)
%\item WP6: Refactoring-Based Methodology for Secure Application Development (USTAN)
%\item WP7: Use cases (SOPRA)
%\item WP8: Dissemination and Exploitation (UOD)
%\end{itemize}

\label{sect:objs-detailed}

\TOWRITE{UOD and USTAN}{This needs to be done after
the objectives are defined above   We need about 1-2 paragraphs
per objective, just to flesh it out.}

\subsubsection{Detailed Description of the Objectives}

\subsubsection*{Objective 1: Secure Patterns for Distributed Data Processing}
%subsubsecton*{Security Contracts for Formal Specification of Security Properties}
\vspace{-6pt}
Code patterns are a well-established abstraction for designing and implementing complex software systems, including parallel and distributed applications.  They have been endorsed by a number of major IT companies such as Intel and Microsoft. Patterns allow structuring of the end-user code at a high level of abstraction and make formal reasoning about the code much easier, as they can provide additional information to the underlying formal models. On the other hand, formal treatment of the security properties of the code requires a multi-level approach, where a high-level human-understandable description of the properties of the code is transformed into precise, machine-readable description that can be used for formal reasoning. In the \TheProject{} project, we will develop novel \emph{secure} design patterns for distributed large-scale data processing that will specify how to combine different layers of abstraction in distributed applications (from low-level libraries for distributed filesytems and databases to high-level machine-learning libraries and end-user code). These patterns will give strict guarantees about the security properties of the underlying code. This will be achieved by supporting these patterns with both \emph{security certificates}, which will describe in a natural way the properties of the code, and the associated lower-level, machine-readable \emph{security contracts} that will be appropriate for automated reasoning. We will also develop mechanisms to automatically prove the properties specified in the security contracts for C++ and Java applications, and also mechanisms to translate security certificates into security contracts and vice versa. The end users will then be able to choose a security-certified patterns suitable for their data analytics task, and will be presented with guaranteed security properties of the resulting code in an easily-understandable way. 
%Formal treatment of the security properties of the code requires a multilayer approach. On one hand, end users want to see a description of the security properties in a clear, high-level and human-understandable way. On the other hand, formal reasoning, including automatic proving of the properties, requires significantly lower-level approach. The language for formal reasoning needs to be concise and machine-readable. This necessitates different approach to describing properties at different levels. In the \TheProject{} project, we will develop a strict and precise notion of security properties at different levels of abstraction. We will develop a concept of \emph{security certificates}, which will describe in natural way the properties of the code. We will also develop \emph{security contracts}, which will be specified in low-level, machine-readable way appropriate for automated reasoning. We will also develop mechanisms to automatically prove the properties specified in the security contracts for pieces of C++ and Java code and also mechanisms to translate security certificates into security contracts and vice versa. The end users will then be able to specify the required properties using high-level language, and these properties will be automatically checked by lower-level mechanisms.
%\cbcomment{See above comments about similarities with teamplay. CSL is a source-level annotation language that allows the programmer to describe security propoerties in a clear high-level human-understandable way with certificates. Idris provides the proofs and formal reasoning and contracts. I'm also not keen on multiple languages. Is there a reason for both C++ and Java?}


\subsubsection*{Objective 2: Identification of Security and Privacy Risks in AI-Based Data Analytics}
\vspace{-6pt}

With the emergence of big data, many different technologies for both storing and processing this data have also emerged. On the storage side, most commonly used tools such as distributed filesystems and databases provide good reliability via fault tolerance and are usually optimised for fast reading. However, the security mechanisms used by them are mostly still very basic, making them a security liability. On the data processing side, machine-learning has emerged as the prominent set of methods for making sense of and analysing the vast amount of data available. Distributed machine learning algorithms, however, present a whole new set of challenges, both in terms of security and in terms of privacy. Security risks and privacy leaks can come both from training the models and from using the built models, as well as from the models themselves. In the \TheProject{} project, we will develop methods to identify known and unknown security risks coming both from the individual agents involved in the process of storing and analysing big data, and from the interaction between different agents in distributed settings. For identifying security risks, we will use i) static source code analysis; ii) dynamic \emph{symbolic execution}, where a program is executed abstractly, covering multiple possible inputs of the program that share a particular execution path through the code; and, iii) run-time machine-learning based monitoring of the application execution. We will also develop methods based on stochastic modelling for identifying and quantifying leakage of private information coming from building and applying of distributed machine learning models.

\subsubsection*{Objective 3: Self-Healing Mechanisms for Repairing Vulnerabilities and Addressing Privacy-Leakage}
While identifying security risks and privacy leaks is critical for security of distributed data analytics and knowledge sharing across parties, it is equally important to develop methods for \emph{repairing} the identified risks and leaks. These methods should be as transparent to the end user as is practically possible, not requiring them to be security experts while still allowing them to develop efficient and secure code. At the same time, the end user should be aware of the changes made to the code, allowing them to intervene and apply their domain-specific knowledge to the code transformation that aim to improve security and privacy. In the \TheProject{}, we will develop novel \emph{self-healing} techniques for semi-automatic transformation of the code to eliminate security risks and privacy leaks. Self-healing will be based on both incremental, end-user-driven \emph{code refactoring} of the source code and machine-learning based run-time adaptation mechanisms. Any potential privacy-leakage will be quantified and addressed using optimal data release mechanisms. We will also address the problem of knowledge transfer from the data owned by one party to another party while still preserving privacy of sensitive information contained in the data. 


%% We will move part of this to the background work & advancement beyond STOA
%%\subsubsection*{Objective 3: Symbolic Execution for Discovering Security Vulnerabilities}
%%\vspace{-6pt}
%\vjcomment{IBM to write this part}
%%Particular execution path through the code. The execution treats these inputs symbolically, “returning” a result that is expressed in terms of symbolic constants that represent those input values.
%%A known advantage of the symbolic execution technique is its ability to avoid giving false warnings; any error found by symbolic execution represents a real, %%feasible path through the program, and can be witnessed with a test case that illustrates the error.
%%In the \TheProject{} project, we will extend the ExpliSAT symbolic execution technology of IBM to discover known security vulnerability patterns in C/C++code. Moreover, to avoid known limitations of the symbolic execution approach (e.g. path explosion, memory growth, etc.), we will develop a technology that leverages the combination of white box fuzzing and symbolic execution to find exploitable bugs. The idea is to perform symbolic execution as the main technique to discover vulnerabilities combined with tailored fuzzing techniques in specific small areas for the purpose of assisting the symbolic execution engine to consistently make progress.

%% VJ : Also move this to the background & advancement beyond STOA
%\subsubsection*{Objective 4: Identifying and Controlling Privacy-Leakage in Distributed Machine Learning}
%\vspace{-6pt}
%The datasets containing sensitive information can't be publicly shared as a privacy-risk posed by several types of attacks exists. A novel information theoretic framework is introduced for privacy-preserving distributed machine learning such that privacy-leakage is quantified by the mutual information between sensitive data and released data. At the core of privacy-preserving framework lies a stochastic model approximating the uncertain mapping between released noise added data and private data such that the model is employed for variational approximation of informational privacy. The suggested privacy-preserving framework consists of three components: 1) Optimal Noise Adding Mechanism; 2) Modeling of Uncertain Mapping Between Released Noise Added Data and Private Data; and 3) Variational Approximation of Information Privacy. 
%There is an interest in sharing knowledge extracted from the data owned by a party with another party while simultaneously preserving the privacy of private data of both parties. An analytical framework is introduced to study and optimize the privacy-preserving transfer of knowledge extracted from a large set of labelled private data owned by a party to another party owning a few labelled data samples. An information theoretic approach is considered to quantify transferability of knwoledge from source to target domain in-terms of mutual information between source and target data. The privacy secured knowledge sharing framework facilitates development of transfer and multi-task machine learning algorithm while optimizing the privacy-transferability tradeoff.                

\subsubsection*{Objective 4: Security Coding Standards}
Source code of infrastructure software and applications is a major vector to introduce security vulnerabilities. This is specially relevant in native languages, as C or C++, where the source code is translated into a binariy excutable and there is direct access to the hardware plaform. Some of such vulnerabilities may be introduced becase of bad practices. However, there is also a category of vulnerabilities that are intrdocued due to specific features of the programming language itself.
In \TheProject{} we will develop new coding standards that can help to mitigate vulenerabilities in typical big-data applications with special attention to vulnerabilities derived from  the programming language. Then we will apply those coding standards to identify vulnerabilities that may be found in distributed data storage components and in distributed pattern based machine learning software components.

\subsubsection*{Objective 5: Intelligent User Authentication-as-a-Service}
\vspace{-6pt}

User authentication is a cornerstone of security in modern computing systems. Two important quality dimensions of an effective authentication system relate to its \textit{security} and \textit{usability aspects}. The security level determines its strength against adversary attacks, whereas usability levels are commonly determined by task execution efficiency and effectiveness. However, user authentication has become a complex and time-consuming task for any organization today due to constant cybersecurity threats and strict regulatory and new directives such as PSD2 that require strong, multi-factor authentication solutions. At the same time, end-users demand a seamless authentication experience. In the \TheProject{} project, we will develop an Authentication-as-a-Service technology allowing developers to easily integrate, deploy and manage their preferred authentication methods depending on custom requirements and policies. From the users’ point of view, the authentication technology will provide a fluid authentication experience based on state-of-the-art authentication methods such as usable, single-touch user approvals, as well as intelligent and continuous authentication based on users' interaction and eye gaze data analysis. In addition, we will develop data analytics and reporting services aiming to provide intelligent insights from user interactions and eye gaze behavior data allowing developers to deliver personalized security experience and thus increase user acceptance and trust.

\subsubsection*{Objective 6: Methodology for Development of Secure Applications}
\vspace{-6pt}

In order for the \TheProject{} technology to be usable by non-experts in security, it will be necessary to bridge the gap between the relatively low-level techniques for identification and repairing of the security and privacy vulnerabilities and verification of the security properties of the code on one side, and the high-level of abstraction at which the end-user wants to have the code and security properties presented in. It is essential to develop a structured methodology, accompanied by the associated tool chain, that will provide guidance to the end user to help them develop their code and provide trust in its security properties. In the \TheProject{}, we will develop a novel methodology for developing secure distributed AI-based data analytics applications. The cornerstone of our methodology will be \emph{software refactoring} as a method for user-guided semi-automatic structured rewriting of the end-user code, which will enable easily understandable step-by-step transformation of the initial code into its provably-secure semantically-equivalent version. We will also integrate authentication, vulnerabilities identification and properties proving techniques into tools for static analysis of the code and develop a common easily-accessible front end that will be tailored to domain experts who might not be experts in security. In this way, the programmer will be fully included in the development loop, having full control over the analysis and transformations that will be performed to make their code more secure and avoid any privacy breach. 
\cbcomment{I wonder if the refactoring is downplayed too much. If the proposal focused more on the idea of the end-user, refactoring becomes vital to that. Also the idea of refactorings to transform programs to make them more secure. Perhaps meeting the specifications/contracts that are described in Objective 1. Also, maybe the refactoring use some kind of security pattern in the rewriting, which would be a novelty. }

\subsubsection*{Objective 7: Demonstrating \TheProject{} Tools and Technologies on Real-World Use Cases}
%  from automotive, machine learning and IoT domains.}
\vspace{-6pt}
We will validate the \TheProject{} methodology for developing secure applications, together with the associated tools and technologies, on realistic use cases taken from our partners at \SOPRAshort{}, \FRQshort{} and \DEMshort{}.
%In particular, the \TheProject{} technologies will be applied to the development
%of a \emph{completely new} distributed data analytics application by XX.
Consequently, all of the technologies that will be developed over the course of the \TheProject{} project will be extensively tested in a realistic setting, enabling us to identify new issues and security risks as they arise, and guiding our road-map for future technological adoption. The use cases will demonstrate that we are able to deal with large-scale AI-based distributed data analytic in secure and privacy-preserving way, processing large volumes of data with increased trust in the safety of the operations. At the same time, by using user-guided refactoring-driven methodology, supported by a tool chain, we will reducing the cost of development, deployment and maintenance of the distributed systems.
%Thanks to our \emph{data fabrication} technology, we will be able to test parts of the system
%and a system as a whole on large volumes of synthetic, but realistic,
%data, both during development and during deployment.

\subsubsection*{Objective 8: Long-Term Uptake of \TheProject{} Technologies} 
\vspace{-7pt}
\TheProject{} aims to ensure long-term uptake of the technologies that it will develop by engaging with relevant user, developer and adopter communities, by following an Open Science approach, and by providing a road-map for the future development and exploitation of the \TheProject{} technologies. We have built in specific user community building activities, including workshops, tutorials, webinars and training sessions, that will serve to actively promote the use of the \TheProject{} technologies. Whenever feasible, our software and results will be made publicly available in open source/open data repositories. We will actively engage with potential users from the chosen domains through exhibitions, demonstrations and presentations at existing events and conferences such as XX, through the annual EU ICT conference, through exploiting our excellent high-level contacts with the XX. Innovation Centres and other relevant organisations, through contacts with national government agencies, including XX and through other relevant expert networks.

%\pagebreak
\subsection{Relation to the Work programme}

\TheProject{} directly addresses the challenge that is posted by \textbf{H2020-SU-DS-2020}. By exploiting the partners' expertise in security and distributed data analytics based on machine learning and by integrating the developed solutions into a semi-automated static analysis and refactoring-based software development methodology, we aim to \textbf{integrate state-of-the-art approaches for security and privacy management in a holistic and dynamic way} while \textbf{relying on Artificial Intelligence and automation and reducing the level of human intervention necessary}. Our techniques for identifying and repairing security and privacy vulnerabilities, addressing both storage and processing of data in big-data analytics applications will address the problem of \textbf{storage and processing of data in different interconnected places}. Integration of all the methods into a coherent technology, comprising also run-time vulnerabilities detection and run-time adaptation to address the identified problems, will allow the end users to  \textbf{constantly forecast, monitor and update the security of their ICT systems}. Collectively, detecting and repairing security and privacy risks in data storage and processing will allow \textbf{monitoring and mitigation of security risks, including those related to data and algorithms}. Our inherent focus on distributed systems, with a particular focus on privacy and security of machine learning frameworks and models, both while they are trained and while they are used, will improve \textbf{collaboration and sharing of information related to security and privacy management}. 

The specific focus of \TheProject{} is on on part \textbf{c) advanced security and privacy solutions for end users or software developers} of the proposal call. Within that, our main aim is to \textbf{develop automated tools for checking the security and privacy of data, systems, online services and applications}. By building on the well-established software engineering concept of \emph{patterns} for data processing, and by supporting the techniques that will be developed over the course of the project by semi-automated tools for analysing and transforming the user code, as well as the techniques of the secure authentication, we will \textbf{support end users or software developers (possibly including developers of AI solutions) in their efforts to select, use and create trustworthy digital services}. By evaluating the \TheProject{} techniques on real-world applications coming from aerospace, banking, healthcare and telecommunication domains, we will address \textbf{real application cases}. Refactoring-based programming methodology will contribute to \textbf{automatic code generation}. Supporting the secure patterns for distributed data processing by the techniques for the formal specification and verification of their security properties, we are contributing to creation of \textbf{trustworthy data boxes}, \textbf{certification and assurance} and \textbf{cyber insurance}.

As described in Section~\ref{sec:impact} (Expected Impacts), \TheProject{} will meet all the expected impacts that have been defined for \textbf{H2020=SU-DS-2020}, developing appropriate metrics to measure its impact on
\begin{itemize}
\item Reduced number and impact of cybersecurity incidents;
\item Efficient and low-cost implementation of the NIS Directive and General Data Protection Regulation;
\item Effective and timely co-operation and information sharing between and within organisations as well as self-recovery;
availability of comprehensive, resource-efficient, and flexible security analytics and threat intelligence, keeping pace with new vulnerabilities and threats;
\item Availability of advanced tools and services to the CERTs/CSIRTs and networks of CERTs/CSIRTs;
\item An EU industry better prepared for the threats to IoT, ICS (Industrial Control Systems), AI and other systems;
\item Self–recovering, interoperable, scalable, dynamic privacy-respecting identity management schemes.
\item Availability of better standardisation and automated assessment frameworks for secure networks and systems, allowing better-informed investment decisions related to security and privacy;
\item Availability and widespread adoption of distributed, enhanced trust management schemes including people and smart objects;
\item Availability of user-friendly and trustworthy on-line products, services and business;
better preparedness against attacks on AI-based products and systems;
\item A stronger, more innovative and more competitive EU cybersecurity industry, thus reducing dependence on technology imports;
\item A more competitive offering of secure products and services by European providers in the Digital Single Market.
\end{itemize}
\eucommentary{Indicate the work programme topic to which your proposal relates, and
 explain how your proposal addresses the specific challenge and scope
 of that topic, as set out in the work programme.}

{\color{blue}{
    Specific Challenge:
    
 In order to minimise security risks, ICT systems need to integrate state-of-the-art approaches for security and privacy management in a holistic and dynamic way. Organisations must constantly forecast, monitor and update the security of their ICT systems, relying as appropriate on Artificial Intelligence and automation, and reducing the level of human intervention necessary.

Security threats to complex ICT infrastructures, which are multi-tier and interconnected, computing architectures, can have multi-faceted and cascading effects. Addressing such threats requires organisations to collaborate and seamlessly share information related to security and privacy management.

The increasing prevalence and sophistication of the Internet of Things (IoT) and Artificial Intelligence (AI) broadens the attack surface and the risk of propagation. This calls for tools to automatically monitor and mitigate security risks, including those related to data and algorithms. Moreover, storage and processing of data in different interconnected places may increase the dependency on trusted third parties to coordinate transactions.

Advanced security and privacy management approaches include designing,
developing and testing: (i) security/privacy management systems based
on AI, including highly-automated analysis tools, and deceptive
technology and counter-evasion techniques without necessary human
involvement; (ii) AI-based static, dynamic and behaviour-based attack
detection, information-hiding, deceptive and self-healing techniques;
(iii) immersive and highly realistic, pattern-driven modelling and
simulation tools, supporting computer-aided security design and
evaluation, cybersecurity/privacy training and testing; and (iv)
real-time, dynamic, accountable and secure trust, identity and access
management in order to ensure secure and privacy-enabling
interoperability of devices and systems.

\begin{itemize}
\item[(c)]: Advanced security and privacy solutions for end users or software developers

Proposals should develop automated tools for checking the security and privacy of data, systems, online services and applications, in view to support end users or software developers (possibly including developers of AI solutions) in their efforts to select, use and create trustworthy digital services. Proposals should address real application cases and at least one of the following services: automatic code generation, code and data auditing, trustworthy data boxes, forensics, certification and assurance, cyber insurance, cyber and AI ethics, and penetration testing.

The outcome of the proposal is expected to lead to development up to Technology Readiness level (TRL) 6; please see Annex G of the General Annexes.

The Commission considers that proposals requesting a contribution from the EU of between EUR 2 and 5 million would allow this specific challenge to be addressed appropriately. Nonetheless, this does not preclude submission and selection of proposals requesting other amounts.

Type of Action: Research and Innovation Action

\item[(d)]: Distributed trust management and digital identity solutions

With particular consideration to IoT contexts, applicants should propose and test/pilot innovative approaches addressing both of the following points: (i) distributed, dynamic and automated trust management and recovery solutions; and (ii) developing novel approaches to managing the identity of persons and/or objects, including self-encryption/decryption schemes with recovery ability. Proposals should address real application cases.

The outcome of the proposal is expected to lead to development up to Technology Readiness level (TRL) 5-6; please see Annex G of the General Annexes.

The Commission considers that proposals requesting a contribution from the EU of between EUR 3 and 6 million would allow this area to be addressed appropriately. Nonetheless, this does not preclude submission and selection of proposals requesting other amounts.

Type of Action: Research and Innovation Action
\end{itemize}

}}


\subsection{Concept and Approach}

\eucommentary{Describe and explain the overall concept underpinning the project. Describe the main ideas, models or assumptions involved. Identify any trans-disciplinary considerations;}

%% Why do we need distributed data analytics?
In modern world, data is everywhere around us. With the emergence of Internet-of-Things, the amount of data available from a variety of sources is doubling less than every two years. It is estimated that in the year 2025, the amount of data collected from various sources will grow to 175 zetabytes\footnote{One zetabyte is $10^{21}$ bytes. If we were to store 175 zetabytes of data on DVDs and stack them together, this stack would be long enough to circle the Earth 222 times (\url{https://www.bernardmarr.com/default.asp?contentID=1846})}. Such a vast amount of data is crucial for making more informed decisions in many areas and improving services in many areas of industry and science. Because of this, AI-based data analytics have become one of the most prominent areas of computer science, with a wide variety of applications to the real world problems. With numerous applications in healthcare, e.g.~in training machine learning models for early detection of cancerous cells, it can be said that big data can save lives. As a consequence, a variety of systems that support complex analytics techniques on raw data have been developed, with some examples being Spark MLLib, Tensorflow and Azure AI. Many of these systems are \emph{parallel and distributed} by nature. The requirement for parallelism comes from the fact that performing data analytics on large volumes of data is usually computationally very expensive, and doing such computations in parallel on many-core systems can significantly reduce response time and deliver results much faster. The distributed aspect of data analytics comes from both distributed nature of the sources of data (where some data cannot be moved between different sites due to legal or practical reasons), and by the fact that distributed systems (such as clouds) offer much more computational power at relatively cheap price than even the high-end local highly-performance systems. 

The digitisation of modern world, together with advances in the software systems, has had one undesirable consequence - the prevalence of cybersecurity attacks. As more and more devices are connected to the internet, there are more and more potential targets for cyber attacks. Rapid development of AI algorithms and techniques allowed these attacks to become more and more sophisticated and harder to detect and to deal with. Distributed nature of many of the data analytics techniques just exacerbates this problem. In the distributed world, all of the potential security issues for individual data storage/processing agents are present, as well as many additional risks that come from interaction of these agents in the distributed world and from movement of data between different distributed nodes. The mechanisms for ensuring security and preserving privacy that are currently used in the big data analytics systems are fairly basic. Moreover, \emph{formal} treatment of security properties of the distributed data analytics code is almost completely lacking. Being able to formally guarantee that the data processing code satisfies certain security properties, in terms of not exposing security vulnerabilities and not accidentally allowing breaching of privacy of the data it works on is of paramount importance. Yet there are very few systems that can guarantee these kind of properties of the code and that can, moreover, automatically and strictly check that the end user code conforms to the security specification or one of the desired security standards. Lack of guarantees about security properties, coupled with very complex scheme of interaction between data processing and data storage agents in distributed world makes establishing security of distributed data analytics one of the most complex and overlooked issues currently. The \TheProject{} project aims to tackle this crucial issue of security of AI-driven big data analytics systems.
\textit{
{\color{blue} \YAGshort{} update Proposal} In the meantime, software production have been tremendously increasing over the last decades, reaching up to 110 billion lines of code written every year. The growing needs to bring new digital functionalities as well as the modern software development processes contribute to the production of huger and more complex applications. As a consequence, application security testing activities, such as vulnerabilities check or privacy data flaws detection, become more complex and subject to combinatory explosions.
}

The digitisation of modern world, together with advances in the software systems, has had one undesirable consequence - the prevalence of cybersecurity problems. As more and more devices are connected to the internet, there are more and more potential targets for cyber attacks. Rapid development of AI algorithms and techniques allowed these attacks to become more and more sophisticated and harder to detect and to deal with. Distributed nature of many of the data analytics techniques just exacerbates this problem. In the distributed world, all of the potential security issues for individual data storage/processing agents are present, as well as many additional risks that come from interaction of these agents in the distributed world and from movement of data between different distributed nodes. The mechanisms for ensuring security and preserving privacy that are currently used in the big data analytics systems are fairly basic. Moreover, \emph{formal} treatment of security properties of the code is completely lacking. Being able to formally guarantee that the data processing code satisfies certain security properties, in terms of not exposing security vulnerabilities and not accidentally allowing breaching of privacy of the data it works on is of paramount importance. Yet there are very few systems that can guarantee these kind of properties of the code and that can, moreover, automatically and strictly check that the end user code conforms to the security specification or one of the desired security standards. Lack of guarantees about security properties, coupled with very complex scheme of interaction betweeen data processing and data storage agents in distributed world makes establishing security of distributed data analytics one of the most complex and overlooked issues currently. The \TheProject{} project aims to tackle this crucial issue of security of AI-driven big data analytics systems.



%\vspace{-6pt}

\subsubsection{Challenges for Security and Privacy}
% massively-parallel heterogeneous systems}

Ensuring security of distributed AI-driven big data analytics presents a number of challenges:

\begin{itemize}
\item \textbf{Security of Big Data Storage Systems.} Large volumes of raw data on which data analytics is performed are usually stored in distributed NoSQL databases (such as Cassandra) or simply as flat files in some form of distributed file system (such as Hadoop's HDFS). Primary concerns in the systems for data storage are robustness, scalability, fault tolerance and the speed of access for reading. Because they are rarely designed with security in mind, security mechanisms that distributed databases and filesystems use are very rudimentary. In addition to the problem of protecting the data stored in the database, distirbuted data analytics presents additional security issues which occur when the data is on the move between different sites. \textbf{We must ensure both safety of storage of data on local nodes in a distributed system and that the movement of data between nodes does not expose potential security and/or privacy vulnerabilities that can be exploited in cyber attacks.}

\item \textbf{Enforcement of Secure Coding.} Software components are continuously developed both at the infrastructure and application levels. During the development process introduction of unsecure code is a major hazard that may compromise the full system security. In this context coding guidelines play a key role to avoid the introduction of undesired vulnerabilities. Many of such vulnerabilities, specially in native languages as C or C++, are derived from the language definition itself. Moreover, programming languages are not static entities. For example, the C programming language has had three version ins the past 12 years, while the C++ programming language has had four versions during the last 9 years. \textbf{We must enforce that new and existing code adheres to coding standards that avoid the introduction of new vulenarabilties derived from programming languages features}. 

\item \textbf{Vulnerability Detection in Source Code.} The importance of cyber security has become more and more signiﬁcant. The root cause of most cyber-attacks is vulnerabilities in applications source code. Vulnerabilities exploited by attackers compromise the conﬁdentiality, integrity, and availability of information systems. Early detection of vulnerabilities is an eﬀective way to reduce the loss and potential damage. Despite the eﬀorts of experts, vulnerabilities remain a huge problem and will continue to exist in the long term. This can be justiﬁed by the fact that an increasing number of vulnerabilities are published every year. Vulnerability detection is a method to discover vulnerabilities in software. \textbf{We must ensure that new and existing C/C++ code is free as much as possible from known vulnerabilities.}

\item \textbf{Security of Pattern-Based Distributed Parallel Processing.} In order to analyse large volumes of data and extract useful information and knowledge from it, it is essential to use parallel processing. This is not only in order to get the results in reasonable time. Requirement for parallel processing comes from the way in which big data is stored. The space requirements for large data sets usually surpass the storage capabilities of individual machines and, therefore, this data has to be distributed. Furthermore, bringing all data to one place for analysis is usually prohibitively expensive, even when it is theoretically possible. Therefore, it is standard to have different processing agents on different distributed nodes which work together in analysing large data sets. Parallel patterns, such as Google's MapReduce that is used in Hadoop, provide high-level framework for data analytics where parallelism is provided for free. However, for the same reasons as with data storage, these systems potentially expose many additional security risks, coming from their interaction over networks as well as from the individual processing agents. \textbf{We must ensure that distributed parallel processing used in AI-based data analytics does not leak sensitive information from the data it processes to the malicious users.}

\item \textbf{Optimization of Privacy-Preserving Data Release Mechanism.} A data release mechanism aims to provide useful data available while simultaneously limiting any reveled sensitive or private information. Different methods such as k-anonymity, l-diversity, t-closeness, and differential privacy have been developed to address the privacy issue. Differential privacy is a formal framework to quantify the degree to which the privacy for each individual in the dataset is preserved while releasing the output of a data analysis algorithm. Differential privacy guarantees that an adversary, by virtue of presence or absence of an individual's data in the dataset, cannot draw any conclusions about an individual from the released output of the analysis algorithm. Differential privacy, however, does not always adequately limit inference about participation of a single record in the database. Differential privacy requirement does not necessarily constrain the information leakage from a data set. Correlation among records of a dataset would degrade the expected privacy guarantees of differential privacy mechanism. These limitations of differential privacy motivate an information theoretic approach to privacy where privacy is quantified by the mutual information between sensitive information and the released data. The data perturbation approach uses a random noise adding mechanism to preserve privacy, however, results in distortion of useful data and thus utility of any subsequent machine learning and data analytics algorithm is adversely affected. There remains the challenge of studying and optimizing privacy-utility tradeoff especially in the case when statistical distributions of data are unknown. 

\item {\textbf{\YAGshort{} Proposal: Optimization of vulnerability detection in the source code with static analysis.}} Detection of vulnerabilities in the source code of applications is achieved though static analysis. This approach generates a huge amount of information to the user. On one hand, a single vulnerability can be detected in different sequences of code, each of which will generate a warning to the user and result in duplicate warnings. On the other hand, when there is a doubt that a code sequence is suspicious a warning will also be raised to the user, and possibly generate a false positive. A manual step is then needed to analyse the local context of each potential vulnerability and qualify the warnings before they can be used to fix real security issues. When it comes to self healing and big data analytics applications, this manual phase must be automated and additional information is required from static analysis to feed the self healing process with relevant qualified decision making information.
\textbf{We must ensure that static code analysis provides relevant and qualified decision making information to optimize self healing and vulnerability detection.} 

\item \textbf{Intelligent User Authentication.} 


\end{itemize}


The challenges identified above require a new and radical
approach that tackles these issues in a coherent and
holistic way. 

\subsubsection{Achieving The \TheProject{} Vision}

\begin{figure}[tp]
  \begin{center}
  \vspace{-5mm}
  \includeimage[scale=0.55]{DigitalFortress-ConceptualDesign-v3.png}
%  \vspace{-3cm}
  \caption{Overview of the \TheProject{} Methodology}
  \label{fig:overview}
  \end{center}
  \end{figure}

Figure~\ref{fig:overview} gives a high-level vision of the \TheProject{} methodology for developing secure and privacy-preserving applications that use distributed AI-based data analytics. Our assumption is that the end user starts with their code that is based on data analytics, but for which safety properties have not been firmly established. The users will be presented with the \TheProject{} tool-chain, supported by the visual frontend developed by \USTANshort{} and \YAGshort{}. The code development will proceed in a number of phases.

\begin{itemize}
\item \emph{Vulnerabilities detection phase}, where the tool-chain will identify potential vulnerabilities in the code, both in terms of potential targets for cyber attacks (security) and in terms of potential leakage of information (privacy). The identification will combine static code analysis that will be developed by \YAGshort{} and \UCMshort{}, dynamic symbolic code execution mechanisms that will be developed by \IBMshort{} and machine-learning based run-time monitoring that will be developed by \SCCHshort{}. Combining these three complementary methods will allow us to discover a wide range of potential vulnerabilities in the code, as well as to avoid the common problems when each of them is used alone (such false positives, time complexity and vulnerabilities that arise at run-time). The general techniques for detecting vulnerabilities will be adapted to the specific problems of data storage and processing in distributed data analytics by \UODshort{} and \SCCHshort{}. 

\item \emph{Self-healing phase}, where the identified vulnerabilities will be repaired using a combination of semi-automatic code refactoring techniques that will be developed by \SAshort{} and run-time adaptation based on run-time monitoring that will be developed by \DEMshort{} and \SCCHshort{}. Our aim here is to provide both source-to-source code transformations, the output of which will be the repaired code that is semantically equivalent to the original code, but with the identified (static) vulnerabilities patched and also run-time patching that will, over a series of steps, restore the system into a stable and secure state where cyberattacks are happening at run-time (e.g. man-in-the-middle attacks). In the process of self-healing, we will also apply novel methods and techniques for optimising the privacy-preserving data release mechanisms to address the issue of privacy-leakage. The developed core methods will be adapted to the problem of AI-based distributed data analytics by \DEMshort{} and \UODshort{}. 

\item \emph{Formal verification phase}, where we will use formal methods on the resulting code to verify that the required security properties of the code are satisfied. We will also use static code analysis to verify that the code conforms to the required security standards. We will build on the existing security standards such as XX and extend them with new rules, as applicable to the problem of distributed data analytics. The formal verification models will be built by \SCCHshort{} and \SAshort{}, while the analysis for the conformance to standards will be developed by \UCMshort{} and \YAGshort{}. The output of this phase will be the final secure code, together with some form of a \emph{digital certificate} that will specify what standards the code conforms to and the security properties that it satisfies. Depending on the results of this phase, the code might need to be fed back to the \emph{self-healing phase}, if further problems with conformance to standards and/or security properties have been identified.
\end{itemize}

In addition to security vulnerabilities, the goal of the \TheProject{} is to address the \emph{privacy} concerns too. Distributed AI-based data analytics, with multiple processing agents fetching data from multiple storage sites and exchanging data between themselves, can easily lead to leakage of confidential data. This is especially important in light of new EU regulations on protection and ownership of private data, such as General Data Protection Regulation~\cite{gdpr} (GDPR). Our mechanisms for identifying and repairing vulnerabilities will ensure that the end user code conforms to the parts of the GDPR regulation that are relevant to data movement between sites (e.g. articles 20, 25, 33 and 37). 

Finally, to add the additional layer of security, we will specifically address the problem of user authentication, by both developing and integrating the novel secure authentication mechanisms and by providing diagnostics and repairing of the authentication mechanisms that are already a part of distributed data analytics. Both these part will be led by \COGNIshort{}. All of the developed techniques will be demonstrated on the identified use cases in the areas of air traffic management (\FRQshort{}), network analysis (\DEMshort{}), open banking (\SOPRAshort{}), space technologies (\SOPRAshort{}) and healthcare (\SOPRAshort{}). 


\begin{figure}[tp]
  \begin{center}
  \vspace{-5mm}
  \includeimage[scale=0.5]{DigitalFortress-WP2.pdf}
  \vspace{-1.5cm}
  \caption{Data Storage and Processing in \TheProject{}}
  \label{fig:storageprocessing}
  \end{center}
  \end{figure}

Figure~\ref{fig:storageprocessing} shows a more detailed view of the different software layers that a typical big data analytics applications is built on. Each of these layers presents its own potential security issues that have to be considered in the \TheProject{} project. At the lowest level, the data that needs to be analysed (as well as the machine learning models that are built) is usually stored in distributed data bases or filesystems that possibly span multiple sites and organisations. Each of the individual nodes that stores the data will have a local database/filesystem manager process through which the communication with other nodes commences. This will mostly be invisible to the actual end user application, but it has to be addressed as a source of potential security vulnerabilities. The lowest level accessible to the application is the level of language drivers and APIs to the distributed database/filesystem. Through the calls to this API, the application communicates with different nodes of the distributed database/filesystem. The examples of this layer are Hadoop HDFS~\cite{hdfs} distributed filesystem bindings for Java/C++ and Cassandra~\cite{cassandra} API. On top of this layer, in a typical data analytics application there will be a layer of parallel patterns for distributed computations. Parallel patterns are usually implemented as high-order functions that implement common parallel behaviour, with the most well-known example being Google MapReduce~\cite{mapreduce}. Parallel patterns allow the end users to specify just the problem-specific parts of their computation, while they get the benefits of its parallel execution (and, hence, faster response times) for free. Unfortunately, parallel patterns are another possible source of vulnerabilities, due to the fact that they involve intensive communication and exchange of data between different distributed nodes. On top of the parallel patterns, there are domain-specific machine learning libraries, that implement specific machine-learning techniques in terms of instances of parallel patterns. The examples of these libraries are Keras~\cite{keras} and XX. Finally, the end user application uses these machine-learning libraries to conduct specific data analysis that is needed. \TheProject{} aims to investigate sources of vulnerabilities and the transformations required to mitigate them in each of these different layers.

\subsubsection*{Key assumptions}

\vjcomment{This may not be necessary.}

\TheProject{} makes a number of fundamental assumptions that will be tested and verified in the course of the project,
and which form the basis for a register of technical risk.  The most significant assumptions are:

\begin{enumerate}[{A}1)]
\item XX
\end{enumerate}

\subsubsection*{Transdisciplinary concerns}
 \vjcomment{Only if relevant}


\subsubsection{Positioning of the project}
\eucommentary{Describe the positioning of the project e.g. where it is situated in the spectrum from 'idea to application', or from 'lab to market'. Refer to Technology Readiness Levels where relevant.}

In line with the expectations of the XX call, \TheProject{} aims
to achieve overall Technology Readiness Level (TRL) 5-6 (``technology
validated in relevant environment (industrially relevant environment in
the case of key enabling technologies)'').

\begin{center}
  \begin{tabular}{|p{4.9in}|l|l|}
    \hline
    \textbf{Key Enabling Technology} & \textbf{Current TRL} & \textbf{Final TRL} \\
    \hline
     &  & \\
    \hline Refactoring Tool Support & TRL x & TRL x \\  
    \hline YAG YAG-Suite vulnerability detection tool - Machine learning augmented SAST for security and privacy breaches in Java, C/C++ and Python source code & TRL 4 & TRL 6 \\  
    \hline YAG YAG-Suite vulnerability detection tool - Code mining (extraction of technical, security and end user business orientedhigh value information from source code) & TRL 5 & TRL 7 \\  
    \hline SCCH eKnows software analytics tool - Module for semi-automated extraction of ASMs from source code & TRL 4 & TRL 6 \\
    \hline IBM ExpliSAT Symbolic Interpretation Tool for security vulnerability detection in C/C++ code & TRL 4 & TRL 6 \\
    \hline
  \end{tabular}
\end{center}

\noindent
The specific advances that will be made are described in more detail in Section~\ref{sec:novelty} (page~\pageref{sec:novelty}).


\subsubsection{Linked research and innovation activities}
\label{projects}

\eucommentary{Describe any national or international research and innovation activities which will be linked with the project, especially where the outputs from these will feed into the project;}


%\vspace{-8pt}
\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\teamplay (ICT-779882).}
The TeamPlay project aims to develop new, formally-motivated, techniques that will allow execution time, energy usage, security, and other important non-functional properties of parallel software to be treated effectively, and as first-class citizens. This methodology was built into a toolbox for developing highly parallel software for low-energy systems, as required by the internet of things, cyber-physical systems etc.

The TeamPlay approach allows programs to reflect directly on their own time, energy consumption, security, etc., as well as enabling the developer to reason about both the functional and the non-functional properties of their software at the source code level.
\end{mdframed}

%...
%\vspace{-8pt}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\rephrase (ICT-644235).}
%\vspace{-12pt}

The \rephrase project aims to study the software engineering process as
a whole for heterogeneous parallel machines
using C++.  It considers neglected, but important, issues such as
effective testing, debugging, maintenance and quality assurance for
multicore/manycore machines, and involves major industry players such as IBM.
\emph{\TheProject will, however, substantially extend the work that has been done in \rephrase by:
\begin{inparaenum}[i)]
\item XX
\end{inparaenum}}
\end{mdframed}

%\vspace{-2pt}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\paraphrase (ICT-288570).}
%\vspace{-12pt}
% \TOWRITE{CB,VJ}{Write about ParaPhrase} 
The \paraphrase project introduced a new structured design and implementation process for
heterogeneous multicore/manycore architectures, in which developers exploit a variety of
parallel patterns to develop component based applications in Erlang and C++. These
component based pattern-applications may then be re-mapped to meet the
application requirements and hardware availability. 
\end{mdframed}

%\vspace{-2pt}
\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\grow (EU No.690199).}
The \grow project (funded from the European Union’s Horizon 2020 research and innovation programme under grant agreement No.690199) is the first continental-scale Citizens’ Observatory to monitor a key parameter for science, continuously over an extended period, and at an unmatched spatial density. \grow has, for the first time, used crowdsourced ground observations from low-cost sensors to validate soil moisture information from satellites, including the new generation of high-resolution satellites, Sentinel-1.

24 \grow communities in 13 European countries create an unprecedented network of 6,502 ground-based soil sensors and a dataset of 516M rows of soil data. 
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\weobserve (EU No.776740).}
\weobserve is an umbrella project promoting the use of Citizen Observatories for environmental monitoring.  This innovative project aims at demonstrating the societal and economic benefits of involving citizens in environmental decision making and cooperative planning, supporting Europe’s leading role in integrating citizen science and building resilient communities. Together, these projects will empower and enable citizens to become the ‘eyes’ of the policy makers and to complement existing environmental monitoring systems.
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\satie (EU No.832969).}
\satie adopts a holistic approach about threat prevention, detection, response and mitigation in the airports, while guaranteeing the protection of critical systems, sensitive data and passengers. Critical assets are usually protected against individual physical or cyber threats, but not against complex scenarios combining both categories of threats. In order to handle it, SATIE develops an interoperable toolkit which improves cyber-physical correlations, forensics investigations and dynamic impact assessment at airports. Having a shared situational awareness, security practitioners and airport managers collaborate more efficiently to the crisis resolution. Emergency procedures can be triggered simultaneously through an alerting system in order to reschedule airside/landside operations, notify first responders, cybersecurity and maintenance teams towards a fast recovery.
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\blogdas (FWF: [P26452-N15]).} 
The project Behavioural Theory and Logics for Distributed Adaptive Systems (\blogdas) was supported by the Austrian Science Fund. The first key result of the project is a behavioural theory of such distributed adaptive systems. The theory comprises (1) a general language-independent characterisation by a set of intuitive postulates that are satisfied by all known system formalisms, (2) an abstract machine model that provably satisfies the postulates, and (3) the mathematical proof that all systems stipulated by the postulates can be faithfully represented by the abstract machine model. The behavioural theory enables system specifications in general using a concrete language for the abstract machine model. The second key result is a logic for such distributed adaptive systems that is based on the abstract machine model. The logic allows system developers to formally characterise desirable properties that system specifications must meet and to mathematically verify these properties. In particular, static verification with this logic permits to give assertions for states that result after many adaptations of the system.
The results of this project overcome the imminent restriction of using interleaving to circumvent true asynchronous behaviour, providing better suited theoretical foundations and tools (concurrent ASMs and logics) to formalise and verify contracts for distributed systems as required for WP4 as well as to specify anomaly detection algorithms and associated repairability proof obligations within the context of WP2.  
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\primal (FFG: 873979).}
The \primal project, supported by the Austrian Research Promotion Agency, is concerned with the development of privacy-preserving machine learning industrial applications. The project focuses on the development of algorithms and a software framework for differentially private distributed deep learning, transfer learning, and multi-task learning with applications to industrial use cases. While the results of \primal regarding differential privacy in distributed machine learning has a synergy with the privacy theme of \TheProject{}, information theoretic approach to study and optimize privacy-leakage in distributed AI is one of the novelties of \TheProject{}.
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\sthreeai (FFG: 872172).} The Austrian Research Promotion Agency supported \sthreeai project for building secure collaborative artificial intelligence systems ensuring privacy, protection against hostile attacks and guaranteeing for the intended performance of the system. \sthreeai will develop theoretical frameworks and analysis tools at the interface of mathematics, deep learning and information security regarding new deep model architectures and related privacy learning strategies, new defence strategies against enemy attacks, and new methods for assessing trustworthiness. \sthreeai has synergy with \TheProject{} regarding privacy-preserving deep learning, however, unlike \TheProject{}, \sthreeai does not consider an information theoretic approach to study the concepts of privacy-leakage and knowledge-transferrability across distributed parties in a rigorous and unified manner.    
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{\serums (EU 826278).} \serums is the EU H2020 funded project that is coordinated by \USTANshort{} and also includes \SOPRAshort{}, \IBMshort{} and \SCCHshort{} in the consortium. \serums aims to develop new smart, patient-centric health-care systems that integrate home, workplace etc.~personal medical care with centralised hospital, special consultant and general practitioner provision. The goal of the \serums project is to put patients at the centre of future healthcare provision, enhancing their personal care and maximising the quality of treatment they can receive, while ensuring trust in the security and privacy of their confidential medical data. Like \TheProject{}, \serums also targets distributed systems where confidential data might be exchanged between different devices over untrusted networks. The focus of \serums is, however, on privacy of the data in the healthcare domain. The data analytics aspect is not very prominent there and it does not consider cloud environments. \emph{In contrast, \TheProject{} considers much larger scale of hardware, is focused on data analytics executed in cloud settings and also address the crucial problem of security in terms of cyber threats and attacks. \TheProject{} also considers a wider range of application domains, having use cases in banking, space technology, air traffic management and networking domains.}
\end{mdframed}


\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{YAG-Suite (\YAGshort{} R and D) } The YAG-Suite a is product developed by \YAGshort{} and resulting from the company's research and innovation project. The project started from scratch and developed a new source code scanning technology which detects and identify vulnerabilities in the source code with SAST and bring new automation capabilities to qualify the warnings raising from SAST with machine learning, in a supervised training approach. The machine learning allows to better contextualize the vulnerability detection as well as it significantly reduces the false positive rates and provide decision making information to the user for cost efficient remediation.    
The company has been labelled "Jeune Entreprise Innovante" (Young Innovative Company). The project has been partially subsidised by BPI France for its feasibility phase, as well as from the Region of Brittany and Rennes Métropole. The project also received partial support with industry loans from TOTAL Développement Régional and AIRBUS Développement. 
\end{mdframed}

\subsubsection{Overall approach and methodology}

\eucommentary{Describe and explain the overall approach and methodology, distinguishing, as appropriate, activities indicated in the relevant section of the work programme, e.g. for research, demonstration, piloting, first market replication, etc.;}

\TheProject{} comprises 6 technical work packages: WP2 on foundational mechanisms for identifying security risks and self-healing; WP3 on security and privacy in the context of distributed AI-based big data analytics; WP4 on establishing security properties of the code, including formal verification and analysis for compliance to standards;  WP5 on intelligent user authentication; WP6 on methodology for developing secure applications by end users; and WP7 on use cases and evaluation. There are also WP8 on dissemination, exploitation and community building and WP1 on project management. The relationship between the project objectives and these work packages is shown below

\vspace{-8pt}
\begin{center}
\begin{tabular}{|l|l|l|}\hline
\textbf{Objective} & \textbf{Purpose} & \textbf{Contributing WPs} \\\hline \hline
Objective 1 & Secure Patterns for Distributed AI-based Data Analytics & \textbf{WP3}, \textbf{WP4}, \textbf{WP6} \\\hline
Objective 2 & Identifying Vulnerabilities and Privacy Leakage & \textbf{WP2}, \textbf{WP3} \\\hline
Objective 3 & Self-Healing Techniques for Repairing Vulnerabilities and Leakage & \textbf{WP2}, \textbf{WP3} \\\hline
Objective 4 & Coding Standards for Developing Secure Distributed Applications & \textbf{WP4}, \textbf{WP6} \\\hline
Objective 5 & Intelligent User Authentication-as-a-Service & \textbf{WP5} \\\hline
Objective 6 & Refactoring-Based Methodology for Secure Applications & \textbf{WP3}, \textbf{WP4}, \textbf{WP5}, \textbf{WP6} \\\hline
Objective 7 & Demonstrating the Tools and Techniques on Real-World Use Cases & \textbf{WP6}, \textbf{WP7}\\\hline
Objective 8 & Building a Sustainable User Community & \textbf{WP8}\\\hline
\end{tabular}
\end{center}

\paragraph*{Work Programme for Objective 1.}

Objective 1 aims to build secure patterns for distributed AI-based data analytics and to develop methods for formal specification and verification of security properties of those patterns. In WP3, we will develop methods for identifying and repairing security and privacy vulnerabilities for distributed AI-based data analytics (based on foundational techniques that will be developed in WP2). This will serve as a basis for the development of secure patterns that will be done in WP6 as a part of the \TheProject{} methodology for developing secure end-user applications. In WP5 we will develop the methods for formal specification and verification of security and privacy properties of pieces of C++ code, in form of \emph{security certificates} that will specify these properties at a high level and also \emph{security contracts} that will be machine readable and, therefore, amenable for formal proving. 

\paragraph*{Work Programme for Objective 2.}

Objective 2 aims to develop techniques for identifying security and privacy vulnerabilities in the C++ and Java code. In WP2, we will develop fundamental techniques for identifying security vulnerabilities, based on source code analysis, dynamic symbolic execution and run-time monitoring. In WP3, we will apply these techniques to identifying security vulnerabilities in distributed data storage frameworks (distributed databases and filesystems) and in machine-learning based distributed data analytics applications based on parallel pattern-based data processing frameworks. WP3 will also investigate quantifying potential privacy leakage in machine-learning applications and in exchange of knowledge built using these models. The developed diagnostics for identifying vulnerabilities and data leakages will be integrated into the overall \TheProject{} methodology in WP6.

\paragraph{Work Programme for Objective 3.}

Objective 3 aims to develop self-healing mechanisms for repairing security vulnerabilities and privacy leakage. In WP4, we will develop a foundational code transformations for self-healing. These transformations will be based on the identified vulnerabilities, as well as on the known secure code patterns for distributed data analytics, as developed in WP2. In WP3, we will develop techniques for applying these transformations both to the end-user code that uses distributed databases for data storage and that uses common pattern-based data analytics frameworks such as XX. WP3 will also develop techniques for controlling data leakage using optimal noise-adding mechanisms. In WP6, the transformations will be integrated into the software-refactoring based methodology for deveoping secure pattern-based distributed data analytics applications. 

\paragraph{Work Programme for Objective 4.}

Objective 4 aims to extend the existing code standards for developing secure and privacy-preserving distributed applications. In WP4, we will build static analysis techniques that will check the compliance of the C++ code to the security standards such as CERT SEI C++ Coding Standard, but we will also consider other coding standards such as CppCore Guidelines and High Integrity C++. Based on the methodology for developing secure applications that will be developed in WP6, and in particular based on the secure patterns for composing distributed big-data analytics applications from different layers of abstraction, we will also aim to augment these standards extended them with new coding rules as identified over the course of the project. The outcome of this (in WP6) will be the extended security standards and guidelines for developing secure, large-scale data applications that will be developed in both private and public clouds.

\paragraph{Work Programme for Objective 5.}

Objective 5 aims to analyse, design, develop and evaluate an intelligent and user-centered authentication-as-a-service which will allow digital service providers to integrate, deploy and manage multi-factor authentication (MFA) and continuous user authentication by reducing the complexity of development. In WP6, a User-Centered Design (UCD) methodology will be followed for the design and development of the authentication technology aiming to assure that the solution meets the users' needs and expectations in terms of usability and security. The development life-cycle of the authentication component will include multiple design iterations and a significant amount of evaluation, which will be largely based on studies with the active participation of real end-users. These studies will be both formative (during the project’s course, with the aim of validating/refining the initial design) and summative (towards the end of the project, aiming to measure the quality and effectiveness of the final project results). The UCD approach will embrace three iterative cycles leading to three different system prototype releases (low-fidelity, high-fidelity, final release). Each iteration entails an analysis, design, implementation and integration, ending with an evaluation of derived prototypes providing thus valuable feedback for designing the next release. 

\paragraph{Work Programme for Objective 6.} 

Objective 6 is to tie up all the different techniques developed in other parts of the project into a coherent and tool-supported methodology for repairing the existing and developing new secure applications. In WP6 we will develop both the methodology, which will outline all the steps in the software engineering process of building secure large-scale distributed applications, and we will integrate the tools developed in WP2--WP5 into an user interface, allowing seamless access to the technologies for identification and repairing of security and privacy vulnerabilities, checking for compliance to the security standards, formal verification of the security properties, security contracts and user-centered authentication. The tool-chain will be based on our existing tools for code analysis (provided by \YAGshort{}) and software refactoring (provided by \SAshort{}), which will allow semi-automatic restructuring of the end-user code while keeping the end user in the loop and allowing him input in the transformation and verification process, where they could supply their domain-specific knowledge to drive the process. Finally, in WP6 we will also apply the data fabrication technology from \IBMshort{} to allow rapid generation of large volumes of synthetic data to stress-test the large-scale data analytics systems, helping in both the development of the technologies and methodology and in testing the end-user applications.

\paragraph{Work Programme for Objective 7.} 

Objective 7 is to demonstrate effectiveness of the \TheProject{} approach on the real-world use cases from the air traffic management, space telecommunications, banking and medical domains. In WP6, we will develop a tool-chain that will integrate all different techniques developed in WP2 -- WP5. This will be used in WP7 to evaluate the \TheProject{} technology  on use cases from \SOPRAshort{}, \FRQshort{} and \DEMshort{}. We will apply our techniques both on analysing/repairing the existing and on developing completely new distributed code from the chosen domains. While our ultimate goal is to test the techniques in realistic setting, \emph{data fabrication} methods that will be developed as a part of the methodology will allow us to immediately produce arbitrary amount of realistic data for rapid development and testing of our techniques. We will demonstrate increased security of the end-user software while at the same time decreasing cost of the production, deployment and maintenance of such a system.

\paragraph{Work Programme for Objective 8.}

Objective 8 is to ensure long-term uptake and use of \TheProject{} concepts and technologies. In WP8, we will undertake a range of dissemination and communication activities, that will include producing publications, documents and other materials, giving presentations and demonstrations, running dedicated workshops and tutorials to encourage uptake of \TheProject{} technologies, establishing and maintaining a communication strategy, producing of user-level documentation and training material, including tutorials, webinars etc., production and distribution of promotional and informational materials, including press releases, flyers etc., and updating and managing the web portal with user-related material.  In addition a MOOC will be created about the work of the project and will be hosted by UOD partner Futurelearn, this has the potential of reaching 10s of thousands of users .  Uptake and use will be assisted by the inclusion of the highly-relevant use cases from the air traffic management, telecommunications, banking and medical domains.


\subsection{Ambition}

\eucommentary{Describe the advance your proposal would provide beyond the state-of-the-art, and the extent the proposed work is ambitious. Your answer could refer to the ground-breaking nature of the objectives, concepts involved, issues and problems to be addressed, and approaches and methods to be used.}

\subsubsection{Advances Beyond the State-of-the-Art}
\label{sec:novelty}
%\label{sect:background}
%\label{sect:state-of-the-art}

%\eucommentary{}


\TOWRITE{ALL}{Add sections on your own technologies/update
what's here.}

The \TheProject{} project will revolutionise the development of secure and privacy-preserving big-data applications that are deployed on clouds. We will significantly advance the state of the art in different areas related to security of distributed applications, including identification of security vulnerabilities in large-scale distributed settings, semi-automatic repairing of vulnerabilities, privacy-preserving machine-learning, security of data storage and data processing frameworks, pattern-based design of secure applications, certification and assurance and authentication. Collectively, the advances that are made by the \TheProject{} will:
\begin{itemize}
\item \textbf{reduce the number and impact of cybersecurity incidents} through our advanced security and privacy vulnerabilities detection and semi-automatic repairing, as well as novel user-centered authentication schemes;
\item \textbf{increase trust} in the security of big-data analytics applications by providing formal guarantees of the security properties of the code and certification for compliance to the security standards;
\item \textbf{ensure privacy and allow safe knowledge exchange} in AI-based data analytics by quantifying and reducing privacy leaks and allowing safe transfer of learning;
\item \textbf{reduce the effort} required for securing applications and services by developing a methodology and tool-chain to support the end user and guide them through the process of developing secure applications;

\end{itemize}

\subsubsection{Identifying Security Vulnerabilities in Applications}
\label{sect:background-first}
\label{sect:identifying}

\paragraph{Static Code Analysis.}
\vjcomment{\YAGshort{} and \IBMshort{} to write}
In the secure development life cycle, different complementary techniques make it possible to build and test the cyber security of software applications, from the writing of the first line of code during the development project to the stress of the application in production with ethical hacking. In order to minimize the overall costs of security testing, the first action to be implemented as early as possible in the development process is to analyze the source code on a regular basis to verify that the algorithms, the management of sensitive data and resources by the application will not generate vulnerabilities. The only technique which can produce results when the project is not yet ready for compiling is SAST (Static Application Security Testing). Source code scanners, such as Appscan Source~\cite{AppScan}, Checkmarx Source~\cite{Checkmarx} and Coverity~\cite{Coverity}, rely on SAST to detect suspicious sequences of code that may reveal a vulnerability. 

State of the art source code scanners all face the same dilemna: from the very huge volume of analyzed source code they must ideally warn the user on each and every potential vulnerabilities but only when a relevant vulnerability is detected. In reality vulnerabilities are not so easy to find, because of the combinatory and complexity of vulnerability detection, and scanners will prefer to raise potentially irrelevant warnings (false positive) rather than missing real vulnerabilities (true negatives). As a result, state of the art scanners generate a lot of false positives. It's getting even more difficult when a single vulnerability may show up in several parts of the code, thus generating duplicate warnings. 
To be usefully exploited, users need to carry out additional manual investigations which prove to be heavily time consuming. The current economical efficiency of static analysis is therefore limited and the induced HR costs can be very significant if all warnings are checked for their relevance.


\paragraph{Dynamic Analysis and Symbolic Execution.}
\vjcomment{\IBMshort{} to write}
Software vulnerabilities pose serious risk of exploit and result in system compromise, information leaks, or denial of service. As mentioned in the previous section, various static analyzers are used to detect potential vulnerabilities in C/C++ code. Most of them suffer from high amount of false alarms mainly due to the fact that they don’t use a symbolic execution approach but rather a more simplistic syntactical and semantical analysis of code constructs. This simplistic approach has the advantage of being more scalable as opposed to the inherent scalability deficiencies of symbolic execution which exhaustively analyzes all potential execution paths given non-deterministic inputs. Furthermore the fact that real life programs call out to system functions and other functions residing in libraries, with no access to their source code, creates the need to generate abstractions or simulate their execution flows depending on the nature of the function. This entails a wide variety of challenges ranging from the mitigation of state explosion to simply being able to accurately simulate a large number operations. There are a number of symbolic execution tools such as JPF for Java and Otter for C, both with limited capabilities to scale up to real life programs.

Dynamic analysis systems, such as “fuzzers” natively executes an application while providing ‘tailored’ inputs. When flaws are detected, these systems can provide these inputs and significantly assist in reproducing and mitigating the flaws. However, generating the tailored “input test cases” to drive execution, typically requires an exhaustive set of test cases that required manual effort to generate. 

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of software vulnerability detection by combining symbolic execution with fuzzing and advanced static analysis capabilities. We will use a known analysis technique that is based on reachability analysis, and discard paths that cannot reach a target location. Then we plan to use static analysis augmented with heuristics to detect and abstract code segments that are deemed difficult for a symbolic execution and/or fuzzing to cope with, such as cryptographic code. A combination of symbolic execution and fuzzing approaches will help the symbolic execution engine to make progress in code segments that are difficult for symbolic interpretation due to the “path explosion” problem or missing source code. The overall goal is to create a combined tool with symbolic execution, fuzzing and static analysis capabilities that would outperform any other existing tool in the marketplace.

\end{mdframed}


\paragraph{Run-time Analysis and Adaptation}
%\vjcomment{\SCCHshort{} to write}
%Current practice is for security to be ensured at run-time, using efficient monitoring techniques coupled with dynamic run-time checks such as ISR~\cite{isr}, ASLR~\cite{aslr} and CFI~\cite{cfi}. These methods are, however, expensive in terms of computational complexity and energy consumption. 
According to the general behavioural theory of concurrent systems~\cite{BorgerS16} concurrency can be captured by families of sequential ASMs indexed by agents, such that each agent proceeds stepwise evaluating its actions in a consistent virtual state and bringing in its updates into some later (not necessarily the next) virtual state. Extending the thesis such that families of parallel ASMs~\cite{FerrarottiSTW16}  are captured is rather straightforward. Reflective (self-adaptive) extensions~\cite{abs-2001-01873} should not cause serious problems to be integrated, as the concurrency model does only depend on integrating results of steps of the involved agents. This provides the necessary theoretical foundations  needed to formally specify the novel algorithms that we envisage for run-time analysis and adaptation based defence of modern distributed data analytics systems. Among others we can capture within this model the semantics of bulk synchronous parallel computations~\cite{FerrarottiGS19}, and thus the MapReduce algorithms which are prevalent in data analytics. 


The common state-of-the-art approach for security analysis (also at runtime) is to start from an attacker model and system vulnerabilities~\cite{Biskup09}. With respect to possible attacks at run-time the project will focus on unauthorised intrusion with the potential of causing (planned or carelessly) damage to the system, i.e. identity management and access control are primary objects of interest. For this the project will take an approach based on anomaly detection (see~\cite{BaileyCL14} for an overview concerning security breaches) and counter-measures based on the adaptation of the security controllers. The anomaly detection will be based on the learning of the protocol language used in the interaction with the system~\cite{Lampesberger13}, and the counter-measures will change the rules of the identity manager and the access control~\cite{BaileyCL14}. Furthermore, the research on anomaly detection will not only focus on the recognition of anomalies that may indicate unauthorised and potentially malicious intrusion and the adequate countermeasures that repair a violation, but also on the early anticipation of damaging activities and the start of counter-measures before any damage has occurred. For instance, components may produce incorrect Byzantine behaviour or not react anymore. In order to cope with such vulnerabilities, the challenge is to identify also states that are close to states that will require counter-measures and sequences of update sets by individual agents that led to these states, such that the combination of both indicate the potential creation of a non-acceptable state within a small number of steps. 

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art by addressing means for interaction and synchronisation, by which desirable security properties of the concurrent system at run-time can be enforced. For instance, by means of transaction operators~\cite{BorgerSW16} strict serialisability and recoverability can be enforced, but there are weaker properties that might be applied to ensure exclusive or priority access to shared locations. Also messaging instead of location sharing will be exploited, for which it will be shown how to integrate the interface theory for concurrent systems developed in~\cite{BauerHW11}. By means of sharing, messaging and synchronisation monitoring algorithms (e.g., see~\cite{DiekertL14} for the handling of run-time verification and monitoring) for the detection of security risks situations will be integrated into the systems. \TheProject{} will further advance the state of the art concerning the handling of security requirements by adding preemptive counter-measures.
\end{mdframed}



\paragraph{Security of Multi-Library Applications}
Including third party libraries into the application code presents additional security risks, as these libraries may include intentional or unintentional vulnerabilities resulting in unwanted behaviour or data leakage of sensitive  information. For example, an open source JavaScript library used in a website may contain malicious code that collects data and sends it to the third party. This is especially problematic for large-scale big data analysis applications, as they are typically composed of calls to several different libraries, as outlined in Section~\ref{sec:XX}. The state of the art solutions to this problem is to use security testing, static analysis and dynamic analysis, such as sandboxing~\cite{jsand}, to detect vulnerabilities and malicious packages before integrating them into the application code. In~\cite{Cova}, a combination of anomaly detection and emulation systems is built to detect malicious code. OWASP~\cite{OWASP} presents the risks in using third party  packages, how to determine if these packages are vulnerable or contain malicious code and how to deploy these packages. Other possible solutions to the problem of integrating third party libraries are based on analysis of the libraries, building control flow graph, looking for copies of buggy code, and profiling packages~\cite{Hanna, XinSun}. These solutions miss many vulnerabilities or malicious dormant code as a result of obfuscation, packing, and the ease in rebuilding new variants of the malicious code, or just the rapid release of new buggy version of packages. There are cases in which it is not beneficial to fix a vulnerability in the software or in a third party code integrated into the software. For example, in cases where the patch is hard to deploy or where fixing the vulnerability has significant impact on performance. In fact, 99\% of attacks are believed to exploit known and fixed vulnerability~\cite{GartnerVulnerability} for which the fix was not deployed. One famous example is the WannaCry attack that was based on a fixed and updated CVE. To protect the software in these cases, \emph{virtual patching} may be used. A virtual patch is a set of rules that implements complex logic to prevent malicious traffic from reaching the application. The challenge is to come up with a set of accurate rules that filter exactly the malicious inputs without blocking other inputs. This work is currently done manually, and it would heavily benefit from integrating a predictive model to automatically detect inputs that may utilise the vulnerability.

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of identification of security vulnerabilities in several ways:
\begin{itemize}
\item \emph{Machine learning augmented SAST for security and privacy breaches detection in Java, C/C++ and Python source code} \YAGshort{ to complete}.
\item \emph{Code mining (ability to extract high value technical, cybersecurity and end userbusiness oriented information from source code) } \YAGshort{ to complete}.
\item \emph{XX} XX.
\end{itemize}
\end{mdframed}

\subsubsection{Security in Pattern-Based Big Data Analytics}
\label{sect:patterns}
%\vjcomment{UOD to write}

The concept of \emph{parallel patterns}, a high-level abstraction for writing parallel programs, dates back to early 90s when it was first applied to distributed programming based on MPI~\cite{cole89skeletons}. A parallel pattern is a high-order function that implements some common parallel behaviour (e.g.~independent processing of different parts of some data structure), doing all the work related to creation, synchronisation and management of parallel threads/processes. The end user only needs to supply their problem-specific code, getting the parallelism for free. In this way, it is possible for a user to parallelise their computation (provided that its structure conforms to one of the available patterns) and hence increase performance without having to deal with low-level details of parallel implementation. The obvious appeal of the concept of parallel patterns resulted in different major IT companies developing their own pattern libraries (Intel Thread Building Blocks~\cite{tbb}, Microsoft Parallel Patterns Library~\cite{ppl} etc.)

%On the other hand, \emph{software design patterns}, introduced in 1994 via \emph{gang of four} patterns~\cite{gofpatterns}, serve different but somewhat similar purpose. They prescribe generic methods for object generation and interaction for particular classes of object-oriented applications and are mostly used in the design phase of the software engineering process. In the \textbf{RePhrase} project, we have investigated how parallel patterns can fit into the software design patterns when developing parallel applications.

In the area of big data processing, parallel patterns have become very popular with the emergence of Google MapReduce programming model~\cite{mapreduce}. This model (which is, essentially, a parallel pattern) allows the development of large-scale, distributed, fault-tolerant data analytics applications without any knowledge of distributed systems. The most popular implementation of the MapReduce model is in the Hadoop framework~\cite{hadoop} for big data storage and processing. Being a generic model that allows many more specific classes of distributed computations to be implemented on top of it~\cite{bigdatabook}, MapReduce was used as a building block for several machine-learning based data analytics frameworks and libraries, such as MLLib~\cite{mllib} for Apache Spark, Keras~\cite{keras} and Tensorflow~\cite{tensorflow}. These libraries also work on the principle of patterns - they provide generic parallel machine-learning methods that can be used in end-user data analytics applications, provided that the end user supplies their problem-specific parameters and operations.

Frameworks for big data processing in general, and the machine learning frameworks in particular, are designed mostly with scalability and fault-tolerance in mind, with the main goals of i) being able to implicitly scale to larger and larger datasets distributed over a large number of machines; and, ii) accommodating for longer running computation by automatically restarting the parts of computation that fail. It is, however, a very well known fact that large-scale distributed processing exposes a number of potential security vulnerabilities~\cite{bigdatasecurity}. In big data processing, the data that needs to be analysed by a single application is often distributed across a large number of machines. Furthermore, the machine that stores and the machine that processes the data is most often not the same. All this results in a requirement for frequent data and computation movement, which gives opportunities for the threats such as DoS attacks, Man-in-the-Middle attacks, replay attacks and eavesdropping. This is especially true as many times the large-scale data processing applications are executed in public clouds, due to cost efficiency and maintainability. As a result of this, there are many tools and frameworks that aim to improve security of big data storage and processing, dealing with the aspects of user authentication (e.g.~Apache Knox~\cite{knox} and Apache Sentry~\cite{sentry}), result verification (e.g.~Cluster BFT~\cite{bft}, Secure MR~\cite{securemr}, Accountable MapReduce~\cite{accountablemr}) and log analysis~\cite{loganalysis}. 

One of the biggest issues in machine-learning based big data processing is that the applications are built using different layers of abstraction, as discussed in Section~\ref{sec:XX}. A typical end user application would use one of the machine learning frameworks or libraries, which would in turn use some pattern-based distributed processing framework such as MapReduce which operates on distributed filesystems and databases. While there is a lot of work in ensuring security of each individual layer, there is not much work in identifying and repairing security issues that arise from the interaction between these different layers. This is a fundamental problem for security of big data processing, since even if perfectly secure frameworks are used for each layer, their interaction may bring unforeseen security vulnerabilities and targets for attacks. The \TheProject{} will address the problem of security of big data processing in a holistic way, considering not only all layers of a typical AI-based data analytics application, but also interactions between these different layers.

%Very little attention has been paid to the security issues. This is especially an issue for large-scale data analytics because typical applications for doing that need to combine several layers of functionality, possibly mixing several different libraries and systems. For example, a typical analytics application will use a machine-learning library such as Keras, which will in turn use more general frameworks for distributed machine learning, such as Tensorflow~\cite{tensorflow}. Tensorflow itself uses lower-level pattern-based frameworks, such as Hadoop MapReduce, which in turn operate on distributed filesystems and databases. While each of the individual layers may or may not have guarantees for safety, interaction between these different layers presents plethora of opportunities for potential cyber attacks. Coupled with the fact that the computations are performed in a distributed fashion, meaning there will possibly be data movement between different nodes and communication between processing agents running on different nodes, all of which can expose additional vulnerabilities that are not present in single-core or even locally-distributed settings, it is clear that ensuring security for distributed machine-learning based data analytics is a very complex problem.
 
\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of security of distributed pattern-based data analytics in several ways:
\begin{itemize}
\item \emph{Security for data storage in big data analytics.} The current systems for storage of data that is used for data analytics use very basic security mechanisms such as XX. With the advanced security diagnostics that will be developed over the course of the project, we will identify what security vulnerabilities arise in using both distributed databases and distributed file systems in end-user data application, as well as identifying methods to mitigate these vulnerabilities by transforming (in static or dynamic way) the user code appropriately without changing its semantics. 
\item \emph{Security of distributed data processing frameworks.} Similarly as for the distributed data storage, the security of distributed data processing frameworks such as Hadoop MapReduce, has never been established. These frameworks expose even more possible security vulnerabilities than the systems for data storage, as they need to move the data around between different distributed processing nodes, as well as to communicate between themselves. In \TheProject{}, we will identify security vulnerabilities coming from using the distributed pattern-based data processing frameworks in the end-user applications. As far as is practically possible, we will also analyse the source code of these frameworks and identify potential security vulnerabilities within them too.
\item \emph{Interaction between machine learning libraries and distributed data storage/processing frameworks.} We will investigate the problem of identifying and repairing security vulnerabilities that can arise from the interaction of machine learning libraries and lower level mechanisms based on distributed data storage and processing. Using machine learning libraries introduces another layer in the hierarchy of data analytics applications, with its own set of potential vulnerabilities, making the overall problem even harder than if only data storage and processing is concerned. 
\end{itemize}
\end{mdframed}


\subsubsection{Privacy in AI-based Data Analytics}
\label{sect:privacy}
%\vjcomment{UOD and SCCH to write}
A machine learning or a data analytics algorithm operates on datasets which might contain private data or sensitive information. The data-owner might not be willing to share data even for machine learning purposes, as a privacy-risk posed by several types of attacks exists. Different methods such as k-anonymity~\cite{10.1142/S0218488502001648}, l-diversity~\cite{10.1145/1217299.1217302}, t-closeness~\cite{DBLP:conf/icde/LiLV07}, and differential privacy~\cite{10.1561/0400000042} have been developed to address the privacy issue. Differential privacy is a formal framework to quantify the degree to which the privacy for each individual in the dataset is preserved while releasing the output of a data analysis algorithm. Differential privacy guarantees that an adversary, by virtue of presence or absence of an individual's data in the dataset, can't draw any conclusions about an individual from the released output of the analysis algorithm. Differential privacy, however, doesn’t always adequately limit inference about participation of a single record in the database~\cite{10.1145/1989323.1989345}. Differential privacy requirement does not necessarily constrain the information leakage from a data set~\cite{Calmon_privacyagainst}. Correlation among records of a dataset would degrade the expected privacy guarantees of differential privacy mechanism~\cite{DBLP:conf/ndss/LiuMC16}. These limitations of differential privacy motivate an information theoretic approach to privacy where privacy is quantified by the mutual information between sensitive information and the released data~\cite{5288525,Calmon_privacyagainst,6482222,7888175,DBLP:journals/corr/abs-1710-09295}. 

A data release mechanism aims to provide useful data available while simultaneously limiting any reveled sensitive information. The data perturbation approach uses a random noise adding mechanism to preserve privacy, however, results in distortion of useful data and thus utility of any subsequent machine learning and data analytics algorithm is adversely affected. There remains the challenge of studying and optimizing privacy-utility tradeoff especially in the case when statistical distributions of data are unknown. Information theoretic privacy can be optimized theoretically using a prior knowledge about data statistics. However, in practice, a prior knowledge (such as joint distributions of public and private variables) is missing and therefore a data-driven approach based on generative adversarial networks has been suggested~\cite{Huang_2017}. The data-driven approach of \cite{Huang_2017} leverages recent advancements in generative adversarial networks to allow learning the parameters of the privatization mechanism. However, the framework of \cite{Huang_2017} is limited to only binary type of sensitive variables. A similar approach~\cite{8919758} applicable to arbitrary distributions (discrete, continuous, and/or multivariate) of variables employs adversarial training to perform a variational approximation of mutual information privacy. The approach of approximating mutual information via a variational lower bound was also used in~\cite{NIPS2016_6399}.         

\TheProject{} will introduce a novel information theoretic approach for studying privacy-utility tradeoff suitable for multivariate data and for the cases with unknown statistical distributions. The approach is to consider entropy of the noise as a design parameter for studying privacy of a data release mechanism. A relevant optimization problem here is to minimize the privacy-leakage quantified by the mutual information (between private and released data) while simultaneously minimizing the amount of data distortion. The optimization of tradeoff between minimizing privacy-leakage and minimizing data distortion can be analytically solved for a known data distribution. However, the data distribution may not be available in practical applications. The framework to be proposed in this study, without knowing data distribution, will optimize the tradeoff curve between privacy-leakage and data distortion level. This is done as follows: 1) The probability density function of noise, that for a given noise entropy level, minimizes data distortion is analytically derived. 2) The privacy of sensitive data is preserved via adding random noise (sampled from derived optimal distribution) to the data observations. Only the noise added data observations are meant to be publicly released. 3) A stochastic model is learned to model the relationships between private and public data. 4) A lower bound on privacy-leakage is derived as a functional of distributions characterizing the data model. 5) An approximation to privacy-leakage is provided via maximizing its lower bound w.r.t. distributions characterizing the data model.           

The proposed approach to study and optimize privacy-utility tradeoff is novel. The are three significant features of the proposed framework. First is its generality for any unknown data distribution, the second is deriving optimal noise adding mechanism analytically, and the third is to compute privacy-leakage analytically without relying on the training of black-box models (e.g. adversarial networks~\cite{8919758}) for approximating distributions. 




\subsubsection{Formal Specification and Proving of Properties}
\label{sect:formal}
\vjcomment{USTAN to write}
\cbcomment{Should build on what we've done for teamplay here? Might want to just pick a tractable subset: secret variables?}

\vjcomment{Definitely we need to elaborate on what is done in TeamPlay and how we go beyond that}

Techniques based on formal methods, such as model checking and symbolic interpretation, do not report false  positives, but have potential problem with scaling. AFL~\cite{AFL} uses use fuzz testing to dynamically profile the code to expose potential security vulnerabilities.

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of formal specification and verification of security properties in several ways:
\begin{itemize}
\item \emph{XX.} XX.
\item \emph{XX} XX.
\item \emph{XX} XX.
\end{itemize}
\end{mdframed}

\subsubsection{Coding Standards for Security}
\label{sect:codingStandards}
\vjcomment{UC3M to write}

There is an inherent impact from programming languages features in the
emergence of vulnerabilities in software. Recognizing this issue ISO has
recently revised its technical report ISO/IEC  TR 24772 series on Guidance to
avoiding vulnerabilities in programming languages. This series consists of a
language independent part~\cite{iso24772:1}, and a set of language specific
annexes including Ada (2020), or C (2020). Other programming languages are
under preparation such as Python, PHP, Spark, Ruby, Fortran, C++ or Java.
Another very relevant source for identifying vulnerabilities are the SEI CERT
secure coding standards for C~\cite{cert:c}, C++~\cite{cert:cpp}, and
Java~\cite{cert:java}. In the context of C++ there are multiple relevant coding
guidelines focused with different intensities in security and safety as
HIC++~\cite{hicpp} , the C++ Core Guidelines~\cite{cpp:core-guidelines}, or the
JSF C++~\cite{cpp:jsf}. Some rules of those guidelines are already supported by
some proprietary tooling (e.g.  Perforce) as well as by open source components
(e.g.  clang-tidy).  Other rules may be supported by library components as the
GSL~\cite{cpp:gsl} or extensions to tools like ThreadSanitizer~\cite{dolz:2017}
or by the use of contract based programming~\cite{lopez-gomez:2019}.

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of security standards and static analysis to conformance to standards in several ways:
\begin{itemize}
\item \emph{Produce a security centric coding guideline} 
Based on existing and ongoing coding guidelines for C++ we will develop a new
integrated coding guideline completely focused on the security aspect and with
a strong bias to specific characteristics of distributed storage and machine
learning data analytics.

\item \emph{Provide tooling for enforcing guideline conformance} 
We will develop a single interface to multiple tools that enforce different aspect
of the new C++ coding guidelines, producing also new checks that need to be developed.

\item \emph{Check and refactor for specific components} 
We will check and enforce the security guidelines in software components identified
in the project to validate that the approach is faeasible and useful to
application and infrastructure developers. 

\end{itemize}
\end{mdframed}

%\subsubsection{Identifying Security Vulnerabilities}
%\label{sect:security}
%\vjcomment{This is the material for one of the old proposals. We need to check this and move the relevant stuff to the other sections (there seems to be lots of relevant stuff here).}
%
%Application security is becoming increasingly important in modern computer systems.
%Current practice is for security to be ensured at run-time, using efficient monitoring
%techniques coupled with dynamic run-time checks such as ISR~\cite{isr}, ASLR~\cite{aslr}
%and CFI~\cite{cfi}. These methods are, however, expensive in terms of computational
%complexity and energy consumption. As most of the security vulnerabilities come from
%code defects, bugs and logic flaws, the most cost-effective way to ensure security is
%to follow the secure code best practices~\cite{OWASP} and eliminate vulnerabilities
%before code is deployed. This is usually done with static and dynamic code analysers.
%Static code checkers, such as Appscan Source~\cite{AppScan} and Coverity~\cite{Coverity}
%%and Klocwork~\cite{Klocwork}, 
%are based on techniques for quality checking that
%have been extended to cover security vulnerabilities in the code. The main drawback of 
%these methods is their high rate of false positives they produce. Techniques based on
%formal methods, such as model checking and symbolic interpretation, do not report false 
%positives, but have potential problem with scaling. AFL~\cite{AFL} uses use fuzz testing 
%to dynamically profile the code to expose potential security vulnerabilities. 
%None of the above tools includes special algorithms for detecting vulnerabilities 
%resulting from parallel execution. Parallelism, due to undeterministic nature of the
%application execution, presents many additional problems from the security perspective. These problems have to be dealt with efficiently
%in future systems.
%
%
%Including third party libraries into the application code presents additional
%security risks, as these libraries may include intentional or unintentional
%vulnerabilities resulting in unwanted behavior or data leakage of sensitive 
%information. For example, an open source JavaScript library used in a website 
%may contain malicious code that collects data and sends it to the third party. 
%%Another example is binary packages which contain vulnerability which is detected by crackers and compromise users??? machines. 
%The state of the art solutions to this problem is to use security testing, static 
%analysis and dynamic analysis, such as sandboxing~\cite{jsand}, to detect 
%vulnerabilities and malicious packages before integrating them into 
%the application code. % is proposed to detect malicious JavaScript code, in addition, they 
%%suggest wrapping the components to control access to security sensitive operations. 
%In~\cite{Cova}, a combination of anomaly detection and emulation systems is built to detect malicious code. OWASP~\cite{OWASP} presents the risks in using third party 
%packages, how to determine if these packages are vulnerable or contain malicious code 
%and how to deploy these packages. %This problem is not met only in JavaScript packages, it is in many other 3rd party packages, especially, Android 3rd party libraries. 
%Other possible solutions to the problem of integrating third party libraries are
%based on analysis of the libraries, building control flow graph, looking for 
%copies of buggy code, and profiling packages~\cite{Hanna, XinSun}.%~\cite{Hanna, XinSun, Nora, Backes}.
%These solutions miss many vulnerabilities or malicious dormant code as a result of 
%obfuscation, packing, and the ease in rebuilding new variants of the malicious code, 
%or just the rapid release of new buggy version of packages.
%
%There are cases in which it is not beneficial to fix a vulnerability in the 
%software or in a third party code integrated into the software. For example, 
%in cases where the patch is hard to deploy or where fixing the vulnerability 
%has significant impact on performance. In fact, 99\% of attacks are believed to exploit known and fixed vulnerability~\cite{GartnerVulnerability} for which the fix was not deployed. One famous example is the WannaCry attack that was based on a fixed and updated CVE.
%To protect the software in these cases,
%\emph{virtual patching} may be used. %Virtual patching is a security policy 
%%enforcement layer which prevents the exploitation of a known vulnerability. 
%A virtual patch is a set of rules that implements complex logic to prevent 
%malicious traffic from reaching the application. 
%%The virtual patching mechanism will usually be integrated in the organization firewall. 
%The challenge is to come up with a set of accurate rules that filter 
%exactly the malicious inputs without blocking other inputs. This work is 
%currently done manually, and it would heavily benefit from integrating a 
%predictive model to automatically detect inputs that may utilize the 
%vulnerability.
%
%
%
%
%\TheProject{} will advance the state-of-the-art of security of parallel code in the following directions:
%\begin{itemize}
%\item XX
%\end{itemize}

\subsubsection{Authentication}
\label{sect:auth}
\vjcomment{COGNITIVE to write}

Researchers and practitioners have attempted to provide various \textit{solutions for multi-factor authentication (MFA)} such as tokens based on push notifications on smartphones, Time-based One Time Passwords (TOTP), Quick Response (QR) codes, graphical Transaction Authentication Numbers (PhotoTANs) [REF]. With regards to \textit{continuous user authentication}, several works proposed solutions based on users’ interaction behavior analysis on both desktop computers and smartphones [REF], physiological data analysis such as body signals (heart rate, skin conductance, etc.) [REF] and face biometrics [10, 11], and eye gaze data analytics [REF]. Various organizations and companies also exist that are either adopting MFA, or continuous user authentication (e.g., Cisco’s Duo Security (https://duo.com), Futurae (https://futurae.com), Acceptto (https://acceptto.com), Auth0 (https://auth0.com), Saaspass (https://saaspass.com)), however, these are far from fully adopting solutions that combine MFA and continuous user authentication. 

\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of authentication for big data analytics systems by:
\begin{itemize}
\item \emph{XX.} XX.
\item \emph{XX} XX.
\item \emph{XX} XX.
\end{itemize}
\end{mdframed}

\subsubsection{Security Patterns}
\vjcomment{USTAN and UOD to write -- possibly merge with the previous subsections?}

\vjcomment{I think this can be integrated into the section \ref{sect:patterns}}

\begin{itemize}
	\item Design pattern
	\item parallel patterns or skeletons
	\item abstractions over low-level implementation
	\item provided as a library
	\item structured approach
	\item extended for security
	\item new patterns X Y Z
\end{itemize}

\subsubsection{Security Contracts}
\cbcomment{USTAN to write - we can build on what we've done for teamplay here}

\vjcomment{Let us move this to \ref{sect:formal}}

\begin{itemize}
    \item Contract
    \item CSL system
    \item Proving properties of security
    \item Reflecting proofs back to the programmer
    \item Vulnerabilities
    \item Secret variables
    \item Branching and side channel attacks?
\end{itemize}

\subsubsection{Refactoring}
%\vjcomment{USTAN to write}
%\begin{itemize}
%	\item Refactoring history
%	\item program transformation
%	\item refactoring used as a software engineering strategy
%	\item structural changes to the code
%	\item refactoring to introduce parallel patterns
%	\item will be extended to introduce security patterns, fix vulnerabilities, adhere to standard
%	\item some aspects may be proved correct
%\end{itemize}

In essence, refactoring is about changing the structure of a program's source code in such a way that the change does not alter the functional behaviour of the program. It is a software engineering practice that is used daily by developers, who restructure code to make it more amenable to maintenance and extension. Refactoring is also a technique that is usually practised \emph{manually} by developers. The downside of manual refactoring is that it becomes very error-prone over large code bases; it also requires the refactorer to be an expert in the particular structural change that is being implemented. Refactoring tools, on the other hand, automate the process, allowing machine-checkable pre- and post-conditions to automatically ensure that the refactored code is functionally correct to the original. 
%
Automated refactoring has roots in a long history, with the original refactoring work going back to the \emph{fold/unfold} system proposed by Bustall and Darlington in 1977~\cite{darlington77}. Since then, refactoring has been applied to a wide range of applications as an approach to program transformation~\cite{Mens:2004:SSR:972215.972286}, , and has been applied to a wide range of applications as an approach to program transformation~\cite{mens_refactoring}, with refactoring tools a feature of popular IDEs including, \textit{e.g.}, Eclipse and Visual Studio.

%There has been very little prior work on refactoring for improving energy consumption, with the exception of some preliminary refactorings for Erlang~\cite{Nagy:2019:TSG:3331542.3342570}. Previous work by Brown et al. on parallelisation \textit{via} refactoring has primarily focussed on the introduction and manipulation of parallel pattern libraries in C++~\cite{brownagricultural}, Haskell~\cite{kar30667} and Erlang~\cite{Brown:2014:CRP:2630180.2630190,DBLP:journals/cai/BarwellBHTB16, parco2015}. Another approach
%has been the automated introduction of annotations in the form of C++ attributes~\cite{cfs}{rio:2018}.

In previous work, refactoring has been used successfully to \emph{introduce parallel patterns and concurrency} into the code in C++~\cite{brownagricultural,DBLP:conf/pdp/JanjicBMHDAG16, grppirefactoring, mcts} and Erlang~\cite{hlpp,DBLP:journals/cai/BarwellBHTB16}. Another approach
has been the automated introduction of annotations in the form of
C++ attributes~\cite{rio:2018}. Dig \textit{et al.}~\cite{dig} use refactoring to introduce parallelism in Java.  
%
%Parallel design patterns, or algorithmic skeletons, were suggested as solution to the difficulties presented by low-level approaches~\cite{Asanovic:2009:VPC,DBLP:journals/spe/Gonzalez-VelezL10}.
%A range of pattern/skeleton implementations have been developed for a number of programming languages; these include: RPL~\cite{DBLP:conf/pdp/JanjicBMHDAG16}; Feldspar~\cite{DBLP:conf/ifl/AxelssonCSSEP10}; FastFlow~\cite{fastflow:2017}; Microsoft's Parallel Patterns Library~\cite{ACM:book/msoft/CampbellM11}; and Intel's Threading Building Blocks (TBB) library~\cite{DBLP:reference/parallel/X11pz}.
%Since patterns are well-defined, rewrites can be used to automatically explore the space of equivalent patterns, e.g.\ optimising for performance~\cite{DBLP:conf/europar/MatsuzakiKIHA04,DBLP:conf/ipps/GorlatchWL99} or generating optimised code as part of a DSL~\cite{DBLP:conf/dagstuhl/Gorlatch03}. Moreover, since patterns are architecture-agnostic, patterns have been similarly implemented for multiple architectures~\cite{DBLP:conf/cgo/HagedornSSGD18,DBLP:conf/parco/ReyesL15}.
%
%This introduces a level of specialisation, and the possibility of choice between pattern implementations. Conversely, GrPPI~\cite{DBLP:journals/concurrency/AstorgaD0G17} is capable of invoking other libraries, and is thereby able to take advantage of the specialisations that they present without potentially laborious reimplementation.
%
Elsewhere, approaches to automatic parallelisation have traditionally focussed on the transformation of loops. Examples include Lamport's early approaches in Fortran~\cite{DBLP:journals/cacm/Lamport74}, Artigas' approach for Java~\cite{kennedy}, on \textsc{doall} and \textsc{doacross} loops~\cite{DBLP:conf/pldi/BurkeC86,DBLP:conf/popl/LimL97}, the polyhedral model~\cite{DBLP:conf/ppopp/AncourtI91,DBLP:conf/IEEEpact/Bastoul04,DBLP:conf/IEEEpact/BouletF98} and more recently on the generation of pipelines~\cite{DBLP:conf/IEEEpact/TournavitisF10,DBLP:journals/taco/WangTFO14}. 
%
%The polyhedral model~\cite{DBLP:conf/ppopp/AncourtI91} also \textcolor{red}{looms large} (\cite{DBLP:conf/IEEEpact/Bastoul04,DBLP:journals/ijpp/GrieblFL00}).
%
Other approaches to automatic parallelism have included a focus on coarsely dividing programs into sections that can be run in parallel~\cite{DBLP:conf/pepm/LiT15,DBLP:conf/ppopp/RulVB08}; less-abstractly on exploiting potential parallelism at the instruction-level~\cite{DBLP:conf/europar/StefanovicM00}; and on exploiting specialised hardware such as GPUs for automatic parallelisation~\cite{DBLP:conf/popl/GuoTS11,DBLP:conf/pppj/LeungLL09}.
% 
%\textcolor{red}{Other approaches: slicing for concurrency \& Rul's Extracting coarse grain parallelism (\cite{DBLP:conf/ppopp/RulVB08}); instruction level parallelisation (??); GPUs (\cite{DBLP:conf/popl/GuoTS11,DBLP:conf/pppj/LeungLL09}).}
%
Whilst fully automatic approaches simplify the parallelisation process for the programmer by removing them from the process, such approaches can be very specific in both the parallelism they are able to introduce and the code to which they can be applied. Conversely, programmer-in-the-loop approaches, such as refactoring, allow the programmer to employ their knowledge about both code and parallelism.
%
There is only limited previous work on refactorings for increased security. This includes proposed refactorings to increase the protection level of sensitive information stored in the Java code~\cite{maru2007} and refactorings to also alert its impact on code vulnerabilities~\cite{maru2008}.
Brown et al. proposed refactorings for removing \emph{anti-patterns} in Distributed Java Components~\cite{brown1998antipatterns}.  Improving software security using search-Based refactoring was proposed by Ghaith and Cinneide for Java programs, using the Code-Imp tooling~\cite{ghaith}. Finally, using refactoring rules to increase security for Object Oriented design was proposed by Khan and Khan~\cite{khan} . However, none of these proposed systems uses the concept of a \emph{security pattern}, static analysis for code vulnerabilities, formal reasoning or tool support for languages other than Java. 
 
\begin{mdframed}[backgroundcolor=gray!10]
\TheProject{} will advance the state of the art in the area of refactoring for security in several ways:
\begin{itemize}
\item \emph{Refactorings to introduce security patterns.} We will develop new software refactorings that will rewrite source code to introduce security aware patterns. The results of which will be equivalent software in terms of functionality, but with increased security.
%\item \emph{Refactorings targetting security of big-data applications.} XX.
\item \emph{Verifiable refactorings using formal reasoning.} We will provide correctness and general soundness proofs for some of the refactorings developed, using abstract state machines and verification techniques. This will give confidence that the refactoring approach will not only increase the security level of the source code, but do it in such a way that the behaviour of the program will not change as a result.
\item \emph{Refactorings that will repair vulnerabilities in the code.} We will take the results of the static analysis and self healing techniques and produce further refactorings and transformations that rewrite source-code making it more secure by eliminating or reducing code vulnerabilities. 
\item \emph{Refactorings that will transform code to make it conform to security coding standards.} We will produce new refactorings and transformations that will rewrite the source code so that it conforms to a security standard, therefore increasing the confidence that the program is secure
\item \emph{Refactorings guided by static and dynamic analysis.} The refactorings developed as part of \TheProject{} project will be guided by both static and dynamic analyses, finding the security vulnerabilities and ``hot spots'' within the source code as candidates for refactoring. 
\end{itemize}
\end{mdframed}

\subsubsection{Use Cases}
\label{sect:applications}
\label{sect:background-last}
\vjcomment{FREQUENTIS, DEMOCRITOS and SOPRA to write}

\begin{table}
\begin{center}
\begin{tabular}{|c||c|c|c|c|}
\hline \hline
\multicolumn{5}{|c|}{Use Cases} \\ \hline
Layer & \textbf{Air Traffic Management} & \textbf{Digital Banking} & \textbf{Healthcare} & \textbf{5G Networks} \\
\hline \hline
\emph{Environment} & Public/Private Cloud & Public Cloud & Public Cloud & X \\
\emph{Data Storage} & Azure DL, MySQL & Azure DL, Cassandra & Azure DL & X \\
\emph{Data Processing} & Azure DL Analytics & Azure DL Analytics & Azure DL Analytics & X \\
\emph{ML Framework} & Azure AI & Spark MLLib & Azure AI & X \\
\emph{Prog. Language} & Java, JavaScript, Python & C++ & C++ & X \\
\hline \hline
\end{tabular}
\caption{Technologies Used in \TheProject{} Use Cases}
\label{tab:usecases}
\end{center}
\end{table}


\paragraph{Air Traffic Management}
\label{sec:atm}
- Air Traffic Management (ATM) is an aviation term for the amalgamation of various systems that supports an aircraft to depart from an aerodrome, transit required airspace, and land at a destination aerodrome. The system includes the following sub-systems Air Traffic Services (ATS), Airspace Management (ASM), and Air Traffic Flow and Capacity Management (ATFCM). With the rise of drones, it becomes essential to integrate Unmanned Traffic Management (UTM) with ATM. The increasing emphasis of modern ATM is on inter-operable and harmonised systems permitting an aircraft to operate with the minimum of performance change from one airspace to another. With the increase use of Internet-of-Things via sensors and on-board monitoring of the aircraft and required ground support infrastructure the ATM/UTM effectiveness and efficiency are now critical. The introductions of other new unmanned aircraft and drones into the airspace is requiring a rethink of the system and security required to enable the exchange of relevant information with the ATM/UTM without introducing any data breath issues. 
Various stakeholders in ATM are facing two key challenges: the requirement to migrate from ATS Message Handling System (AMHS) to System Wide Information Management (SWIM), and the requirement to integrate UTM with ATM. SWIM is an integral component of the digital transformation that is taking place in the Aviation industry. This digital transformation is having a major impact, as not only systems but also new processes and new staff expertise are required Today Air Navigation Services Providers (ANSPs) rely on closed systems with proprietary point-to-point communication for data exchange. SWIM enables these systems to operate as an overlay to IP networks using open standards and ATM-defined data exchange models (Aeronautical Information Exchange Model (AIXM), ICAO Meteorological Information Exchange Model (IWXXM), Flight Information Exchange Model (FIXM). 

\paragraph{Open banking}
\label{sec:banking}
- As the digital disruption moves the banking services closer to the end-user more diverse software tools are deployed into the banking workflow via open banking solution. We plan to look at how we auto-generate data processing code for a processing hub for the Competition and Markets Authority Nine (CMA9) that forms the nine largest banks in the United-Kingdom. We will use the tools we develop during the research to secure the auto-generated code is secure and without unwanted vulnerabilities.

%\paragraph{Space-based Open Internet Technologies}
%\label{sec:spacenet}
%- As the digital access in the world moves to a more cost effective hybrid space and ground solution the use of Delay/Disruption Tolerant Networking (DTN) and Solar System Internet (SSI) technology opens the global communication system to penetrate into more remote areas and will but more social care and healthcare systems onto the open internet. We want to ensure that we create code to deploy within these edge computing devices that can auto-heal their own security vulnerabilities. This new internet will orbit at +/- 500 kilometers above the earth and will supply a coverage of communication across the globe. As this uses the DTN solution to communicate is required to ensure the system is secure at all times as it has data in-rest and in-motion at the same-time.

\paragraph{Distributed Healthcare (IoT)}
\label{sec:health:IoT}
- As more sensors enter into the Distributed healthcare (IoT) domain the code for these sensors, edge collection devices and core processing capability will becoming a life-threatening issue if any vulnerabilities cause unwanted outcomes for a patients health. We want to ensure auto-healing of our solution and indirectly auto-healing our citizens' healthcare ecosystem.

\subsubsection{Key Technologies Used in the \TheProject{} Project}
\label{sect:key-technologies}

\paragraph{\SCCHshort{} Machine Learning Tools.}
\label{sec:mlpp}
\emph{mlpp} is a C++ library with interfaces to several other languages. It 
is currently under heavy development. So far, it contains algorithms for 
linear regression, time series analysis, feature selection and causal 
dependency discovery. Its focus is on building interpretable models. It is hosted 
on SourceForge (\url{https://sourceforge.net/projects/ml-pp/}), and released 
as Open Source (GPL license); paid licenses for commercial use will be provided
with the first mature release.

\paragraph{\SCCHshort{} eKNOWS - Reverse Engineering Formal Models from Source Code.}
As part of application-oriented research projects, SCCH has built and applied reverse engineering and analysis tools with a strong focus on extracting knowledge from source code and related software engineering artifacts (http://codeanalytics.scch.at/). The results are made available via eKNOWS, a platform for the analysis and evolution of software systems, documentation generation, and quality assurance and automated testing. This tool was recently extended with semi-automated reverse engineering capabilities to extract formal ASM specifications (models) from Java source code. The extracted ASMs provide a consistent high-level view of systems, abstracting irrelevant details and providing a precise and complete description of the behaviour of the system. The approach enables us to prove that the original source code is a correct refinement of the extracted model. Given the multi-language reverse engineering technology (based on abstract syntax trees) used by eKNOWS, this capability can be extended to other programming languages such as C++ (as required by the project). We plan to extend this tool to extract formal ASM specifications of contracts from source code.    

\paragraph{\IBM{} Security Vulnerability Detection Technology.}

IBM developed vulnerability detection technologies based both on static and dynamic methods. IBM developed Beam for static analysis of C/C++ code and extended it to look in particular for security vulnerabilities with a low ratio of false positive. In addition, IBM developed the ExpliSAT tool which uses symbolic interpretation for more precise program analysis including checks for security vulnerabilities such as buffer overflow detection. During the \rephrase{} project, ExpliSAT was extended to analyse parallel programs. To complement the static methods IBM uses also Fuzz testing technology that detects security vulnerabilities dynamically using genetic algorithms. In order to achieve accurate and scalable security vulnerability detection, IBM is now working on combining the static and dynamic analysis vulnerability detection tools.     

\paragraph{\SAshort{} \paraformance Refactoring and Code Analysis Tools.}


The \paraformance Refactoring tool was originally developed for the EU FP7 \paraphrase{} project, and later extended and enhanced in both \rephrase{} and the Scottish Enterprise innovation project, \paraformance{}. 
\paraformance is a tool-chain designed to \emph{democratise} parallel programming by allowing software developers to quickly and easily write parallel software. \paraformance enables software developers to find the sources of parallelism within their code, automatically (through user-controlled guidance) through a process of \emph{pattern discovery}. \paraformance also offers refactoring support to allow parallel patterns to be introduced directly into the source code of the application, inserting the parallel business logic. \paraformance also has integrated safety checking features, to not only enable  the parallelised code to be thread-safe, but also the checking of sequential code, eliminating potential sources of parallelism errors that occur, such as race conditions and deadlocks. Finally, \paraformance is able to repair some of the parallelism errors detected by the safety checking to automatically make the application thread safe.
\paraformance will be extended in \TheProject{} by adding new refactorings that aim to increase the security levels of an application, this will be done by new refactorings that introduce security-aware patterns into the source code, repair security vulnerabilities and refactor the code to ensure it conforms to a security standard. Further more, the refactorings will be verified increasing confidence in their implementation.
%
% \paraformance will be extended in \TheProject{} by adding safety checks for the distributed code, in addition to the existing mechanisms for checking the code that is executed on shared-memory machines. We will also extend the tool with features to check for \emph{security} of the code. 

\paragraph {\SAshort{} CSL: The Contract Specification Language}
CSL (Contract Specification Language) was developed as part of the ongoing H2020 \teamplay{} project. It is designed to act both as an annotation library for C applications, allowing the developer to annotate their source code both to reflect non-functional properties of time, energy and security back into the source code, but also in order to express assertions ---or contracts--- about the non-functional properties. Such non-functional properties could be, for example, the worst-case execution time (or energy) of a block of code, the security vulnerability level of a variable, etc. The assertions themselves are expressed in terms of these properties and other normal variables within the source code. The properties can also be reflected back into the source-program as first-class citizens: normal programming variables with values in the source-level space.
The assertions are then passed to an underlying proof system together with an abstract interpretation of the C program. This proof system (currently implemented using the dependently typed language, Idris) produces  a proof of whether or not the assertion can be met. CSL will be extended in \TheProject{} by adding ... 

\paragraph{\FRQshort{} MosaiX SWIM: Aviation Integration Platform}
\label{sec:swim}
The Frequentis Integration Platform is an end-to-end solution that provides aviation stakeholders (ANSPs, airports, airlines, met offices) with tools required to unlock and monetize their data by offering new services to their customers (i.e. airlines). It allows aviation stakeholders to easily interface with legacy and third-party systems, enabling data to be freed from traditional storage silos and fused with different sources. MosaiX SWIM is designed around a microservices architecture and an open messaging system in which all software components exist as independent artefacts and communicate in a decoupled way. This reduces vendor lock-in by allowing customers to substitute components and incorporate new technologies in the future. For the software components  Java, Elasticsearch, Phyton, and on client side JavaScript is used. The platform provides an efficient API Manager and tools for billing consumers of services using models such as hourly billing or consumption-based billing. In addition, it offers preconfigured and customisable metrics and dashboards, together with data-analysis tools.



\paragraph{\YAGshort{} Machine learning augmented Static Analysis.}

YAGAAN developed vulnerability detection technologies based both on static and dynamic methods. IBM developed Beam for static analysis of C/C++ code and extended it to look in particular for security vulnerabilities with a low ratio of false positive. In addition, IBM developed the ExpliSAT tool which uses symbolic interpretation for more precise program analysis including checks for security vulnerabilities such as buffer overflow detection. During the \rephrase{} project, ExpliSAT was extended to analyse parallel programs. To complement the static methods IBM uses also Fuzz testing technology that detects security vulnerabilities dynamically using genetic algorithms. In order to achieve accurate and scalable security vulnerability detection, IBM is now working on combining the static and dynamic analysis vulnerability detection tools.     



\subsubsection{Innovation Potential}
\label{sec:innovationpotential}
\label{innovationpotential}

\eucommentary{Describe the innovation potential which the proposal represents. Where relevant, refer to products and services already available on the market. Please refer to the results of any patent search carried out.}

\clearpage
\section{Impact}
\label{sec:impact}

\TODO{Look at this once the rest of the project is together.}


\eucommentary{Describe how your project will contribute to:\\
o the expected impacts set out in the work programme, under the relevant topic;\\
o improving innovation capacity and the integration of new knowledge (strengthening the competitiveness and growth of companies by developing innovations meeting the needs of European and global markets; and, where relevant, by delivering such innovations to the markets;\\
o any other environmental and socially important impacts (if not already covered above).}

\TheProject{} aims to achieve significant impact in most of the areas outlined in the XX call, providing a tool-supported methodology for developing and deploying secure and privacy-preserving large-scale distributed AI-based data analytic applications. It will allow the end users to develop their complex data analytics applications using high-level programming models based on design patterns, supporting them in the process of making these applications secure and reducing the number of security and privacy vulnerabilities.  

\subsection{Expected Impacts}

\subsubsection{Short Term Impacts}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 1:}
Reduced number and impact of cybersecurity incidents.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} directly addresses this impact by developing tools to advance the state-of-the-art in identification of security threats in distributed big data analytics applications, as well as building the self-healing mechanisms to repair the identified vulnerabilities. Combining techniques of static analysis, dynamic analysis and runtime monitoring for identification of vulnerabilities we will ensure that we can identify a wide range of threats that are both inherent to the application design and that can arise from malicious behaviour at runtime. By using a combination of code transformation and runtime adaptation, we will be able to offer multilayer security to the end user applications, providing protection both at design/implementation time and at runtime.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 2, 3 } and \textbf{4}; \textbf{Objectives 1, 2, 3, 5} and \textbf{6}.}
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 2:}
Efficient and low-cost implementation of the NIS Directive and General Data Protection Regulation.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} specifically looks into privacy of sensitive data in big data analytics. We will address multiple aspects of data privacy in this setting, considering privacy of not only data in transit but also of the machine learning models, using noise-adding mechanisms to further ensure that the models themselves do not accidentally reveal sensitive information. We will explicitly consider compliance of the tooling for big data analysis to the GDPR and other regulations, considering all the layers from low-level distributed filesystem/database access to the composition of end-user applications using high-level machine-learning libraries.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 2}  and \textbf{3}; \textbf{Objectives 2, 3, 4} and \textbf{5} }
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 3:}
Effective and timely co-operation and information sharing between and within organisations as well as self-recovery;
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
Self-recovery is one of the main themes of the \TheProject{} project, addressed by our self-healing technology based on code transformation and restructuring. We will, furthermore, integrate this technology into the overall software engineering methodology for developing secure and privacy-preserving AI-based big data analytic applications, making it transparent to the end user and therefore significantly easing the development of such applications. 
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 4:}
Availability of comprehensive, resource-efficient, and flexible security analytics and threat intelligence, keeping pace with new vulnerabilities and threats.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} will combine several techniques for discovering and repairing security and privacy vulnerabilities in the distributed applications. In particular, we will develop a machine-learning based runtime monitoring mechanisms that will continually monitor the application execution for security threats. Together with the mechanisms for runtime adaptation that will ensure, over a number of steps, that the overall system is in stable and secure state, this will ensure that we can not only identify the known vulnerabilities, but also, by continuously updating the learning model, identify any new threats as they arise. \TheProject{} will also build on the existing security standards for C++ and Java programming and will continually extend them as the new types of threats and vulnerabilities are identified. This will ensure that the new vulnerabilities and threats are addressed in the upcoming coding standards. 

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 2, 3}  and \textbf{4}; \textbf{Objectives 2, 3, 4, 5} and \textbf{6} }


\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 5:}
Availability of advanced tools and services to the CERTs/CSIRTs and networks of CERTs/CSIRTs.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
Parts of the \TheProject{} technology (such as secure authentication, formal methods for verifying security properties, refactoring tools and security standards) will be released as open source products, while the others will be integrated into the products and services offered by our commercial partners at \IBMshort{}, \YAGshort{} and \SOPRAshort{}. This will ensure the availability of the tools developed over the course of the projects to the security teams, that will be able to exploit them as a part of their security infrastructure. Furthermore, the \TheProject{} technologies will be promoted in numerous ways, including online courses for wide audiences (MOOCs), ensuring there is a wide outreach for the techniques we will develop.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aim 5}; \textbf{Objective 8} }

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 6:}
An EU industry better prepared for the threats to IoT, ICS (Industrial Control Systems), AI and other systems;
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\TheProject{} specifically targets distributed systems and the AI-based data analytics, aiming to intercept and eliminate threats in such systems. As such, our techniques will be applicable to the general IoT and edge-computing settings as well. By focusing both on core techniques for identification and elimination of security threats, and the user-centered interface and pattern-based methodology for the development of secure applications, we will allow non-experts in security to develop future-proof secure and privacy preserving data analytics applications. Furthermore, developing mechanisms for formal proving of security properties of applications and for compliance to coding standards will provide further trust in security of the developed applications.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 1, 2}  and \textbf{3}; \textbf{Objectives 1--6} }

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 7:}
Self–recovering, inter-operable, scalable, dynamic privacy-respecting identity management schemes.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
\COGNI to write.
\end{mdframed}

\subsubsection{Medium to Long Term Impacts}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 1:}
Availability of better standardisation and automated assessment frameworks for secure networks and systems, allowing better-informed investment decisions related to security and privacy.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
Standardisation is one of the main goals of the \TheProject{} project. We will build the source code analytics tooling to ensure the conformance of the C++ and Java code to the state-of-the-art coding standards. We will also extend these standards over the course of the project, providing new rules and guidelines specifically aimed at composing the large-scale distributed applications from multiple layers of libraries. This will contribute to better standardisation of the network systems. Furthermore, the formal methods for proving the security properties of the systems will allow additional formal assurance for the security of software, further allowing better-informed investment decisions with respect to security and privacy.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aim 1}; \textbf{Objectives 1} and \textbf{4}}

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 2:}
Availability and widespread adoption of distributed, enhanced trust management schemes including people and smart objects.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
XX
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 3:}
Availability of user-friendly and trustworthy on-line products, services and business;
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
The main aim of the \TheProject{} project is to make the development of complex, large-scale data analytics applications approachable to non-experts in security. The project will deliver a tool-supported methodology for development and deployment of such applications on public clouds, which will enable the end-users to easily write and adapt their products and services. The uses cases from telecommunications, healthcare, banking and air traffic monitoring domains will demonstrate that our methodology can be use to enhance the security of the existing applications, as well as to develop new secure and privacy-preserving applications.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 4}  and \textbf{5}; \textbf{Objectives 6, 7} and \textbf{8} }

\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 4:}
Better preparedness against attacks on AI-based products and system.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
The main target of the \TheProject{} technologies are AI-based products and specifically the emerging large-scale AI-based big data analytics, which nowadays represents the fastest growing industry in computer science. Our focus on multi-layered big data analytics applications that are deployed on distributed infrastructure, including distributed filesystems and databases and that combine the generic big-data analytics processing platforms such as Hadoop with higher-level machine-learning frameworks such as Azure AI will allow our technologies to be applicable to the widest range of AI-based products and systems. This will be further demonstrated by a variety of the domains of the use cases provided by \SOPRAshort{} and \FRQshort{}. 

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 1, 2} and \textbf{3}; \textbf{Objectives 1--6}}
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 5:}
A stronger, more innovative and more competitive EU cybersecurity industry, thus reducing dependence on technology imports.

\paragraph{Expected Impact 6:}
A more competitive offering of secure products and services by European providers in the Digital Single Market.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve This Impact:}
Industry partners involved in the \TheProject{} project, specifically \IBMshort{} and \YAGshort{}, aim to deliver the technologies that will be of the TRLs 6 and 7, which indicates the technologies or system prototypes demonstrated in industry relevant environment. This means that the \TheProject{} will produce technologies that will be close to being usable in practice. The technologies developed will be either open source, where feasible, or available as parts of commercial tool suites. This, coupled with the educational program using MOOCs to promote the \TheProject{} technologies, will contribute to the more competitive EU cybersecurity industry and reducing dependence on importing technologies from abroad.

\emph{\TheProject{} aims and objectives related to these impacts: \textbf{Aim 5}; \textbf{Objectives 6} and \textbf{7}}
\end{mdframed}


%\end{mdframed}
%\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{Expected Impact 6:}
%A more competitive offering of secure products and services by European providers in the Digital Single Market.
%\end{mdframed}

%\begin{mdframed}[backgroundcolor=gray!10]
%\paragraph{How Will \TheProject{} Achieve These Impacts:}
%XX
%\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!5]
\paragraph{Expected Impact 7:}
A wider understanding amongst developers of the issues involved in Code Security
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!10]
\paragraph{How Will \TheProject{} Achieve These Impacts:}
One of the objectives of the \TheProject{} project is to develop a methodology for producing secure and privacy-preserving big data analytics applications. In this way, we will look at the whole process of developing applications, from the design using security patterns to the deployment on distributed architectures, allowing us to understand the issues related to security and privacy that arise in each of the stages of the development. Dissemination and exploitation activities, and in particular the education program that will be implemented in MOOCs, will allow us to communicate the identified issues and solutions to the wider audience.

\emph{\TheProject{} aims and objectives related to this impact: \textbf{Aims 4 } and \textbf{5}; \textbf{Objectives 6, 7} and \textbf{8}}
\end{mdframed}

%\begin{mdframed}[backgroundcolor=blue!5]
%\paragraph{Expected Impact 5:}
%A more competitive offering of secure products and services by European providers in the Digital Single Market.
%\end{mdframed}

%\begin{mdframed}[backgroundcolor=gray!10]
%\paragraph{How Will \TheProject{} Achieve This Impact:}
%XX
%\end{mdframed}


\pagebreak
\paragraph*{Improving Innovation Capacity}
\noindent
\TheProject{} will significantly improve long-term innovation capacity by enabling European companies and individuals to develop complex and innovative new secure large-scale big data analytics applications for a variety of commercially-important and emerging markets, including healthcare, air traffic monitoring and digital banking. A combination of different methods for identifying and repairing security risks and privacy leaks, coupled with the formal methods for proving the security properties and continuously evolving security standards will allow identification and repairing of not only current, but also the future security threats, making the software developed using our technologies robust and resilient in future too. The \TheProject{} methodology, together with the associated tool chain, will significantly reduce the level of expertise required from the developers in developing secure code, allowing non-expert in this area to develop software that is provably secure and automatically checked for the conformance to standards. This will bring a major boost to the European markets in software, as the security and privacy will become the most important factors when designing and implementing the new software, especially in the light of the increased distribution of data across domains in parallel with more and more strict regulations governing privacy and ownership of the data. Section~\ref{sec:innovationpotential} (page \pageref{sec:innovationpotential}) describes the market opportunities and potential for innovation 
in more detail.

\paragraph*{Societal Impact.}
\noindent
\subsubsection*{Possible Barriers to Achieving the Expected Impacts and Associated Mitigations}

\newcounter{barrier}

\begin{longtable}{|p{125pt}|p{320pt}|}%\hline

\hline \textbf{Possible Barrier}&

\textbf{Mitigation}\\ \hline
\endfirsthead

\multicolumn{2}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous
page}} \\ \hline
 \textbf{Possible Barrier}&

\textbf{Mitigation}\\ \hline
\endhead

\hline \multicolumn{2}{|r|}{{Continued on next page}} \\ \hline
\endfoot

\hline \hline
\endlastfoot


\addtocounter{barrier}{1}
\noindent
\emph{Barrier \thebarrier.}
\par \emph{Barrier 1:}

&
\noindent
XX
\end{longtable}

%\draftpage
\subsection{Measures to Maximise Impact}

The impact of \TheProject{} will be maximised by:
\begin{inparaenum}[i)]
\item
  XX
\end{inparaenum}
%
The following section describes these measures in more detail.

\subsection{Dissemination and Exploitation of Results}
\label{sect:dissemination}

\eucommentary{Provide a draft 'plan for the dissemination and exploitation of the project's results' (unless the work programme topic explicitly states that such a plan is not required). For innovation actions describe a credible path to deliver the innovations to the market. The plan, which should be proportionate to the scale of the project, should contain measures to be implemented both during and after the project.
Dissemination and exploitation measures should address the full range of potential users and uses including research, commercial, investment, social, environmental, policy making, setting standards, skills and educational training.
The approach to innovation should be as comprehensive as possible, and must be tailored to the specific technical, market and organisational issues to be addressed\\
o Explain how the proposed measures will help to achieve the expected impact of the project. Include a business plan where relevant.\\
o Where relevant, include information on how the participants will manage the research data generated and/or collected during the project, in particular addressing the following issues:\\
o What types of data will the project generate/collect? o What standards will be used? o How will this data be exploited and/or shared/made accessible for verification and re-use? If data cannot be made available, explain why.
o How will this data be curated and preserved?}



\subsubsection{Draft Dissemination Plan}

We will focus on propagating our results both to the computer science
research community and to potential users of the \TheProject{} technology.
We will do this through a mixture of high-quality publication, presentations
and direct engagement with the user community.
%
The main research communities that we expect to target are:
XX
% 
We anticipate that the primary users of our technology will be:
XX.

\paragraph{Scientific Publications:}  The main routes to good scientific dissemination are
through peer-reviewed publication, and through presentation of results at key scientific events.
\TheProject{} partners will therefore aim to produce high-quality \emph{peer-reviewed
research publications} in relevant leading
conferences, technical workshops and journals.
We will build on the existing good publication records of the \TheProject{} partners,
aiming to produce a sizeable volume of good quality publications in the course of the project. 
%
\noindent
The conferences that we propose to target include:
\vjcomment{Update these.}

\begin{quote}
\textbf{ICSE:} International Conference on Software Engineering;
 \end{quote}

\noindent
We also propose to target relevant high-impact journals such as:
\begin{quote}
\emph{the ACM Transactions on Software Engineering and Methodology (TOSEM)}, 
\end{quote}

\paragraph{Project Web site:}  
A crucial component of the \TheProject{} dissemination strategy is a
high-quality project website. The public section will provide ample
and consistent information about all aspects of the \TheProject{}
project, with the goal of positioning the \TheProject{} website as a
prime information source for relevant scientific and technical
information.  The vast majority of the \TheProject{} deliverables are
public, and full access to these will be provided.  The web site will also contain lists of publications and links
to open-access repositories; copies of technical reports and white
papers; a news feed; technical documentation; downloadable software
and pre-installed virtual machines; video demonstrations; online
tutorials; information about project partners; copies of
presentations, podcasts and other material; data and results; plus
links to the Horizon 2020 programme in general and to related 
Horizon 2020 research projects in order to highlight the role played by \TheProject{} within the
broader EC research framework.

\paragraph{Project News Feed:}  We will set up open public mailing lists/twitter/facebook accounts that will
be used to communicate project news and results to interested parties, whether they are scientists, academics, developers
or the general public.  This news feed will be highlighted on the project web site.

\paragraph{Developer and User Community:} We will engage with the broader
developer and user communities by a series of focused activities
that will include the organisation of dedicated user community workshops, presentations at
 developer and other conferences, 
 the production of posters, delivering tutorials, staffing booths, and providing hands-on
 guidance in the use of our tools and technologies etc.  This direct engagement will be
 supported by the production of video demonstrations, training materials, tutorials and documentation that can
 be accessed through the project web site.  We will also aim to produce white papers and slide sets that
 can be used to explain the benefits of the \TheProject{} approach to prospective users, both developers
 and managers.

 \paragraph{Expert Working Groups and Networks of Excellence:}
In order to ensure good dissemination to the research and development
community, including industrial researchers, we propose to disseminate
our project results through the most relevant scientific/technological
networks and working groups, including HiPeac,%  the TACLe Cost Action on Timing Analysis, the Ercim DES Working Group, 
IFIP Working Groups 2.11 \& 10.3, and other EU and national projects,
as well as national groups such as the UK's network on manycore computing.
\SAshort{} is a full member of HiPeac and a charter member of IFIP working
group 2.11, and has been heavily involved in activities organised by these and other expert groups.
These provide a high-level interface between academic and industrial interests, and a valuable
melting pot for ideas and new technologies. 

\paragraph{Standardisation Committees:}
% \khcomment{We should mention any committees that we are members of. Remove this section if not relevant.}
This proposal is tied to Standards from the beginning, through the end and beyond. It leverages the key leadership position of several members, acting as senior leaders, officers, working group experts, proposal authors, collaboration with other industry and academic experts.
\UCMshort{} is member of the ISO/IEC JTC1/SC22/WG21 (C++ standard committee) for more than a decade and they have contributed to the C++11, C++14, C++17 and C++20 standards. They also participate in the joint group with SC22/WG23 (Programming Language Vulnerabilities) where a specific guide for C++ vulnerabilities is under development.

%% Doesn't make sense?
% Current C++ Standards only support CPU, though it is adding parallel programming support starting with C++ 11, enhanced through C++ 17 with parallelSTL. Yet it is still lacking many things that can support Heterogeneous computing, which has been already explored by SYCL and OpenCL. 
%
%

\paragraph{General Scientific and Technical Community:}
We will further engage with the general scientific and technical community
 by participating in relevant workshops, conferences and clustering events (including ones which
 will develop our technologies beyond the bounds of our own research communities), by engaging with the
 different EC-sponsored CORDIS information channels, by presenting at trade
 fairs, and through other dissemination activities.  We will use materials such as presentations,
 papers,  posters, demonstrations and the project web site to do this.


\paragraph{Education:} We will target 
the educational communities through the production of relevant educational materials, that will
also have a training benefit. Young researchers, software
 developers and application programmers will learn how to
 use our software technologies in their
 respective fields. This is aligned with recent EC
 initiatives on the subject such as ``Increasing the Attractiveness
 of Science, Engineering \& Technology Careers''.
 The academic consortium partners will integrate Bachelor,
 Master, Diploma, and PhD students into the \TheProject{}
 project whenever possible, e.g.~through PhD and MSc theses, student
 projects, academic courses, and research seminars. 
 We will also take advantage of opportunities to engage with broader
 audiences through guest lectures at other institutions and summer schools etc.
 These students will
 carry the methodology and techniques of the \TheProject{}
 project into their future work places in the ICT industry.
 In that way the \TheProject{} vision and the project
 results will disseminate into many research groups and
 companies working in the field of security. 
 \paragraph{MOOC:}In order to disseminate security and the work of the consortium to the wider public a MOOC (massively open online course) will be developed to reach an audience far beyond the academic and industrial community.  MOOCs provide access, often to hundreds or thousands of people at a time, to focused online courses with learning goals and paths that present the opportunity to learn together at a flexible pace. Participants start and complete the training course on the MOOC at the same time as a cohort, to create community and for peer learning. The provision of a MOOC will also be
used to enthuse and educate citizens about security and in particular the work of the project.  Work packages will provide accessible materials (text, videos, audio clips) highlighting the work that is being carried out. UoD brings a long‐term, established partnership to a market leading MOOC, FutureLearn, which will be supported far beyond the funded period. The FutureLearn MOOC is free and open, and brings a highly scalable platform into the project ecosystem, capable of sustaining more than 100,000 learners at one time.

% \paragraph{General Communication:} We will adopt a good general communication strategy
% aimed at maximising the outreach of the \TheProject{} project to the public at large.  We will issue
% regular press releases describing relevant events and research results, conduct
% radio/TV interviews, adopt an open policy to disseminating our research results through use of appropriate
% repositories for publications and the project web site, use public media such as
% \emph{twitter} to communicate project results, engage with public lectures and seminars,
% write general articles for newsletters, newspapers etc. as described in \ref{wp:dissem}.

\begin{quote}

\emph{Although all these dissemination activities
will be centrally coordinated, the full and active participation of
all partners is expected. Key project representatives from
the various \TheProject{} academic and industrial
partners will also arrange specific meetings
with scientific, commercial, industrial, and/or
governmental representatives to facilitate public
engagement.}
\end{quote}

\subsubsection{Draft Exploitation Plan}
\label{sect:exploitation-plan}
\vspace{-12pt}

All partners will have the right to use foreground IPR for
the purposes of the project. In order to ensure that all
contributions are recognised, exploitation plans will be
shared with the consortium as a whole. Partners will not,
however, have the right to veto or delay exploitation
unless their own IPR is directly involved.


\horizontalline

\subsection*{Draft Exploitation Plan for \IBMshort{}}

\begin{wrapfigure}{R}{4cm}
\vspace{-1.4cm}
\hfill \includeimage[width=4cm]{logos/ibm.jpg}
\vspace{-0.6cm}
\end{wrapfigure}

IBM will aim to deliver \TheProject technologies into IBM cloud and analytics products and services, aiming to commercially benefit both citizens and businesses across Europe. \TheProject technologies can be relevant to improve many IBM products and services, such as:
\begin{itemize}
    \item IBM Cloud Pak for Data. It is a fully-integrated data and AI platform that modernizes how businesses collect, organize and analyze data and infuse AI throughout their organizations. Built on Red Hat OpenShift Container Platform, IBM Cloud Pak for Data integrates market-leading IBM Watson AI technology with IBM Hybrid Data Management Platform, data ops, and governance and business analytics technologies. Together, these capabilities provide the information architecture for AI that meets your ever-changing enterprise needs. The enhanced code vulnerability detection technologies combined with novel techniques for repairing the identified vulnerabilities that will be developed in the \TheProject project will significantly enrich the ability to develop secure and robust AI-driven big data applications. In addition, the IBM Cloud Pak for Data is likely to benefit from the results of \TheProject in the areas of security and protection of Big Data and new big data analytics.
    \item IBM Security Portfolio. We will aim to enrich the existing IBM security solutions by exploiting the advanced vulnerability detection and code repairing technologies that will be developed and enhanced in the \TheProject project to commercially benefit businesses across Europe.
\end{itemize}

\horizontalline

\subsection*{Draft Exploitation Plan for \SCCHshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/SCCH.jpg}
\vspace{-0.8cm}
\end{wrapfigure}

Exploitation plan for SCCH.

\horizontalline

\subsection*{Draft Exploitation Plan for \SOPRAshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/Sopra-Steria-logo2.png}
\vspace{-0.8cm}
\end{wrapfigure}

Exploitation plan for \SOPRAlong{} is to use the tools that we develop in the research and deploy it against the various software development solution the company develop for use in various private and public companies within Europe. \SOPRAshort{} has software installation spread over twenty-five counties. The company covers the business domains like Aerospace, Defence plus Security, Energy plus Utilities, Financial Services, Insurance plus Social, Government, Retail, Telecommunication, Media plus Entertainment, Transport and IT consulting via our 40K+ consultants. This new security methodology will enhance the future delivery capability of \SOPRAshort{} plus customers our consultants support within the customers own systems. The outcome of this research will generate a significant disputer in the speed we can generate new solutions while ensuring the code is secure and of good quality.

\horizontalline

\subsection*{Draft Exploitation Plan for \DEMshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/DEM-logo.jpeg}
\vspace{-0.8cm}
\end{wrapfigure}

Exploitation plan for Democritos.

XX

XX

XX

XX

XX

XX

XX

XX
\horizontalline

\subsection*{Draft Exploitation Plan for \FRQshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/FRQ_logo.png}
\vspace{-0.8cm}
\end{wrapfigure}

Exploitation plan for Frequentis.
XX
XX
XX
XX
XX
XX
XX

\horizontalline

\subsection*{Draft Exploitation Plan for \YAGshort{}}
\vspace{-6pt}

\begin{wrapfigure}{R}{3.6cm}
\vspace{-1.3cm}
\hfill \includeimage[width=4cm]{logos/YAG-logo.png}
\vspace{-0.8cm}
\end{wrapfigure}

 \YAGshort{} will deliver \TheProject technologies into YAGAAN product and services, aiming to commercially benefit businesses across Europe. The YAG-Suite is the \YAGshort{} product which supports developers in detecting vulnerabilities in their source code during their security and privacy by design development processes. \TheProject technologies can be relevant to improve YAG-Suite and the company services, such as:
\begin{itemize}
    \item YAG-Suite improvement for developers. The enhanced static code analysis and security breaches detection technologies combined with novel techniques for vulnerabilities remediation that will be developed in the \TheProject project will enrich the YAG-suite capabilities.
    \item YAG-Suite improvement for privacy assessment of big data related applications: To support GDPR compliance assessments, \TheProject technologies will significantly enrich the YAG-Suite for \YAGshort{} to offer new source code privacy breaches detection capabilities and support European businesses in delivering better privacy compliant applications to the EU citizens. Additionnally \TheProject will support \YAGshort{} in accessing the new market segment of big data application producers.
    \item YAG-Suite improvement for code auditors: As YAG-Suite not only is used by developers but also by service providers who deliver source code audits, the same capabilities will also benefit to audit service providers.
\end{itemize}

\horizontalline

\pagebreak

\subsection*{Draft Exploitation Plan for \COGNIshort{}}
\vspace{-6pt}

%\begin{wrapfigure}{R}{3.6cm}
%\vspace{-1.3cm}
%\hfill \includeimage[width=4cm]{logos/YAG-logo.png}
%\vspace{-0.8cm}
%\end{wrapfigure}

Exploitation plan for Cognitive

\horizontalline

\subsection*{Draft Exploitation Plan for \SAshort{}}

\begin{wrapfigure}{R}{2cm}
\vspace{-1.4cm}
\hfill \includeimage{logos/st-andrews-logo.jpg}
\vspace{-0.9cm}
\end{wrapfigure}

\SAshort{} will work to actively exploit the foreground IPR that it produces in the course of the \TheProject{} project. During the course of the \TheProject{} project, we envisage the development and production of new and innovative refactorings for increasing the security level of big-data applications, together with new verification techniques to show general soundness and correctness of the refactorings. The intention is to release these tools as open-source software, allow their widespread exploitation by the community. We will also work with the industrial collaborators to further exploit these tools and other foreground intellectual property that we will produce in the course of the project. 
Additionally, \SAshort{} will actively pursue avenues of commercial exploitation with the possible formation of a spinoff company to commercialse the project results, and apply for patents, etc. Christopher Brown has already startup experience, with the former spinout of ParaFormance technologies Ltd. from previous EU research on the H2020 RePhrase project. Scotland provides an excellent and vibrant base for commercial exploitation, with additional support available from the Scottish Enterprise and other sources.  Edinburgh alone, 162 start-up/spin-off companies have been formed from academic research since 2007, and two new science parks are currently being developed. Edinburgh TechnoPole is a new 126-acre science and technology park that offers flexible accommodation to new and growing high-technology businesses. At St Andrews, the new Eden business campus will provide an excellent base for growing the new startup company. USTAN has excellent connections within the UK and Scottish communities.

\horizontalline

\subsection*{Draft Exploitation Plan for \UODshort{}}

\begin{wrapfigure}{R}{3cm}
\vspace{-1.4cm}
\hfill \includeimage[scale=0.25]{logos/UOD-logo.png}
\vspace{-0.9cm}
\end{wrapfigure}

University of Dundee will actively work on promoting and exploiting the tools and techniques produced by the \TheProject{} project. School of Science and Engineering at the University of Dundee has a long experience in Data Science and Big Data Analytics, being the first school in the UK that had an MSc programme in Data Science. Over the course of the previous projects, \UOD has established an active collaboration with big data industry, collaborating with Teradata  on Proteomic research along with open source solutions such as Cassandra and Neo4j. In addition there are strong links to the games industry in Scotland with companies such as Ninja Kiwi and Outplay and also Find my Past which hosts over 4 billion searchable records of census, directory and historical record information. These links will be used to promote the results of the \TheProject{}. \UOD also has a long-standing collaboration with the Health Informatics Centre (also a part of the University of Dundee and tightly linked with National Health Service Scotland), which is recognised in Scotland as a leader in health data linkage. This is the area where ensuring security of access and protecting privacy of the data is absolutely crucial, and opens another avenue for testing and exploitation of the \TheProject{} technologies, both on a repository of anonymised sensitive eHealth data that HIC owns and which covers approximately 20\% of the Scottish population, and, where feasible, on real data in co-operation with local company Waracle who specialise in assisting the NHS in data collection. The School of Science and Engineering is also one of the main hubs in Scotland for Human-Computer Interaction and it operates a User Centre, which is used to test and evaluate research tools and technologies on various classes of users. This gives us a potentially large base of users to test e.g. authentication mechanisms developed over the course of the project. There are also numerous other projects currently running at the University of Dundee, especially in the area of application of machine learning to analyse medical images, where the output of \TheProject{} can be fed to further exploitation, providing additional assurance in security and privacy of the data. Finally, we will also look into creation of spin-off companies based on the technologies produced over the course of the project, seeing the \TheProject{} technologies as an essential layer of security for big data analytics in future.

On the education side, there is a huge potential of incorporating the results of the \TheProject{} project into the teaching curriculum at \UOD. There are many undergraduate and postgraduate modules that teach data analytics, and we also have dedicated MSc programs on Data Engineering, Data Science and Health Data Science for Applied Precision Medicine which can be expanded with the research output and technologies developed in \TheProject{}. Furthermore, we will look into developing new MSc courses on Security of Big Data Analytics and new interdisciplinary courses on applications of Big Data technologies into other areas, such as Medicine Life Sciences and Citizen Science and Observatories and we will investigate feeding the results of \TheProject{} into these new programs. \TheProject{} also gives opportunities for new research at the PhD level, investigating particular topics that the project deals with.

\vjcomment{Add something about MOOCs}

\accomment{Do we need hyperlinks to futurelearn and the DataLab?}

A Massive Open Online Course (MOOC)
is a recent (since 2008) innovation in education and learning. MOOC’s provide access, often to hundreds or thousands of people across the EU and the World, to focused online courses with learning goals and paths that present the opportunity to learn together at a flexible pace. UOD already has a strong collaboration with FutureLearn, which is the leading global MOOC 2.0 provider (an inclusive, open, sustainable and collaborative approach)  and can bring in vast experience from the Grow Observatory, WeObserve and the "Data Science in the Games Industry" MOOC for Scotland's DataLab.  The FutureLearn MOOC is free and open and brings a highly scalable platform into the \TheProject{} 
ecosystem, capable of sustaining more than 100,000 learners at one time. The FutureLearn platform will provide metrics and data as evidence of numbers of participants, progress through courses, data submission and completion and interpret data for \TheProject{} reports and deliverables.


\horizontalline

\subsection*{Draft Exploitation Plan for \UCMshort{}}

\begin{wrapfigure}{R}{5.5cm}
\vspace{-1cm}
\hfill \includeimage[scale=0.2]{logos/UC3M-logo.png}
\vspace{-0.9cm}
\end{wrapfigure}

\UCMshort{} will exploit the results from the project in two main areas:
the definition and tool support of coding standards and
the C++ language extension for contract based programming.
The new security centric codign guideline and the supporting toolset will
open new market opportunities that will be investigated.
Addtionally, the contract based programming insights for the project
will enrich the feedback that \UCMshort{} will provide in the 
ISO C++ standards committee and its study group on contracts which
is currently targeting the ISO/IEC 14882:2023 standard.

In addition, the participation in the project will strengthen the ability of
\UCMshort{} to open new cooperations in the area of cybersecurity for big data
analytics. On one hand, the support of coding guidelines may be of major
interest in the financial sector, where they have had several cooperations with
BBVA (one of the major banks in Spain).  On the other hand, a better suppor fo
contract based programming also allow to reinforce \UCMshort's current position
in the aerospace industry where it currently cooperates with major players in
the Spanish market, including GMV or SENR. In both cases, \UCMshort{} will be
able to improve its offerings with less effort devoted to security increasing
its competitiveness.





\subsubsection{Knowledge Management and Protection}
\vspace{-12pt}

\eucommentary{Outline the strategy for knowledge management and protection. Include measures to provide open access (free on-line access, such as the 'green' or 'gold' model) to peer-reviewed scientific publications which might result from the project}

Before the project starts, all project partners will agree on explicit rules concerning IP ownership, access rights to any
Background and Results for the execution of the project and the
protection of intellectual property rights (IPRs) and confidential
information as part of the Consortium Agreement.
As part of the Consortium Agreement, in order to ensure a smooth
execution of the project, the project partners will agree to grant each other
royalty-free Access Rights to their Background and Results for the
execution of the project. The Consortium Agreement will define further
details concerning the Access Rights after the duration of the project 
with respect to Background and Results.

\paragraph{Dissemination and Communication:}
While fully taking into account issues of potential exploitation and IPR ownership by project partners
as governed by the Consortium Agreement,
the project aims to provide good general access to its research results.
Balancing access with cost, the project will therefore generally adopt a ``green'' model to open access for publications,
but has included funding to support targeted ``gold'' open access for key publications.
The academic partners all maintain suitable institutional repositories, which will allow public access to research papers produced in the
course of the project, perhaps with some moratorium.  Some publishers (e.g. the ACM) also provide links that allow
free access to their publications from authors' home pages, and this will be exploited wherever possible.
% y, where publisher charges are not excessive, the project will consider ``gold'' open access to key project publications.
%
Furthermore, and perhaps of most significance, the project website will provide free,
open and publicly searchable access to all the public deliverables, to technical reports, data and results, to software tools
and libraries, to white papers and also to all
the other non-confidential documents that are generated in the course of the project.  
% The material released in this way will represent the vast bulk of the scientific
% and technical output of the project.

%\subsubsection*{Intellectual Property Rights}

\pagebreak
\paragraph{IP Ownership.}

Results shall be owned by the project partner carrying out the work
leading to such Results. If any Results are created jointly by at least
two project partners and it is not possible to distinguish between the
contributions of each of the project partners, such Results, including
inventions and all related patent applications and patents, will be
jointly owned by the contributing project partners. In order to further
the competitiveness of the EU market, and to enhance exploitation of the
Consortium Results, each contributing party shall have full own freedom
of action to exploit the joint IP as it wishes, and further the goals of
the consortium. To promote this effort the contributing party will have
full own consideration regarding their use of such joint Results and will
be able to exploit the joint IP without the need to account in any way to
the other joint contributor(s).Further details concerning jointly owned
Results, joint inventions and joint patent applications will be addressed
in the Consortium Agreement.

\paragraph{Transfer of Results.}

As Results are owned by the project partner carrying out the work leading
to such Results, each project partner shall have the right to transfer
Results to their affiliated companies/organisations without prior notification to the
other project partners, while always protecting and assuring the Access
Rights of the other project partners.  Such use of Results will encourage
competitiveness of the EU market by creating broader uses of the Results
and opening up the markets for the Consortium's Results in all markets.

\paragraph{Open Source and Standards.}

A central aim of this consortium is to provide benefit to the European community.  Some of the project partners may be either using Open Source code in their deliverables or contributing their deliverables to the Open
Source communities. Alternatively, some of the partners may be contributing to Standards, be they open standards or other. Details concerning open source code use and standard contributions will be
addressed in the Consortium Agreement.

The base technologies being developed by the academic partners within the duration of the project will be published under Open Source Licenses except where specified under Intellectual Property Management to allow the broader community to benefit from the outcome of this project.

The major validated project results  will be contributed to the key international standardisation bodies  such as ISO, ITU and others where the consortium members take part in. 


\paragraph{Data Management Plan}
The primary research data that will be produced by the project will be the performance results that are reported in
various research publications.  This data will predominantly be scientific,
without confidentiality restrictions, and it will
therefore be made available through the project website, in line with the agreements that will be
made in the Consortium Agreement.  As far as possible, 
this data will be recorded in a human-readable form, such as plain ASCII text
or XML.  Where this is not possible, converters will be provided to make the data accessible to other researchers
in a human-readable form. 
The research data on the project website will be associated with the relevant research publications. 
We have budgeted for adequate disk and processing capacity to allow for the expected access to this data.
The project website will be maintained after the end of the project, but to ensure long-term continuity
and value, data will also be transferred to the \SAshort{} institutional data repository.

\draftpage
\subsubsection{Communication Activities}
\label{sect:comm-activities}

\eucommentary{Describe the proposed communication measures for promoting the project and its findings during the period of the grant. Measures should be proportionate to the scale of the project, with clear objectives. They should be tailored to the needs of various audiences, including groups beyond the project's own community. Where relevant, include measures for public/societal engagement on issues related to the project.}

As described in the draft dissemination plan above, and in the description of~\ref{wp:dissem} (page~\pageref{wp:dissem}),
the \TheProject{} project aims to communicate itself and its findings intensively to various communities.
Research publications and presentations will aim to target various groups of academic and industrial researcher,
including parallel programmers, big data researchers, and programming language
designers, and will add scientific weight and credibility to our findings.  Press releases and news articles will be used to communicate project results and major
project life events (start, finish, key milestones) to both a technical and general audience.
We will also take advantage of opportunities as they arise for radio/TV interviews, public seminars and general articles
in both the technical and non-technical press.
The project website will be used to provide open access to project results, public deliverables,
software tools, technical reports, white papers, (video) tutorials, podcasts etc., and will serve
as a key resource for those wishing to use the project results, whether they are acting as an academic researcher, scientific, commercial or independent
software developer, public sector worker,  educator or private individual.
By making research results public in this way, we especially aim to engage with the software developer
community, who may not normally have access to academic papers and reports.
We will disseminate information about our tools and standards directly to customers, aiming to
increase engagement with an already motivated group of developers/users.
We will run open technical workshops that will showcase our work to interested parties.
These will generally be co-located with major networking events, such as the annual HiPeac
conference and the HiPeac spring/autumn gatherings.
We will also engage with relevant industrial/developer conferences, grass-roots meetings, workshops etc.,
producing poster and demonstrations as necessary to communicate with the broader developer community
and especially with project managers and decision makers.
Finally, we will actively participate in standardisation activities through e.g. the ISO C++ standard committee and ITU FG-DPM to support IoT and Smart Cities and Communities,
aiming to influence development and awareness of the \TheProject{} results.

\clearpage

% ---------------------------------------------------------------------------
%  Section 3: Implementation
% ---------------------------------------------------------------------------


\clearpage
\section{Implementation}

\subsection{Work Plan --- Work Packages, Deliverables and Milestones}
\label{sect:workplan}


\begin{figure}[tp]
\begin{center}
\vspace{-5mm}
\begin{tabular}{ll}
\hspace{-0.75in}
\includeimage[scale=0.5]{PertChart.pdf}
\vspace{-10mm}
\end{tabular}
\caption{Overview of the \TheProject{} Workpackage Structure and Dependencies (PERT chart)}
\label{fig:wps}
\end{center}
\end{figure}

\subsubsection*{Overall Structure of the Work Plan}

The work plan is broken down into 6 technical work packages as shown
in \textbf{Figure~\ref{fig:wps}}: WP2 deals with fundamental techniques for vulnerabilities detection and self healing, WP3 deals with application of these techniques in multi-layer AI-based big data analysis applications, WP4 deals with formal specification and proving of the security and privacy properties, WP5 deals with intelligent user authentication, WP6 deals with the \TheProject{} methodology for development of secure and privacy preserving applications and WP7 deals with use cases and evaluation of the project tools, technologies and methodology. In addition, WP1 deals with the management of the project and WP8 deals with dissemination, exploitation, community building and promotion of the project.

\input{deliverables}

\bigskip\bigskip
\addtocounter{subsubsection}{1}
\addcontentsline{toc}{subsubsection}{\protect\numberline{\thesubsubsection}Work
Package List}
\fbox{\begin{minipage}{\textwidth}\begin{center}{\Large\bf
        Work package list} % (full duration of project)}
  \end{center}
  \end{minipage}}

\bigskip\bigskip

\begin{tabular}{|p{1.2cm}|p{9cm}|p{0.8cm}|p{1.35cm}|p{1cm}|p{0.9cm}|p{0.9cm}|}
\hline
{\bf Work \mbox{package} No} & {\bf Work package title} &
{\bf Lead \mbox{partic.} no.} &
{\bf Lead short name} &
{\bf Person months} & {\bf Start month} & {\bf End month} \\\hline 

\newcounter{wp}

\addtocounter{wp}{1}
\workpackageentry{\thewp}{USTAN}{24}{1}{36}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{IBM}{XX}{XX}{XX}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{SCCH}{XX}{XX}{XX}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{USTAN}{XX}{XX}{XX}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{COGNI}{XX}{XX}{XX}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{UCM}{XX}{XX}{XX}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{SOPRA}{XX}{XX}{XX}
\addtocounter{wp}{1}
\workpackageentry{\thewp}{UOD}{XX}{XX}{XX}

{\textbf{Total}} & & & &
\textbf{\large XX}&
&
\\\hline
\end{tabular}

\landscape

\subsubsection*{Work Plan Timing: GANTT Chart showing Task Dependencies and Information Flows}


%\vspace{-0.7in}
\centerline{\hbox to \columnwidth{\hss%
    \includeimage[scale=0.85]{Gantt.pdf}
\hss}}
\label{fig:gantt}
\vspace{-1in} % Fool LaTeX into avoiding unnecessary page break
\endlandscape

\newpage
%\bigskip\bigskip\bigskip

%% Set up the milestone numbers.
\input{milestones}

\fbox{\begin{minipage}{\textwidth}\begin{center}\Large\bf List of Milestones
  \end{center}
  \end{minipage}}
\label{sect:milestones}

\bigskip

\newcounter{ms}
\renewcommand{\thems}{MS\arabic{ms}}
\begin{minipage}{\textwidth}
\begin{center}
 \begin{tabular*}{\textwidth}{|p{1cm}|p{10.3cm}|p{1.2cm}|p{0.6cm}|p{2.7cm}|}  \hline
 \textbf{MS} & \textbf{Milestone name} & \textbf{Related WPs} & \textbf{Est. date} & \textbf{Means of
   verification} \\ % (success criteria below)} \\ % (deliverables shown here + success criteria below) \\
\hline
\ref{mil:req1} & Initial Requirements Analysis & WP7 & M3 & \ref{del:req1} \\
   \hline
 \ref{mil:dmp} & Initial Data Management Plan & WP8 & M6 & \ref{del:data-mgt-plan} \\
   \hline
\ref{mil:auth1} & User Authentication Mechanisms for Private Clouds & WP5 & M9 & \ref{del:auth1} \\
   \hline
   \ref{mil:vul1} & Initial Vulnerabilities Identification and Self-Healing Framework & WP2 & M10 & \ref{del:vul1} \\
   \hline
   \ref{mil:bigdata1} & Formally-Verifiable Security and Privacy of Big Data Analytics for Private Clouds & WP3, WP4 & M11 & \ref{del:bigdata1}, \ref{del:formal1} \\
   \hline
   \ref{mil:meteval1} & Initial \TheProject{} Methodology; Application and Evaluation on Initial Uses Cases for Private Clouds & WP6, WP7 & M12 & \ref{del:met1}, \ref{del:cs1}, \ref{del:eval1} \\
   \hline
   %\ref{mil:met1} & \TheProject{} Methodology for AI-Based Big Data Analytics on Private Clouds & WP6 & M12 & \ref{del:met1}, \ref{del:cs1} \\
   %\hline
   %\ref{mil:eval1} & Use Cases and Evaluation of \TheProject{} Technology for Private Clouds & WP7 & M13 & \ref{del:eval1} \\
   %\hline
   \ref{mil:req2} & Final Requirements Analysis & WP7 & M15 & \ref{del:req2} \\
   \hline
   \ref{mil:auth2} & User Authentication Mechanisms for Public Clouds & WP5 & M22 & \ref{del:auth2} \\
   \hline
   \ref{mil:mooc} & First run of Code Security MOOC & WP8 & M22 & \ref{del:mooc} \\
   \hline
   \ref{mil:vulbigdata2} & Refined Vulnerabilities Identification and Self-Healing Framework, Formally-Verifiable Security and Privacy of Big Data Analytics for Public Clouds & WP2 -- WP4 & M24 & \ref{del:vul2}, \ref{del:bigdata2}, \ref{del:formal2} \\
   \hline
   \ref{mil:meteval2} & Refined \TheProject{} Methodology; Application and Evaluation on Refined Use Cases for Public Clouds & WP6, WP7 & M25 & \ref{del:met2}, \ref{del:cs2}, \ref{del:eval2} \\
   \hline
%   \ref{mil:met2} & \TheProject{} Methodology for AI-Based Big Data Analytics for Public Clouds & WP6 & M25 & \ref{del:met2}, \ref{del:cs2} \\
 %  \hline
  % \ref{mil:eval2} & Use Cases and Evaluation of \TheProject{} Technology for Public Clouds & WP7 & M26 & \ref{del:eval2} \\
   %\hline
   \ref{mil:authvulbigdata3} & Final Versions of \TheProject{} Technologies for Public Clouds & WP2 -- WP5 & M34 & \ref{del:vul2}, \ref{del:bigdata3}, \ref{del:formal3}, \ref{del:auth3} \\
   \hline
   \ref{mil:meteval3} & \TheProject{} Methodology, Use Cases and Evaluation for Hybrid Clouds; \TheProject{} Roadmap; End of Project & WP6, WP7 & M36 & \ref{del:met3}, \ref{del:cs3}, \ref{del:eval3} \\
   \hline
%   \ref{mil:authvulbigdata3} & User Authentication Mechanisms for Hybrid Clouds & WP5 & M34 & \ref{del:auth3} \\
  % \hline
%   \ref{mil:vulbigdatameteval3} & \TheProject{} Technologies and Methodology for Hybrid Clouds; Use Cases and Evaluation on Public Clouds; End of the Project & WP2 -- WP4, WP6, WP7 & M36 & \ref{del:vul3}, \ref{del:bigdata3}, \ref{del:formal3}, \ref{del:met3}, \ref{del:cs3}, \ref{del:eval3} \\
  % \hline
   
\end{tabular*}
\end{center}
\end{minipage}

\setcounter{ms}{0}
\vspace{20pt}
\begin{center}
\begin{tabular*}{\textwidth}{|p{1.2cm}|p{13.3cm}|p{2.2cm}|}\hline
\textbf{MS No.} & \textbf{Success Criteria} & \textbf{Contributes to
  Objective(s)} \\
  \hline
\ref{mil:req1} & Initial Requirements and Dependencies for Tools and Use Cases Identified & \textbf{1 -- 8} \\
  \hline
 \ref{mil:dmp} & Gathering of likely data outputs from project & \textbf{ 8} \\
  \hline
\ref{mil:auth1} & Initial Authentication Mechanisms for AI-Based Big Data Processing on Private Clouds Developed & \textbf{5} \\
\hline
\ref{mil:vul1} & Initial Static, Dynamic and Runtime Analysis and Self-Healing Methods Developed & \textbf{2, 3} \\
\hline
\ref{mil:bigdata1} & Vulnerabilities Detection and Self Healing Methods Applied and Verified for AI-Based Big Data Analytics on Private Clouds & \textbf{1 -- 3} \\
\hline
\ref{mil:meteval1} & Initial \TheProject{} Methodology Developed, Applied and Evaluated on the Initial Versions of Use Cases on Private Clouds & \textbf{4, 6, 7, 8} \\

%\ref{mil:met1} & \TheProject{} Methodology for Developing Secure AI-Based Data Analytics Code for Private Clouds Developed & \textbf{4, 6} \\
%\hline
%\ref{mil:eval1} & \TheProject{} Methodology Applied and Evaluated for Initial Versions of Use Cases for Private Clouds & \textbf{7, 8} \\
%\hline
\ref{mil:req2} & Final Requirements and Dependencies for Tools and Use Cases Identified & \textbf{1 -- 8} \\
  \hline
\ref{mil:auth2} & Refined Authentication Mechanisms for AI-Based Big Data Processing on Public Clouds Developed & \textbf{5} \\
\hline
\ref{mil:mooc} & MOOC developed bu UOD and approved by Futurelearn & \textbf{5, 8} \\
\hline
\ref{mil:vulbigdata2} & Different Analysis and Self-Healing Methods Combined, Applied and Verified for AI-Based Big Data Analysis on Public Clouds & \textbf{1 -- 3} \\
\hline
\ref{mil:meteval2} & Refined \TheProject{} Methodology Developed, Applied and Evaluated on Refined Versions of the Use Cases for Public Clouds & \textbf{4, 6, 7, 8} \\
\hline
%\ref{mil:met2} & \TheProject{} Methodology for Developing Secure AI-Based Data Analytics Code for Public Clouds Developed & \textbf{4, 6} \\
%\hline
%\ref{mil:eval2} & \TheProject{} Methodology Applied and Evaluated for Refined Versions of Use Cases for Public Clouds & \textbf{7, 8} \\
%\hline
\ref{mil:authvulbigdata3} & Final \TheProject{} Technologies for AI-Based Big Data Processing on Hybrid Clouds Developed & \textbf{1 -- 5} \\
\hline
\ref{mil:meteval3} & Final \TheProject{} Methodology for AI-Based Big Data Analytics on Hybrid Clouds Developed, Applied and Evaluated on the Final Versions of Use Cases on Hybrid Clouds; The Project Roadmap Produced; & \textbf{4, 6, 7, 8} \\
\hline

%\ref{mil:auth3} & Final Authentication Mechanisms for AI-Based Big Data Processing on Hybrid Clouds Developed & \textbf{5} \\
%\hline
%\ref{mil:vulbigdatameteval3} & Final Technologies and \TheProject{} Methodology for AI-Based Big Data Analytics on Hybrid Clouds Developed, Applied the Final Versions of Use Cases and Evaluated; The Project Roadmap Produced; & \textbf{1 -- 8} \\
%\hline
  
\end{tabular*}
\end{center}

%\landscape
\newpage
\fbox{\begin{minipage}{\textwidth}\begin{center}\Large\bf List of Deliverables
  \end{center}
  \end{minipage}}
\label{sect:deliverables}

\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{|p{0.8cm}|p{9.65cm}|p{0.8cm}|p{1.15cm}|p{1.2cm}|p{0.8cm}|p{0.8cm}|}  \hline
\textbf{Del. no.}              & \textbf{Deliverable name}        & \textbf{WP no.} & \textbf{Lead}
& \textbf{Type}              & \textbf{Dis. level}   & \textbf{Del. date}
\\ \hline
% Year 1
\ref{mgt:mailinglists}           & Internal and public mailing lists
                                                                  & WP1 &\coordshort{} & OTHER & CO &  1 \\
\hline \ref{mgt:swrepository} & Internal software repository & WP1 & \coordshort{} & OTHER & CO & 1 \\
\hline \ref{del:req1} & Report on Initial Requirements for \TheProject{} Techniques & WP7 & \SOPRAshort{} & R & PU & 3 \\
\hline \ref{del:pressrelease1} & Press Release Announcing Start of \TheProject{} & \ref{wp:dissem} & \SAshort{} & DEC & PU & 3 \\
\hline \ref{del:website1} & Initial Project Website / Presentation & \ref{wp:dissem} & \UODshort{} & DEC & PU & 3 \\
\hline \ref{del:data-mgt-plan} & Data Management Plan & WP8 & \coordshort{} & R & CO & 6 \\
\hline \ref{del:auth1} & Report on Initial User Authentication Techniques for Data Analytics on Private Clouds & WP5 & \COGNIshort{} & R & PU & 9 \\
\hline \ref{del:vul1} & Software on Initial Techniques for Vulnerabilities Detection and Self Healing & WP2 & \IBMshort{} & OTHER & CO & 10 \\
\hline \ref{del:bigdata1} & Report on Vulnerabilities Detection and Self-Healing for AI-Based Big Data Analytics on Private Clouds & WP3 & \UODshort{} & R & PU & 11 \\
\hline \ref{del:formal1} & Report on Formal Mechanisms for Verifying Security Properties on Private Clouds & WP4 & \SCCHshort{} & R & PU & 11 \\
\hline \ref{mgt:periodic-rep-1} & Project Periodic Report (first year) & WP1 & \coordshort{} & R & CO & 12 \\
\hline \ref{del:dissemplan1} & First Interim Report on Dissemination and Exploitation & \ref{wp:dissem} & \UODshort{} & R & PU & 12 \\
% Year 2
\hline \ref{del:met1} & Report on \TheProject{} Methodology for Developing Secure Big Data Analytics on Private Clouds & WP6 & \YAGshort{} & R & PU & 13 \\
\hline \ref{del:cs1} & Initial \TheProject{} Coding Standards & WP6 & \UCMshort{} & R & PU & 13 \\
\hline \ref{del:eval1} & Report on Initial Implementation and Evaluation of Use Cases & WP7 & \FRQshort{} & R & PU & 14 \\
\hline \ref{del:req2} & Report on Final Requirements for \TheProject{} Techniques & WP7 & \SOPRAshort{} & R & PU & 15 \\
\hline \ref{del:auth2} & Software on Refined User Authentication Techniques for Data Analytics on Public Clouds & WP5 & \COGNIshort{} & OTHER & PU & 22 \\
\hline \ref{del:mooc} & Report on first run of Code Security MOOC & WP8 & \UODshort{} & R & CO & 24 \\
\hline \ref{mgt:periodic-rep-2} & Project Periodic Report (second year) & WP1 & \coordshort{} & R & CO & 24 \\
\hline \ref{del:vul2} & Report on Refined Techniques for Vulnerabilities Detection and Self Healing & WP2 & \IBMshort{} & R & PU & 24 \\
\hline \ref{del:bigdata2} & Software on Vulnerabilities Detection and Self-Healing for AI-Based Big Data Analytics on Public Clouds & WP3 & \SCCHshort{} & OTHER & CO & 24 \\
\hline \ref{del:formal2} & Report on Formal Mechanisms for Verifying Security Properties on Public Clouds & WP4 & \SA{} & R & PU & 24 \\
\hline \ref{del:dissemplan2} & Second Interim Report on Dissemination and Exploitation & \ref{wp:dissem} & \UODshort{} & R & PU & 24 \\
% Year 3
\hline \ref{del:met2} & Report on \TheProject{} Methodology for Developing Secure Big Data Analytics on Public Clouds & WP6 & \SAshort{} & R & PU & 25 \\
\hline \ref{del:cs2} & Refined \TheProject{} Coding Standards & WP6 & \UCMshort{} & R & PU & 25 \\
\hline \ref{del:eval2} & Report on Refined Implementation and Evaluation of Use Cases & WP7 & \SOPRAshort{} & R & PU & 26 \\
\hline \ref{del:auth3} & Report on Final User Authentication Techniques for Data Analytics on Hybrid Clouds & WP5 & \COGNIshort{} & R & PU & 34 \\
\hline \ref{mgt:periodic-rep-3} & Project Periodic Report (third year) & WP1 & \coordshort{} & R & CO & 36 \\
\hline \ref{del:vul3} & Report on Final Techniques for Vulnerabilities Detection and Self Healing & WP2 & \IBMshort{} & R & PU & 36 \\
\hline \ref{del:bigdata3} & Report on Vulnerabilities Detection and Self-Healing for AI-Based Big Data Analytics on Hybrid Clouds & WP3 & \SCCHshort{} & R & PU & 36 \\
\hline \ref{del:formal3} & Software on Formal Mechanisms for Verifying Security Properties on Hybrid Clouds & WP4 & \SA{} & OTHER & PU & 36 \\
\hline \ref{del:met3} & Software on \TheProject{} Methodology for Developing Secure Big Data Analytics on Hybrid Clouds & WP6 & \YAGshort{} & OTHER & CO & 36 \\
\hline \ref{del:cs3} & Final \TheProject{} Coding Standards & WP6 & \UCMshort{} & R & PU & 36 \\
\hline \ref{del:eval3} & Report on Final Implementation and Evaluation of Use Cases and Project Roadmap & WP7 & \FRQshort{} & R & PU & 36 \\
\hline \ref{del:pressrelease2} & Final Press Release Describing the \TheProject{} Results & \ref{wp:dissem} & \SAshort{} & DEC & PU & 36 \\
\hline \ref{del:website2} & Final Project Website / Presentation & \ref{wp:dissem} & \UODshort{} & DEC & PU & 36 \\
\hline \ref{del:dissemplan3} & Final Report on Dissemination and Exploitation & \ref{wp:dissem} & \UODshort{} & R & PU &  36 \\

\hline
\end{tabular}
\end{center}
\end{minipage}


%\comment{JB}{Should we assume that there will be only one review in M18 rather than a yearly one?}
%\comment{CB}{Yes, I think so. In H2020 there are two periods. But the PO might want yearly reviews, negotiated on funding stage.}

\input{WPs/WPs}
\input{WPs/WP3_SCCH}


% \TODO{Milestones need to be discussed and then described here.}

\bigskip\bigskip\bigskip
%\draftpage
\pagebreak
\fbox{\begin{minipage}{\textwidth}

\begin{center}\Large\bf
Critical Risks for Implementation
\label{sect:risks}
\end{center}
\end{minipage}}

\bigskip
Steps have already been taken to reduce the level of risk within the overall implementation plan.  The table below
identifies the main residual risks that are foreseen.  This register will be maintained
and updated as necessary during the project in order to minimise risk and so to maximise its successful completion.

\bigskip

%\begin{tabular}{| p{3.2cm} | p{1.8cm} | p{1.5cm} | p{10.3cm}  |}  \hline
%\begin{longtable}{| p{3.2cm} | p{1.8cm} | p{1.5cm} | p{10.3cm}  |}  \hline
\begin{longtable}{| p{3.5cm} | p{1.5cm} | p{11.8cm}  |}  \hline
\textbf{Description of risk} & \textbf{WPs\newline involved} & \textbf{Proposed Risk-mitigation measures} \\ \hline
\multicolumn{3}{l}{\ }
\\\hline

Disagreement over task requirements/implementation.
\par
(\textbf{Likelihood: Low}
\par
\textbf{Severity: Medium})
& ALL &
A conflict resolution mechanism has been defined that can be used to resolve
problems as they arise (see Page~\pageref{conflict-resolution}).
Most of the partners have worked together
successfully in previous projects, there will be regular technical
and management meetings, and the Project Coordinator
is actively involved in all but one work package.  These measures will help ensure
good working relationships between project partners.

\\\hline
Non-performance of a partner.
\par
(\textbf{Likelihood: Low}
\par
\textbf{Severity: High}
)
& ALL & 
Where skills overlap, 
effort will be redeployed to other partners;
otherwise the tasks may be scaled back, if possible; or,
if necessary, new partners with required competencies will be
incorporated into the project.

\\\hline
Failure to achieve key project objectives.
\par
(\textbf{Likelihood: Low}
\par
\textbf{Severity: High}
)
& ALL & 
In order to make the major progress that is required,
\TheProject{} incorporates leading-edge
research and complex software development, whose absolute success cannot be predicted. 
All partners will, however, strive to achieve project objectives
to the best of their ability.  Based on our results to date, and taking into account
the excellent technical and research track records of each of
the partners, it appears entirely feasible to achieve not only our general
technical research objectives, but also all the specific objectives
of the work packages.  In the event that it
proves impossible to achieve some specific objective within the
scope of the project, we will firstly attempt to reallocate
resources to ensure that all objectives are obtained, then
to prioritise objectives so that the most critical objectives are achieved,
and finally, if absolutely necessary, we will scale down our technical objectives, by
relaxing or deleting some part of those objectives as required
to achieve success. 

\\\hline
Security and privacy vulnerabilities arise from the internals of the libraries for storage and processing of big data.
\par
({\textbf{Likelihood: Low}}
\par
{\textbf{Severity: Medium}})
& WP3 &
Using patter-based design and implementation approach, the main goal of \TheProject{} is to identify vulnerabilities arising from the interaction of different layers (end-user application layer, machine learning library layer, data processing layer and data storage layer) of a typical big-data analytics application. Therefore, we do not aim to extensively test the internals of libraries used at each layer due to complexity of their code, but rather to treat them as black boxes. It is, however, possible that the majority of vulnerabilities in the end-user application will come from the internal functioning of layers, rather than the interaction between libraries at different layers. This is, however, not likely as we will use tried-and-tested libraries that have been in use for long time. Should this still turn out to be the case, we will redeploy the effort into analysing internals of the considered libraries instead of their composition. This would likely result in us being able to detect and repair fewer vulnerabilities, but it would still allow us to make significant improvement in security compared to the state-of-the-art.
 
\\\hline
Inability of the generic tools for vulnerability detection and self-healing to detect/repair vulnerabilities in big-data analytics applications
\par
({\textbf{Likelihood: Low}}
\par
{\textbf{Severity: High}})
& WP2, WP3 &
WP2 will develop generic techniques for discovering and repairing security and privacy vulnerabilities in distributed applications, focusing on analysing the source code of such applications and profiling their execution. WP3 will apply these techniques for the problem of big-data analytics. It is possible that the techniques from WP2 will not be able to detect majority of vulnerabilities when applied to the specific problem that WP3 considers, due to sheer scale of the computations and unpredictability of the interactions between storage and processing agents. This is unlikely to happen, as the techniques in WP2 will be designed with the scaling in mind and will be adapted to the non-determinism that parallel computations exhibit. In the case that it still happens, we will redeploy effort from other work packages (specifically WP2 and WP3) to further adaptation of the core techniques. 
\\\hline
\end{longtable}
%\newpage

%\vspace{-6pt}
\subsection{Management Structure and Procedures (Figure~\ref{fig:management})}
\label{sect:mgt}

\eucommentary{Describe the organisational structure and the decision-making ( including a list of milestones (table 3.2a)).\\
Explain why the organisational structure and decision-making mechanisms are appropriate to the complexity and scale of the project.\\
Describe, where relevant, how effective innovation management will be addressed in the management structure and work plan.\\
Describe any critical risks, relating to project implementation, that the stated project's objectives may not be achieved. Detail any risk mitigation measures. Please provide a table with critical risks identified and mitigating actions (table 3.2b).}

Responsibility for the overall management and technical
direction of the project will rest with the \emph{Project Coordinator}
(Dr Juliana Bowles, \SA{}) who will be the primary point of
contact with the European Commission. Responsibility for
individual work packages will rest with the \emph{Work Package Team
Leader (WTL)} identified below, who will report to the Project Coordinator.
Where a work package is split across more than one
institution, the day-to-day management of each task will be handled
locally, with the task manager reporting to the WTL.   
% Disputes
% will be resolved at the lowest possible level by an independent
% adjudicator (for disputes between WTLs this will be the Project
% Coordinator, unless he is involved in the dispute).
%
In order to ensure good integration of the project and sound
overall management, the Project Coordinator will convene
annual technical workshops containing representatives from
the entire project team.   These workshops will be open to
invited external researchers/industrialists, including members
of the \emph{Project Advisory Board}, and will usually be
accompanied by a physical meeting of the \emph{Project Steering Committee}. In
addition, the Project Coordinator will convene management
meetings involving the relevant partners and members of the
Project Advisory Board, as necessary and appropriate. These meetings will be
conducted either using video-conferencing, through a
teleconference, or in person, as appropriate and with due consideration to
cost, urgency and effectiveness.  Technical teams
working on a work package that is spread across sites will
coordinate through email, video-conferencing, telephone and
scheduled meetings.  Finally, the research teams will maintain
regular contact with the Project Coordinator and each other
through regular email reports and telephone conversations.
Progress will be carefully monitored with progress reports and
monitoring documents open to inspection by the EU project
monitoring officer. In the event of a serious and urgent matter
involving all partners, the Project Coordinator may also
convene an Extraordinary meeting of the Steering Committee.
%
All project documentation (whether managerial, legal or
technical) will be maintained through a centralised electronic
repository, accessible to all consortium members on an open
basis, and incorporating audit trails concisely recording
reasons for changes etc.  We propose to use either GIT or SVN,
which provide suitable low-cost,  low-overhead solutions that all partners are
familiar with.  Our technical reports will form the basis for
the public deliverables that appear on the project web site.

\begin{figure}[ht!]
\vspace{-0.75in}
\begin{center}
\centerline{\hspace{1in} \hbox to \columnwidth{\hss\includeimage[height=5.7in]{Management}\hss}}
\end{center}
\vspace{-1.8in}
\caption{Management Structure}
\label{fig:management}
\end{figure}

%\begin{figure}[t!]
%\begin{wrapfigure}{l}{0.7\textwidth}
%\vspace{-0.75in}
%\hspace{-1in}
%\begin{center}
%\centerline{\hspace{1in} 
%\hbox to \columnwidth{\hss\includeimage[height=5.7in,trim={0 0 0 2.5cm},clip]{Management}\hss}
%}
%\end{center}
%\vspace{-1.8in}
%\caption{Management Structure}
%\label{fig:management}
%\end{figure}
%\end{wrapfigure}

%\TODO{Update this to reflect the consortium.}

\subsubsection*{Technical Steering Committee}
\vspace{-6pt}

The \emph{Technical Steering Committee} comprises the WTLs, plus the
Project Coordinator (who will act as chair).  Its purpose is to ensure
the effective running of the project on a day-to-day basis, and to
coordinate work across work packages.  In particular the Technical Steering
Committee will be
responsible for the implementation of the directives of the Project Steering
Committee, for the
guidance and monitoring of the technical WPs, for coordination among
WPs, for  the timely preparation, approval, and forwarding to the Commission
of the deliverables produced by the WPs, and for the resolution of conflicts
amongst WPs.  It will meet on a regular basis, usually through a monthly
teleconference.  Meetings may also be convened on request by any member.
% but also in person where necessary.  
Each member of the Technical Steering Committee has one vote,
which may be made by proxy, or in absentia, if necessary.  
Decisions are taken by consensus, if possible, otherwise by majority vote, 
with the
Project Coordinator retaining the casting vote.

\subsubsection*{Project Steering Committee}
\vspace{-6pt}

The \emph{Project Steering Committee} comprises one representative from each partner
(usually the PI), and is chaired by the Project Coordinator.  
The purpose of this committee is to decide
the general technical direction of the project.  It will also
take major decisions on project finances, addition of partners, removal of non-performing
partners, IPR issues, reallocation of workload etc.  It will meet in person
at least once per year, supplemented by more regular teleconference
meetings as required. Extraordinary meetings may also be convened on request by any partner.
Each representative has one vote, which
may be made by proxy if necessary.  Decisions are taken by
consensus, if possible, otherwise by majority vote, with the
Project Coordinator retaining the casting vote.

\subsubsection*{Project Advisory Board}
\vspace{-6pt}

The Project Advisory Board comprises a small group of invited
academics and industrialists who will provide input to the
project on general technical trends and directions, and advise
the steering committee where required.  The initial composition
of the Advisory Board will be determined at the outset of the
project, but we expect to include academic experts from the data-intensive, high-performance and cloud computing, as well as machine learning, compilation, software-defined infrastructures and optimisation domains. We also expect to include senior representatives from the automotive, AI and IoT industry domains. The Coordinator is authorized to 
execute with each member of the EEAB a non-disclosure agreement, which 
terms shall be not less stringent than those stipulated in this 
Consortium Agreement, no later than 30 calendar days after their nomination 
or before any confidential information will be exchanged, whichever date is earlier. 
We have invited senior representatives from Aarhus University, SAP Institute for Digital Government, Ericsson, TypeSafe, British Telecom,
the oil\&gas industries, the Cloud Competency Centre (Dublin),
	and Scottish Enterprise.

\pagebreak
\subsubsection*{Work Package Team Leaders}
\vspace{-6pt}

Work package team leaders (WTLs) are responsible for tracking progress within their work package,
developing metrics for each deliverable at the outset of each
task, ensuring that results are properly reviewed against these
metrics, and consequently providing feedback to the Project Coordinator on the achievement of goals. 
%
WTLs have been chosen on the basis of managerial experience, technical expertise and
commitment to the work package programmes. % , as shown below.

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{|l|l|l|}\hline
\textbf{WP} & \textbf{WTL} \\ \hline
WP1 &  Juliana Bowles (\coordshort{}) \\ \hline
WP2 & Michael Vinov (\IBMshort{}) \\ \hline
WP3 & Michael Rossbory (\SCCHshort{}) \\ \hline
WP4 & Juliana Bowles (\coordshort{}) \\ \hline
\end{tabular}
\quad\quad&\quad\quad
\begin{tabular}{|l|l|l|}\hline
\textbf{WP} & \textbf{WTL} \\ \hline
WP5 & Marios Belk (\COGNIshort{}) \\ \hline
WP6 & Jose Daniel Garcia Sanchez (\UCMshort{}) \\ \hline
WP7 & Andreas Vermeulen (\SOPRAshort{}) \\ \hline
WP8 & Vladimir Janjic (\UODshort{}) \\ \hline
\end{tabular}
\end{tabular}
\end{center}



%\pagebreak
\subsubsection*{Principal Investigators}
\vspace{-6pt}

One principal investigator (PI) will be nominated by each partner.
The PI is responsible for properly managing the budget allocated to the partner and for performing
all the tasks that are carried out by that partner, reporting to the appropriate WTLs where necessary.  PIs
also act as line managers for the researchers/developers employed on the project by the partner.
PIs will usually also act as WTLs for the main WPs that are carried out at that site, and may be allocated their own
technical tasks. They will normally be the partner's representative on the steering committee.
They have been chosen for their technical expertise and experience of line management and budget handling.

\newcounter{partic}

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{|l|l|l|}\hline
& \textbf{Partner} & \textbf{PI} \\ \hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Juliana Bowles \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Vladimir Janjic \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Michael Vinov \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} &  Andreas Vermeulen \\\hline
\end{tabular}
\quad\quad&\quad\quad
\begin{tabular}{|l|l|l|}\hline
& \textbf{Partner} & \textbf{PI} \\ \hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Michael Rossbory \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Marios Belk \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Jose Daniel Garcia Sanchez \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Eduard Gringinger \\\hline
\addtocounter{partic}{1}
\thepartic & \participantshort{\thepartic} & Herve Le Goff \\\hline

\end{tabular}
\end{tabular}
\end{center}

\vspace{12pt}
\subsubsection*{Project Coordinator}
\vspace{-6pt}

The Project Coordinator is \emph{Dr Juliana Bowles}.  Her role
is to act as the primary point of contact with the European
Commission, to receive feedback on research results from each
work package, to ensure the project maintains effective
progress towards the project objectives based on these results,
to produce any required  project management reports, to ensure
that deliverables are produced according to the planned
schedule and delivered to the Commission and project reviewers
as required, and to resolve disputes between project partners
as and when these arise.  He will convene regular management
and technical meetings, monitor progress on each work package,
collate deliverables, and maintain good contact with each site,
in addition to producing the annual management reports, and
ensuring that each site produces the required financial (audit)
certificates.  He will also be responsible for ensuring that
the Consortium Agreement (including IPR issues, voting rules and the conflict resolution procedures)
and any other legal documents are properly prepared and managed. This will be
done through \SAshort{} \emph{Research and Enterprise
Services}, who have significant expertise in preparing such
agreements.

\vspace{12pt}
\subsubsection*{Project Administrator}
\vspace{-6pt}

The Project Coordinator will be supported in her management
duties by a part-time \emph{Project Administrator} (to be appointed
from staff already employed by \SA{}) and located at \SA{}.  The Administrator
must possess both strong organisational skills and a
sufficient level of technical expertise in order to communicate
management requirements to the Partners, but will not be
involved in the management of day-to-day RTD activities.

\vspace{12pt}
\subsubsection*{Consortium agreement}
\vspace{-6pt}

% The relationship between all partners will be fixed in a Consortium Agreement based on the following principles:

% In order to have a management system applicable through all phases of the
% project, a reasonable approach is to have straight, clear and direct
% management and organization protocol at all levels. This is particularly
% relevant given the challenging financial and industrial policy
% constraints. Therefore, in order to have clearly assigned
% responsibilities, to avoid any friction and to progress as per the
% project plan, the responsibilities and authorities of the project manager
% and the team members will be unambiguous.

The partners will be bound by a formal consortium agreement that is
planned to be signed prior to the beginning of the project of the project,
and in which their roles, responsibilities and mutual obligations will be
defined both for the project life and, where relevant, beyond.  This will
formalise key issues including conflict resolution, IPR procedures, governance structure
etc.  %It will be based on the model consortium agreement issued by the European Commission.
The Digital Europe version of the DESCA, including the European Commission's
inputs will constitute the basis for such consortium agreement.

%\pagebreak
% \vspace{-6pt}
\subsubsection*{Conflict Resolution}
\label{conflict-resolution}
If conflicts arise during the execution of the project, they will be resolved according to the following principles:
% 
% \begin{itemize}
% \item
They will first be addressed within the relevant WP through discussion chaired by the WTL;
% \item
If this fails, the issue will be presented by the WTL either to the Technical Steering Committee
or to the Project Steering Committee, depending on the nature of the problem (technical or business/strategic).
% \item
The relevant board will attempt to resolve the issue through the usual voting procedure.
% \end{itemize}
%
Technical issues between WPs will also be addressed by the Technical Steering Committee.
%  As noted above, the TSC of the project consists of the WP leaders (chaired by the technical coordinator), and the GA consists of representatives of each partner (chaired by the management coordinator).
Any conflicts that cannot be resolved through the principles above will
be handled according to the dispute resolution provision set forth in the
Consortium Agreement.

% \subsubsection*{Management Costs}

% We have budgeted for the Project Administrator at 25\% effort
% over the lifetime of the project (i.e. 9 person-months) at
% \SAshort{}, plus small-scale management effort as required by each support to support the
% project through the preparation of reports for review meetings and other project-level
% management tasks.
% We have also budgeted for the cost of running annual Advisory Committee meetings, including travel
% support for unfunded Advisory Committee members, at \euros{} XX p.a. for a total cost of \euros{} XX.
% We have budgeted an additional
% \euros{}~4,800 for travel by the Project Coordinator (estimated
% as an additional two trips per annum).
% In order to minimise costs and time expenditure, as far as
% possible, all project management activities will be carried out using
% low-cost means such as email, Skype, telephone or
% video-conferencing, and any managerial travel will normally be
% combined with technical or research meetings.  Each site with expenditure
% in excess of \euros{}~325,000 also requires specific costs
% to cover the preparation of the required financial certificates, which will
% generally involve subcontracting an external financial auditor.

\draftpage
\subsection{Consortium as a Whole}
\eucommentary{\begin{itemize}
\item
Describe the consortium. How will it match the project's objectives? How do the members complement one another (and cover the value chain, where appropriate)? In what way does each of them contribute to the project? How will they be able to work effectively together?
\item
If applicable, describe the industrial/commercial involvement in the project to ensure exploitation of the results and explain why this is consistent with and will help to achieve the specific measures which are proposed for exploitation of the results of the project (see section 2.3).
\item
Other countries: If one or more of the participants requesting EU funding is based in a country that is not automatically eligible for such funding (entities from Member States of the EU, from Associated Countries and from one of the countries in the exhaustive list included in General Annex A of the work programme are automatically eligible for EU funding), explain why the participation of the entity in question is essential to carrying out the project
\end{itemize}
}

\begin{figure}[t]
\begin{center}
%\includeimage[scale=0.6,angle=0]{BlessConsortium.pdf}
\end{center}
\vspace{-0.3in}
\caption{Areas of Partner Expertise}
\label{fig:consortium}
\end{figure}

\textbf{Figure~\ref{fig:consortium}} shows the areas of expertise that
are relevant to this project and the consortium partners that
possess that expertise.  All partners span multiple areas,
providing technical depth within the consortium, and avoiding
knowledge gaps.  Within the areas, each partner possesses
complementary expertise, but with enough knowledge overlap to
ensure tight cohesion of the consortium. The consortium
comprises both academic and industrial expertise.
%  from XX
% highly-respected partners.
The consortium links the world-leading technical expertise of
the participating groups on XX.


\paragraph*{Integration of the Consortium.}
%\vspace{-6pt}

Several of the partners already have close working relationships through
recent and ongoing research projects (e.g. \rephrase).
The teams share common technical interests and several are active members in e.g. the
HiPEAC network of excellence (\SAshort{};).
Work Packages have been designed to foster close collaboration
between teams at different organisations, with multiple groups involved in all of
the technical and evaluation work packages. The tasks in 
each work package have been allocated on the basis of
technical expertise and ability. All tasks have been designed to involve multi-site
collaboration and/or the exchange of information, which is
intended to promote healthy interaction between the partners.
Finally, in order to ensure good integration between the partners,
we propose to run at least one technical workshop each year, and
have also requested funds to allow researchers from each group
to visit other groups on a regular basis.  We anticipate that
all of the \TheProject{} researchers will participate in these
technical workshops and exchanges.  We also intend to publish a significant
number of research papers and technical reports deriving from our joint research,
and to collaborate on joint tool production. 
We thus foresee a necessary and close level of integration between
the \TheProject{} partners.

\paragraph{Industrial/Commercial Involvement.}

\TheProject{} directly involves one large company (\IBMshort{}), 
%% sloppy -- KH
XX SMEs.
These organisations have included draft exploitation plans that will directly use the results of the project
as part of their ongoing business strategies and commercial development.
% follows an industrially-inspired agenda and addresses a key and topical challenge in
The \TheProject{} project further engages directly with industry through its
dissemination, user community and outreach activities 
% a dedicated  workpackage (WP7) 
% that are aimed at promoting
will promote the
\TheProject{} tools, technologies and above all \emph{mindset} and \emph{methodology} to a wider user base,
especially through industry-focused events. % that should attract C/C++ programmers and Simulink/SCADE users.
The objective is to ensure widespread uptake of the project results in a broad base of potential industrial
software developers targeting a variety of commercially important domains. % including telecommunications, 3D modelling and the automotive sector.
This will be assisted by including major industry participants on its Advisory Committee,
by actively engaging with the C, C++communities, and by engaging with the ISO C++ Standard Committee and ITU FG-DPM.
% and with the upcoming C++17 design.
% and by actively engaging with new coding standards for parallel and data-intensive applications, as well as C++.
% Finally, members of the consortium are also active members of the ISO C++ Standards Committee. This will
% ensure that the techniques, approach and methodology developed in the \TheProject{} will
% have a broad exposure through the C++ community, and through the evolution of the C++ language
% design itself.

% \khcomment{More needed here.}

\paragraph{Project Management Expertise.}

Members of the team have been heavily involved in running
various national and international research projects.  The
Project Coordinator, Dr Juliana Bowles, has obtained numerous
research grants and awards from national and international
bodies...

As described in the partner descriptions below,
%% Should add activities for INRIA, CodePlay etc.
most of the other partners have been extensively involved in previous and
ongoing EU projects at both technical and managerial
levels, and have dedicated experienced senior staff on the \TheProject{} project.
%XX
%
This experience will be called on as necessary to resolve any
managerial problems that may arise during the course of the
project.


%\bigskip
%\bigskip
\subsection{Resources to be Committed}

\eucommentary{Please provide the following:
\begin{itemize}
\item
a table showing number of person/months required (table 3.4a)
\item
a table showing 'other direct costs' (table 3.4b) for participants where those costs exceed 15\% of the personnel costs (according to the budget table in section 3 of the administrative proposal forms)
\end{itemize}}

\input{resources}

%\pagebreak
\subsubsection{Management Level Description of Resources and Budget}
\vspace{-6pt}

\TODO{This needs to be updated in line with the rest of the
project.}

The project will employ XX person-months of effort over three
years, comprising one or more full-time or part-time researchers at each site
plus one part-time project administrator at \SAshort{}, % and one part-time web designer at \INRIAshort,  Not in PMs?
20\% of the Project Coordinator and 10\% of the WTLs. The researchers will be supported by
the necessary dedicated computing equipment,
% the
% usual basic research equipment (workstations and/or laptop
% computers, network facilities, printers, dedicated file
% servers, etc.) funded from the project overheads, by
% special-purpose heterogeneous hardware necessary to carry out
% the research, 
by funding to enable the necessary travel to scientific and technical conferences, trade shows, 
project meetings and other project-related events, and by the funding that is needed to establish/enhance existing
industrial and academic contacts and to establish a user
community for the \TheProject{} tools and technologies.
%
The quoted budget includes all relevant national social and
other legitimate employment costs as permitted under the rules
governing EU Horizon 2020 ICT projects, including costs of
healthcare, social security and pensions provision, in line with national norms for each site.
%
Sufficient travel funding is also needed to support good
collaboration between the groups, including attendance at the
annual technical project meetings, plus individual visits between
sites. We have budgeted approximately \euros{}~8,000 per
site per year (varied in line with previous
costs at each site)  to cover, for example:
% \begin{itemize}
% \item 
attending two project workshops at \euros{}~600 each;
% \item
attending the annual Project Review Meeting at \euros{}~600;
% \item
one 1-week inter-site visit at \euros{}~750 each;
% \item
attending two conferences per year within the EU at \euros{}~1,000 each;
% \item
attending one conference per year outside the EU at \euros{}~1,500;
%\item
three conference fees per year at \euros{}~650 each.
% \end{itemize}
%
\noindent
In addition, \SAshort{} has budgeted \euros{}~1,500 per year to cover attendance at
IFIP Working Group meetings, visits to industrial concerns for dissemination purposes,
attendance at developer conferences, demonstrations etc. to promote the project,
and \euros~1,000 per year to support travel that is related to the management of the
project.
Wherever possible, travel for different purposes will be
combined into a single trip. We have also budgeted \euros~1,000
per year at \SAshort{} to support attendance by the Project Advisory Board members at
the annual Advisory Board Meetings, where this cannot be met from other sources.

\TODO{Add any specialist equipment.  We might add a serious
  multi-core/multi-GPU machine.}

%\pagebreak
\subsubsection{Additional Partner Costs}
\vspace{-6pt}

\paragraph{Testbed systems and development servers.}
\SAshort{} has budgeted \euros{} XX for a central
server to support the various software and document repositories that are needed by the project, to run the project website and to provide access
to the shared research data that will be generated by the project.

\paragraph{Open Access Publication Fees.}
Each academic partner has budgeted approximately \euros{5,000} to
support gold open access publication for key project publications
(representing about 10-20\% of the expected project output).  The
budget will be pooled if not used by a specific partner, and used to
support further gold open access publication by other partners or
other dissemination activities, as necessary to maximise the overall
success of the project.  There will be no charge for green open access
publication, which will be used for the remainder of the project
publications.

%\vspace{-12pt}
\label{bibliography}
\addcontentsline{toc}{section}{References}

\bibliographystyle{abbrv}
\bibliography{bibliography}


%% Write macro to split Sections 1-3
\Split{1-3}

% ---------------------------------------------------------------------------
%  Section 4: Members of the Consortium
% ---------------------------------------------------------------------------

\newpage

\eucommentary{Page limits do not apply.}

\section{Members of the Consortium}

\eucommentary{Please provide, for each participant, the following (if available):\\
\begin{itemize}
\item
a description of the legal entity and its main tasks, with an explanation of how its profile matches the tasks in the proposal;
\item
a curriculum vitae or description of the profile of the persons, including their gender, who will be primarily responsible for carrying out the proposed research and/or innovation activities;
\item
a list of up to 5 relevant publications, and/or products, services (including widely-used datasets or software), or other achievements relevant to the call content;
\item
a list of up to 5 relevant previous projects or activities, connected to the subject of this proposal;
\item
a description of any significant infrastructure and/or any major items of technical equipment, relevant to the proposed work;
\item
[any other supporting documents specified in the work programme for this call.]
\end{itemize}}

\subsection{Participants}
\Participant{SA}{(\url{http://www.st-andrews.ac.uk})}

\begin{wrapfigure}{R}{2cm}
\vspace{-3.95cm}
\hfill \includeimage{logos/st-andrews-logo.jpg}
\vspace{-1cm}
\end{wrapfigure}

\label{sec:participantUSTAN}

%===============================================================================
The \SAlong{} is the third-oldest in the English-speaking world (founded 1413).
The School of Computer Science was likewise one of the earliest Computer Science departments in the world (founded 1972).
It has established an excellent reputation for its pioneering research in e.g.,
parallel computing, software engineering, programming language design,
software architectures, theoretical computer science and
distributed/mobile systems.  This research expertise has been
recognised through the award of numerous research grants and
awards from the UK and the European Commission.

\vspace{10pt}
\textbf{The \SAlong{} coordinates the \TheProject{} project and
leads work packages WPXX and participates in WPXX on XX.}
\vspace{10pt}

\paragraph{Dr Juliana Bowles} \url{http://www.}


\subsubsection*{Relevant publications}
\begin{itemize}
\item XX
\end{itemize}

\pagebreak
\subsubsection*{Relevant Research Projects}

\begin{itemize}
% \item
% Automatic Prediction of Resource Bounds for Embedded Systems (EmBounded, IST-2004-510255, 2005-2008, \url{http://www.embounded.org});
\item XX
\end{itemize}

\Participant{UOD}{\url{http://www.dundee.ac.ul}}
\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/UOD-logo.png}
\vspace{-1cm}
\end{wrapfigure}

The University of Dundee is a leading Scottish institution and one of the world’s top universities. With 18,000 students from 145 countries, it is truly a global University. Re-
search at Dundee delivers impact by harnessing expertise across disciplines to tackle some of the most challenging problems the world faces today. It promotes the sustainable use of global resources, shapes the future through innovative design and improve social, cultural and physical well-being. The University takes pride in delivering research with significant impact on global innovation and international policy. With 44\% of staff devoted solely to research, it continues to advance the lives of people across the world. Research in Dundee benefits from a wealth of experience from around the world, as international academics and external partners work closely together.

The University of Dundee was the first university in the UK that had am MSc program for Big Data Analytics. It continues to be at the forefront of Big Data Science in the UK, collaborating with national and international companies such as Teradata, Microsoft,Yahoo and local companies such as Outplay, Ninja Kiwi and Waracle along national bodies such as DataLab to deliver research and training into Data Science. \UODshort{} also has an established collaboration with various institutions in the area of Health Informatics, including the Health Informatics Centre (HIC) of the University of Dundee, which is recognised as a leader in health data linkage. It was the first centre in Scotland to offer a Safe Haven, which is now Nationally Accredited and ISO27001 certified. Operating under tight data governance controls, the Safe Haven allows secure collaborative research using sensitive eHealth data. HIC also maintains a clinical data repository of eHealth data covering approximately 20\% of the Scottish population, with core datasets providing a continuous record extending back 30 years.

\subsubsection*{Role in Serums}
\textbf{\UODshort{} will lead the WP8 on Dissemination, Exploitation, Community Building and Communication activities, with the special focus on education program by delivering MOOCs. It will also contribute with its expertise on parallel programming, pattern-based application design and development, big data analytics and machine learning to WP2, WP3 and WP6.}  

\subsubsection*{Key Personnel Involved in the Project}

\paragraph{Dr Vladimir Janjic} is a lecturer in Data Science at the Division of Computing in the School of Science and Engineering at the University of Dundee. He received his PhD from the University of St Andrews in 2011. He has relevant expertise in distributed systems, parallel pattern-based programming, frameworks for big data analytics, machine learning, managing complexity and systems integration. His PhD work was undertaken in the context of the EU FP6 \textbf{SCIEnce} project, where he worked on mechanisms to ensure adaptivity on heterogeneous environments, which included building the SCALES simulator for work distribution policies on heterogeneous environments, and the implementation of dynamic scheduling mechanisms. Whilst at the University of St Andrews, he was also involved in several other EU projects, including H2020 RePhrase project, where he investigated software engineering techniques for developing pattern-based parallel applications, including application of machine learning techniques in managing system properties. He has published more than 25 research papers so far. He was one of the main authors and original contributors of the \textbf{SERUMS} EU project that investigates security and privacy of medical data in modern healthcare systems, and he is currently a principal investigator of the project for the University of Dundee.

\paragraph{Andrew Cobley} Andy Cobley is a senior lecturer at the school of Science and Engineering at the University of Dundee.    He is the program director for the MSc programs in Data Science and Data Engineering at the University.   Amongst his research interest is the NoSQL movement and applying these technologies to internet and games applications. He is currently working with the Horizon 2020 \textbf{WeObserve} project, which aims to improve our knowledge of how to build and operate citizen observatories. He has recently finished the highly successful Horizon 2020 \textbf{GROW} project investigating over seeing the technology work package on how to process and store large quantity of  soil moisture data  that is generated from Citizen Science.  He has supervised PHd students working with the Angus Lamond Lab in the University investigating Big Data approaches using Hadoop to analyze the large data that is generated during the study of Proteomics by the Lab.  Some of this work was carried out in co-operation with Teradata and Microsoft.


He is the creator of the FutureLearn MOOC “Data Science in the Games Industry” for the Scottish DataLab, this program has run on more than four occasions attracting attention from around the world. In addition, he is a writer for the on-line computing magazine “The Register” specializing in Data Science, Devops and database technologies at scale. 


\Participant{IBM}{(\url{http://www.research.ibm.com/labs/haifa/)}}


\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/ibm.jpg}
\vspace{-1cm}
\end{wrapfigure}

\ 

%\vspace{24pt}
For more than sixty years, IBM Research, as the world's largest IT research organisation has been the innovation engine of the IBM corporation. Since the beginning of 2000, IBM has spent \$75 billion in R\&D, enabling IBM to deliver key innovations and maintain U.S. patent leadership for the 21st consecutive
year in 2013.
IBM also participates in and contributes to the work of standards consortia, alliances, and formal national and international standards organisations. 
The IBM Haifa Research Lab is IBM's largest research laboratory outside of the United States, 
employing almost 500 researchers, the majority of whom hold doctorate and master degrees in computer science, electrical engineering, mathematics, and related fields. Since its founding in 1972, HRL has conducted world class research vital to IBM's success. R\&D projects are being executed today in areas such as Cognitive computing, Healthcare and Life Sciences, Verification Technologies, Telco, Machine Learning, Cloud Computing, Multimedia, Active Management, Information Retrieval, Programming Environments and Information and Cyber Security. The Quality and Security Department of the IBM Research Haifa includes researchers in the fields of cyber security and privacy. As a multi-disciplinary research area, researchers come from different research domains including verification, data analytic, operating systems and run-time systems, languages and compilers, network systems, and protocols and cloud technologies.
HRL has a long history of successful participation in EU projects. A partial list includes the following: SMESEC (H2020), REPHRASE (H2020), SHARCS (H2020), PINCETTE (FP7, coordinator), RESERVOIR (FP7, coordinator), CloudWave (FP7, coordinator), SHADOWS (FP6, coordinator), CASPAR (FP6), HiPEAC (FP6 + FP7), SARC (FP6), ACOTES (FP7), MilePost (FP7), HYPERGENES (FP7), HERMES (FP7), SAPIR(FP6, coordinator), PROSYD (FP6, coordinator), Modelplex (FP6, coordinator).
The Quality and Security Department develops advanced tools and technologies spanning the entire spectrum of Functional Verification, Code Analysis and Cyber Security.

As a global leader in IT security, IBM offers the strategies, capabilities, and technologies necessary to help organizations in the private and public sectors preemptively protect the organisation from threats and address the complexities and growing costs of security risk management and compliance. IBM is helping to solve essential security challenges including:
\begin{itemize}
\item
  Better secure data and protect privacy
\item
	Control network access and help assure resilience
\item
	Defend mobile and social workplace
\item
	Manage third-party security compliance
\item
	Address new complexity of cloud and virtualization
\item
	Build a risk-aware culture
\end{itemize} 	
To facilitate a comprehensive offering IBM is continuously investing in emerging technologies in the area of security intelligence and has a wide variety of security products and services. IBM is recognized in the industry as a leader in IT cyber security.
In recent years, IBM has acquired several cyber security start-ups in Israel increasing its R\&D presence in the region. IBM has recently also announced the establishment of a Cyber Center of Excellence (CCoE) in Beer-Sheva, Israel. The IBM Haifa Research Lab, as a well recognized IBM research facility and the largest one outside of the US, is collaborating with European research facilities to support the buildup of the CCoE as well as the acquired cyber security start-ups.

\vspace{10pt}
\textbf{IBM leads work package WPXX}

\vspace{10pt}

\paragraph{Michael Vinov}     

\subsubsection*{List of Publications}


%Formal verification:
\begin{itemize}
\item XX
\end{itemize}

\subsubsection*{Relevant Research Projects}
\begin{itemize}
\item
Validating Changes and Upgrades in Embedded Software (PINCETTE, ICT-257647);
\item
CloudWave: Agile Service Engineering for the Future Internet (CloudWave, ICT-610802);
\item
Refactoring Parallel Heterogeneous Resource-Aware Applications --- a Software Engineering Approach (RePhrase, ICT-644235);
\item
Secure Hardware-Software Architectures for Robust Computing Systems (SHARCS, ICT-322014).

\end{itemize}

\Participant{SCCH}{(http://www.scch.at)}

\begin{wrapfigure}{R}{4cm}
\vspace{-2cm}
\hfill \includeimage[width=4cm]{logos/SCCH.jpg}
\vspace{-1cm}
\end{wrapfigure}

\SCCH{} (\url{www.scch.at}) is an Austrian research and technology organisation, funded in 1999 by several institutes of Johannes Kepler University, Linz. Its primary focus is on applied research in the fields of software and data science. 
In projects with partner companies state-of-the-art research results are applied to practical industrial projects to increase and maintain their competitiveness. One of the focus areas is Data Analysis Systems (DAS), specialising on the advancement and application of methods for the analysis and modelling of complex and massive sensor data in its (industrial) application context. Particular application domains include
\begin{inparaenum}[a)]
\item modelling, prognosis, forecast and control of systems
\item industrial fault detection, diagnosis and prognosis
\item the discovery of knowledge and structure in industrial processes.
\end{inparaenum}
\SCCHshort{} is or was involved in the EU-funded projects H2020 ALOHA (Grant Agreement 780788), TRESSPASS (SEC-2016-2017-2, Proposal Nr. 787120) RePhrase (ICT-644235), ParaPhrase (IST-2011-288570), ADVANCE (IST-2010-248828), SECO, and FACETS, where in ALOHA and TRESSPASS SCCH contributes with its expertise in deep learning and the latter are concerned with parallel computation.  The participating investigators are project managers or researchers for relevant applied research projects in the fields of machine learning, parallelisation, and scheduling, and are active in the research community by publishing and reviewing for journals and conferences.

\vspace{10pt}
\textbf{\SCCHshort{} leads WPXX}
\vspace{10pt}

\subsubsection*{List of Publications}

\begin{itemize}

\item XX
  
\end{itemize}

\subsubsection*{Relevant Research Projects}

\begin{itemize}

\item RePhrase (H2020, ICT-644235) - Refactoring Parallel Heterogeneous Resource-Aware Applications. SCCH's role: reinforcement learning based dynamic scheduling and industrial use cases

\item ParaPhrase (IST-2011-288570) - Parallel Patterns for Adaptive Heterogeneous Multicore Systems. SCCH's role: use cases in the area of machine learning

\item ADVANCE (IST-2010-248828) - Asynchronous and Dynamic Virtualisation through performance ANalysis to support Concurrency Engineering, \url{http://www.project-advance.eu});

\item ALOHA (H2020 Grant Agreement 780788) - Software framework for run-time-Adaptive and secure deep Learning On Heterogeneous Architectures. SCCH's role: develop deep transfer learning based methods for surveillance applications

\item TRESSPASS (H2020 SEC-2016-2017-2, Proposal Nr. 787120) - robusT Risk basEd Screening and alert System for PASSengers and luggage. SCCH's role: develop deep learning based methods for security applications

\item GROW (H2020 Grant Agreement No.690199) Citizen Observatory that has ground-truthed Sentinel-1 to improve the accuracy of predictions on extreme events, such as flood, drought and wildfire. \url{https://growobservatory.org};

\item WeObserve (H2020 Grant Agreement No.776740) An Ecosystem of Citizen Observatories for Environmental Monitoring. Umbrella for H2020 projects GROW Observatory, Landsense, Groundtruth 2.0 and Scent \url{https://growobservatory.org};

\end{itemize}

% ============================

\Participant{SOPRA}{(\url{https://www.soprasteria.co.uk/)}}


\begin{wrapfigure}{R}{6cm}
\vspace{-2cm}
\hfill \includeimage[width=6cm]{logos/Sopra-Steria-logo2.png}
\vspace{-1cm}
\end{wrapfigure}

\ 

%\vspace{24pt}

Sopra-Steria, since 1968, supports the primary business areas of consulting services, systems integration, integration of ERP, implementation of applications, as well as providing technical support to users and application maintenance and outsourcing services and operation of professional processes.

Has experience in:

\begin{itemize}
    \item Cyber Security via: (\url{https://www.soprasteria.com/services/cybersecurity})
    \item Artificial Intelligence via:\\ (\url{https://www.soprasteria.com/services/technology-services/artificial-intelligence})
    \item Internet of Things (IoT) via:\\ (\url{https://www.soprasteria.com/services/technology-services/internet-of-things})
\end{itemize}

Supports following industries:
\begin{itemize}
    \item Aerospace
    \item Defense and Security
    \item Energy Utilities
    \item Financial Services
    \item Insurance and Social
    \item Government
    \item Retail
    \item Telecommunication, Media and Entertainment
    \item Transport
\end{itemize}



\vspace{10pt}
\textbf{Sopra-Steria leads work package WPXX}

\vspace{10pt}

\paragraph{Andreas Francois Vermeulen} - Head of Analytics, Digital Consultancy Services within the United-Kingdom.

Andreas has been in information technology industry for forty plus years. He is the consulting delivery head for over hundred consultants in the United-Kingdom. Author of three international books on big data, data science and machine learning.

\paragraph{Ian Kayne} - Head of Cyber Security Consulting within the United Kingdom.

Ian has over twenty-four+ years’ experience in cybersecurity and cloud ecosystems, leading global initiatives for FTSE100 and Fortune500 organizations as well as Government and Public Sector. Expert at penetration test and reverse engineering services. A published author and lectures on secure coding and reverse engineering modules at MSc level in cybersecurity.


\paragraph{Rakhee Porter} - Cyber Resilience Lead within the United-Kingdom.

Rakhee has ten plus years of experience delivering risk based resilience programs to secure ecosystems. Expert in business resilience services including Technology Resilience, Operational Resilience, Crisis Management and Business Continuity.

\subsubsection*{List of Publications}

\begin{itemize}
\item The SERUMS tool-chain: ensuring security and privacy of medical data in smart patient-centric healthcare systems. (IEEE Big Data), Los Angeles, December 2019, IEEE Press. DOI: 10.1109/BigData47090.2019.9005600
\end{itemize}

\subsubsection*{Relevant Research Projects}
\begin{itemize}
\item SERUMS (\url{https://www.serums-h2020.org/})
\end{itemize}

% ============================
\Participant{FRQ}{(\url{https://www.frequentis.com/})}

\begin{wrapfigure}{R}{6,2cm}
\vspace{-3cm}
\hfill \includeimage[width=7cm]{logos/FRQ_logo.png}
\vspace{-1cm}
\end{wrapfigure}
\vspace{10pt}

Established in 1947 Frequentis has grown to an international supplier of communication and information systems with an export quota of more than 95\%. The core market is focused on safety critical control room solutions in various domains. Over 2000 employees are working in the corporate headquarters in Vienna, and subsidiaries in more than 50 countries worldwide. Frequentis is a member of SESAR Joint Undertaking and an active participant in numerous researches, regulatory, industry and standardization communities.

Frequentis contributes its expertise in international project and technology know-how as an industry partner in various safety critical domains. Particularly the knowledge as a system integrator in the ATM domain will help to integrate the different components developed by the partners in the project to have successful demonstration and validation in the end. The international knowledge and experience of Frequentis will assure that the consortium possesses the scientific and technological expertise required for Digital Fortress.

%\vspace{10pt}
%\textbf{Frequentis leads work package WPXX}
\vspace{10pt}

\textbf{Key Personnel:}

Eduard Gringinger (male) (PhD, MSc, MSocEcSc, BSc), is senior lead scientist and project manager with more than 10 years of experience in the research areas of safety critical data and services with a business focus on the air traffic management domain. He has work experience in relation with information management, data and service modelling and is member in various EUROCAE, ICAO/AMO working groups. He received a PhD, MSc, and a MSocEcSc with distinctions from Vienna University of Technology.

Christoph Fabinaek (male) (PhD, MSc, MBA), is senior lead scientist. Since 2006, he worked in various roles in safety critical domains at Frequentis. He was consultant for 3 years at AI Informatics and Siemens and is now is chairman of OwnYourData. He received his PhD from the Vienna University of Technology in mathematics (also working at the Innovative Computing Laboratory/University of Tennessee) and an MBA from the Danube University Krems (1 semester at the Weatherhead School of Management in Cleveland, Ohio).

\subsubsection*{List of Publications}

\begin{itemize}
\item C.G. Schuetz, B. Neumayr, M. Schrefl, E. Gringinger and S. Wilson, "Semantics-based summarisation of ATM information: Managing information overload in pilot briefings using semantic data containers", The Aeronautical Journal (2019), 1-27. \url{doi:10.1017/aer.2019.74}
\item E.Gringinger, R.M,Keller, A.Vennesland, C.G.Schuetz, and B.Neumayr, "A Comparative Study of Two Complex Ontologies in Air Traffic Management", in Proc. of the 2019 AIAA/IEEE 38th Digital Avionics Systems Conference (DASC), IEEE, September 2019, in press.
\item E. Gringinger, C. Fabianek, C. Schütz, B. Neumayr, M. Schrefl, A. Vennesland, S. Wilson, "The Semantic Container Approach: Techniques for ontology-based data description and discovery in a decentralized SWIM knowledge base", Proceedings of the SESAR Innovation Days 2018 (SID 2018), December 3-7, 2018, Salzburg, Austria. \url{https://www.sesarju.eu/sites/default/files/documents/sid/2018/papers/SIDs\_2018\_paper\_78.pdf}
\item E. Gringinger, C. Fabianek, C. Schütz, J. Stöbich, B. Neumayr, M. Schrefl, "A Proof-of-Concept Implementation of a Semantic Container Management System for Air Traffic Management", Proceedings of the EDAW 2018 Posters and Demonstrations Session co-located with 21st International Conference on Knowledge Engineering and Knowledge Management (EKAW 2018), November 12-16, 2018, ISSN 1613-0073, pp. 69-72, Nancy, France. \url{https://dblp.org/rec/bib/conf/ekaw/GringingerFSNS18}
\item E. Gringinger, C. Schuetz, B. Neumayr, M. Schrefl and S. Wilson, "Towards a value-added information layer for SWIM: The semantic container approach", 2018 Integrated Communications, Navigation, Surveillance Conference (ICNS), Herndon, VA, 2018, pp. 3G1-1-3G1-14. \url{https://doi.org/10.1109/ICNSURV.2018.8384870}
\item C. Flachberger, E. Gringinger, "Decision Support for Networked Crisis \& Disaster Management - A Comparison with the Air Traffic Management Domain", Information Systems for Crisis Response and Management (ISCRAM), Rio de Janeiro, Brasil, May 22-25, 2016. \url{https://episecc.eu/sites/default/files/1332\_ChristianFlachberger\%2BEduardGringinger2016.pdf}
\end{itemize}


\subsubsection*{Relevant Research Projects, Patents, and Products}

\begin{itemize}

\item SATIE (H2020-SU-INFRA-2018, Grant agreement ID: 832969) - Security of Air Transport Infrastructure of Europe, \url{https://cordis.europa.eu/project/id/832969};

\item SAPIENT (H2020-SESAR-2015-1, Grant agreement ID: 699328) - Satellite and terrestrial architectures improving performance, security and safety in Air Traffic Management (ATM), \url{https://cordis.europa.eu/project/id/699328};

\item PJ19 CI (H2020-SESAR-2015-2, Grant agreement ID: 731765) - Sesar2020 Pj19-03 - Air Traffic Management Systems and Services, \url{https://www.eurocontrol.int/articles/content-integration-sesar-2020-project-pj19-ci};

\item BEST (H2020-Sesar-03-2015, Grant agreement ID: 699298) - Achieving the BEnefits of SWIM by making smart use of Semantic Technologies, \url{http://www.project-best.eu/};

\item Engage (H2020-SESAR-2016-2, Grant agreement ID: 783287) - SESAR Knowledge Transfer Network, \url{https://www.engagektn.com/};

\item SEMCON (ICT of the Future Call 2018 Data Market: 869781) - Semantic Containers for Data Mobility, \url{https://www.ownyourdata.eu/en/semcon/};

\item Patent for prioritization information (EP3118839A1) could be relevant for Digital Fortress, \url{https://patents.google.com/patent/EP3118839A1/en};

\item MosaiX SWIM, the Frequentis ATM informaiton integration platform, \url{https://www.frequentis.com/sites/default/files/support/2018-02/MosaiX\%20SWIM.pdf}; 

\end{itemize}

% ============================




\subsection{Third parties involved in the project (including use of third party resources)}

No third parties are involved in the project.

% ---------------------------------------------------------------------------
%  Section 5: Ethics and Security
% ---------------------------------------------------------------------------

\newpage

\section{Ethics and Security}

\subsection{Ethics}

The proposal raises no specific ethical concerns.


\subsection{Security}

Please indicate if your proposal involves:

\begin{itemize}
\item
activities or results raising security issues: NO
\item
'EU-classified information' as background or results: NO
\end{itemize}

%% Write macro to split Sections 4-5
\Split{4-5}

%% Finalise batch file
\immediate\write\BatchFile{exit}% 
\immediate\closeout\BatchFile% 

\newpage

\label{bibliography}
\addcontentsline{toc}{section}{References}

%\bibliographystyle{abbrv}
%\bibliography{bibliography_ustan}
%\bibliography{bibliography_scch}

\end{document}
